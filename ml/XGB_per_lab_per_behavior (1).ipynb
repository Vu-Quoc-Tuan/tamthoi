{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7cc0b7e",
      "metadata": {
        "id": "f7cc0b7e"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VY0FvEMEDqyg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY0FvEMEDqyg",
        "outputId": "e4d14dd3-0424-4e11-aca2-67f42d4d029f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fB_khskIHUQa",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB_khskIHUQa",
        "outputId": "be16295c-1520-477d-ef51-c6afc50f35dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n",
            "Downloading MABe-mouse-behavior-detection.zip to /content\n",
            " 95% 2.48G/2.63G [00:01<00:00, 1.58GB/s]\n",
            "100% 2.63G/2.63G [00:01<00:00, 1.63GB/s]\n",
            "Unzipping files...\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"MABe-mouse-behavior-detection.zip\"):\n",
        "    print(\"Downloading dataset...\")\n",
        "    !kaggle competitions download -c MABe-mouse-behavior-detection\n",
        "else:\n",
        "    print(\"Zip already exists, skip download.\")\n",
        "\n",
        "if not os.path.exists(\"./input\"):\n",
        "    print(\"Unzipping files...\")\n",
        "    !unzip -q MABe-mouse-behavior-detection.zip -d ./input\n",
        "else:\n",
        "    print(\"Input folder already exists, skip unzip.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E54FbxqaLrmA",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E54FbxqaLrmA",
        "outputId": "0c3b4d8a-df56-4429-804b-74cf14f1e75a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (13.6.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.3)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x) (0.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U lightgbm cupy-cuda12x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g1AM36ZMHYev",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g1AM36ZMHYev",
        "outputId": "61da412f-1f7f-4ab1-a64a-2ae290123c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q polars xgboost scikit-learn catboost optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KuXZDyINKSsD",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KuXZDyINKSsD",
        "outputId": "2306cedb-35eb-43d3-bbe8-2b33a750e222"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing metric.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile metric.py\n",
        "\"\"\"F Beta customized for the data format of the MABe challenge.\"\"\"\n",
        "\n",
        "import json\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "\n",
        "class HostVisibleError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "def single_lab_f1(lab_solution: pl.DataFrame, lab_submission: pl.DataFrame, beta: float = 1) -> float:\n",
        "    label_frames: defaultdict[str, set[int]] = defaultdict(set)\n",
        "    prediction_frames: defaultdict[str, set[int]] = defaultdict(set)\n",
        "\n",
        "    for row in lab_solution.to_dicts():\n",
        "        label_frames[row['label_key']].update(range(row['start_frame'], row['stop_frame']))\n",
        "\n",
        "    for video in lab_solution['video_id'].unique():\n",
        "        active_labels: str = lab_solution.filter(pl.col('video_id') == video)['behaviors_labeled'].first()  # ty: ignore\n",
        "        active_labels: set[str] = set(json.loads(active_labels))\n",
        "        predicted_mouse_pairs: defaultdict[str, set[int]] = defaultdict(set)\n",
        "\n",
        "        for row in lab_submission.filter(pl.col('video_id') == video).to_dicts():\n",
        "            # Since the labels are sparse, we can't evaluate prediction keys not in the active labels.\n",
        "            if ','.join([str(row['agent_id']), str(row['target_id']), row['action']]) not in active_labels:\n",
        "                continue\n",
        "\n",
        "            new_frames = set(range(row['start_frame'], row['stop_frame']))\n",
        "            # Ignore truly redundant predictions.\n",
        "            new_frames = new_frames.difference(prediction_frames[row['prediction_key']])\n",
        "            prediction_pair = ','.join([str(row['agent_id']), str(row['target_id'])])\n",
        "            if predicted_mouse_pairs[prediction_pair].intersection(new_frames):\n",
        "                # A single agent can have multiple targets per frame (ex: evading all other mice) but only one action per target per frame.\n",
        "                raise HostVisibleError('Multiple predictions for the same frame from one agent/target pair')\n",
        "            prediction_frames[row['prediction_key']].update(new_frames)\n",
        "            predicted_mouse_pairs[prediction_pair].update(new_frames)\n",
        "\n",
        "    tps = defaultdict(int)\n",
        "    fns = defaultdict(int)\n",
        "    fps = defaultdict(int)\n",
        "    for key, pred_frames in prediction_frames.items():\n",
        "        action = key.split('_')[-1]\n",
        "        matched_label_frames = label_frames[key]\n",
        "        tps[action] += len(pred_frames.intersection(matched_label_frames))\n",
        "        fns[action] += len(matched_label_frames.difference(pred_frames))\n",
        "        fps[action] += len(pred_frames.difference(matched_label_frames))\n",
        "\n",
        "    distinct_actions = set()\n",
        "    for key, frames in label_frames.items():\n",
        "        action = key.split('_')[-1]\n",
        "        distinct_actions.add(action)\n",
        "        if key not in prediction_frames:\n",
        "            fns[action] += len(frames)\n",
        "\n",
        "    action_f1s = []\n",
        "    for action in distinct_actions:\n",
        "        if tps[action] + fns[action] + fps[action] == 0:\n",
        "            action_f1s.append(0)\n",
        "        else:\n",
        "            action_f1s.append((1 + beta**2) * tps[action] / ((1 + beta**2) * tps[action] + beta**2 * fns[action] + fps[action]))\n",
        "    return sum(action_f1s) / len(action_f1s)\n",
        "\n",
        "\n",
        "def mouse_fbeta(solution: pd.DataFrame, submission: pd.DataFrame, beta: float = 1) -> float:\n",
        "    \"\"\"\n",
        "    Doctests:\n",
        "    >>> solution = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ... ])\n",
        "    >>> submission = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10},\n",
        "    ... ])\n",
        "    >>> mouse_fbeta(solution, submission)\n",
        "    1.0\n",
        "\n",
        "    >>> solution = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ... ])\n",
        "    >>> submission = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 0, 'stop_frame': 10}, # Wrong action\n",
        "    ... ])\n",
        "    >>> mouse_fbeta(solution, submission)\n",
        "    0.0\n",
        "\n",
        "    >>> solution = pd.DataFrame([\n",
        "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 15, 'stop_frame': 24, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ... ])\n",
        "    >>> submission = pd.DataFrame([\n",
        "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9},\n",
        "    ... ])\n",
        "    >>> \"%.12f\" % mouse_fbeta(solution, submission)\n",
        "    '0.500000000000'\n",
        "\n",
        "    >>> solution = pd.DataFrame([\n",
        "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 15, 'stop_frame': 24, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ...     {'video_id': 345, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9, 'lab_id': 2, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ...     {'video_id': 345, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 15, 'stop_frame': 24, 'lab_id': 2, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ... ])\n",
        "    >>> submission = pd.DataFrame([\n",
        "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9},\n",
        "    ... ])\n",
        "    >>> \"%.12f\" % mouse_fbeta(solution, submission)\n",
        "    '0.250000000000'\n",
        "\n",
        "    >>> # Overlapping solution events, one prediction matching both.\n",
        "    >>> solution = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 10, 'stop_frame': 20, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ... ])\n",
        "    >>> submission = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 20},\n",
        "    ... ])\n",
        "    >>> mouse_fbeta(solution, submission)\n",
        "    1.0\n",
        "\n",
        "    >>> solution = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 30, 'stop_frame': 40, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ... ])\n",
        "    >>> submission = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 40},\n",
        "    ... ])\n",
        "    >>> mouse_fbeta(solution, submission)\n",
        "    0.6666666666666666\n",
        "    \"\"\"\n",
        "    if len(solution) == 0 or len(submission) == 0:\n",
        "        raise ValueError('Missing solution or submission data')\n",
        "\n",
        "    expected_cols = ['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\n",
        "\n",
        "    for col in expected_cols:\n",
        "        if col not in solution.columns:\n",
        "            raise ValueError(f'Solution is missing column {col}')\n",
        "        if col not in submission.columns:\n",
        "            raise ValueError(f'Submission is missing column {col}')\n",
        "\n",
        "    solution: pl.DataFrame = pl.DataFrame(solution)\n",
        "    submission: pl.DataFrame = pl.DataFrame(submission)\n",
        "    assert (solution['start_frame'] <= solution['stop_frame']).all()\n",
        "    assert (submission['start_frame'] <= submission['stop_frame']).all()\n",
        "    solution_videos = set(solution['video_id'].unique())\n",
        "    # Need to align based on video IDs as we can't rely on the row IDs for handling public/private splits.\n",
        "    submission = submission.filter(pl.col('video_id').is_in(solution_videos))\n",
        "\n",
        "    solution = solution.with_columns(\n",
        "        pl.concat_str(\n",
        "            [\n",
        "                pl.col('video_id').cast(pl.Utf8),\n",
        "                pl.col('agent_id').cast(pl.Utf8),\n",
        "                pl.col('target_id').cast(pl.Utf8),\n",
        "                pl.col('action'),\n",
        "            ],\n",
        "            separator='_',\n",
        "        ).alias('label_key'),\n",
        "    )\n",
        "    submission = submission.with_columns(\n",
        "        pl.concat_str(\n",
        "            [\n",
        "                pl.col('video_id').cast(pl.Utf8),\n",
        "                pl.col('agent_id').cast(pl.Utf8),\n",
        "                pl.col('target_id').cast(pl.Utf8),\n",
        "                pl.col('action'),\n",
        "            ],\n",
        "            separator='_',\n",
        "        ).alias('prediction_key'),\n",
        "    )\n",
        "\n",
        "    lab_scores = []\n",
        "    for lab in solution['lab_id'].unique():\n",
        "        lab_solution = solution.filter(pl.col('lab_id') == lab).clone()\n",
        "        lab_videos = set(lab_solution['video_id'].unique())\n",
        "        lab_submission = submission.filter(pl.col('video_id').is_in(lab_videos)).clone()\n",
        "        lab_scores.append(single_lab_f1(lab_solution, lab_submission, beta=beta))\n",
        "\n",
        "    return sum(lab_scores) / len(lab_scores)\n",
        "\n",
        "\n",
        "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, beta: float = 1) -> float:\n",
        "    \"\"\"\n",
        "    F1 score for the MABe Challenge\n",
        "    \"\"\"\n",
        "    solution = solution.drop(row_id_column_name, axis='columns', errors='ignore')\n",
        "    submission = submission.drop(row_id_column_name, axis='columns', errors='ignore')\n",
        "    return mouse_fbeta(solution, submission, beta=beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68JjLSX1DhI_",
      "metadata": {
        "id": "68JjLSX1DhI_"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b91cb2c",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8b91cb2c"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import gc\n",
        "import itertools\n",
        "import json\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import traceback\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from metric import score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "428bb520",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "428bb520"
      },
      "outputs": [],
      "source": [
        "# const\n",
        "# INPUT_DIR = Path(\"/kaggle/input/MABe-mouse-behavior-detection\")\n",
        "# WORKING_DIR = Path(\"/kaggle/working\")\n",
        "\n",
        "INPUT_DIR = Path(\"/content/input\")\n",
        "WORKING_DIR = Path(\"/content/working\")\n",
        "\n",
        "\n",
        "TRAIN_TRACKING_DIR = INPUT_DIR / \"train_tracking\"\n",
        "TRAIN_ANNOTATION_DIR = INPUT_DIR / \"train_annotation\"\n",
        "TEST_TRACKING_DIR = INPUT_DIR / \"test_tracking\"\n",
        "\n",
        "\n",
        "INDEX_COLS = [\n",
        "    \"video_id\",\n",
        "    \"agent_mouse_id\",\n",
        "    \"target_mouse_id\",\n",
        "    \"video_frame\",\n",
        "]\n",
        "\n",
        "BODY_PARTS = [\n",
        "    \"ear_left\",\n",
        "    \"ear_right\",\n",
        "    \"nose\",\n",
        "    \"neck\",\n",
        "    \"body_center\",\n",
        "    \"lateral_left\",\n",
        "    \"lateral_right\",\n",
        "    \"hip_left\",\n",
        "    \"hip_right\",\n",
        "    \"tail_base\",\n",
        "    \"tail_tip\",\n",
        "]\n",
        "\n",
        "SELF_BEHAVIORS = [\n",
        "    \"biteobject\",\n",
        "    \"climb\",\n",
        "    \"dig\",\n",
        "    \"exploreobject\",\n",
        "    \"freeze\",\n",
        "    \"genitalgroom\",\n",
        "    \"huddle\",\n",
        "    \"rear\",\n",
        "    \"rest\",\n",
        "    \"run\",\n",
        "    \"selfgroom\",\n",
        "]\n",
        "\n",
        "PAIR_BEHAVIORS = [\n",
        "    \"allogroom\",\n",
        "    \"approach\",\n",
        "    \"attack\",\n",
        "    \"attemptmount\",\n",
        "    \"avoid\",\n",
        "    \"chase\",\n",
        "    \"chaseattack\",\n",
        "    \"defend\",\n",
        "    \"disengage\",\n",
        "    \"dominance\",\n",
        "    \"dominancegroom\",\n",
        "    \"dominancemount\",\n",
        "    \"ejaculate\",\n",
        "    \"escape\",\n",
        "    \"flinch\",\n",
        "    \"follow\",\n",
        "    \"intromit\",\n",
        "    \"mount\",\n",
        "    \"reciprocalsniff\",\n",
        "    \"shepherd\",\n",
        "    \"sniff\",\n",
        "    \"sniffbody\",\n",
        "    \"sniffface\",\n",
        "    \"sniffgenital\",\n",
        "    \"submit\",\n",
        "    \"tussle\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18f4383f",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "18f4383f"
      },
      "outputs": [],
      "source": [
        "# read data\n",
        "train_dataframe = pl.read_csv(INPUT_DIR / \"train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193a06eaGFlx",
      "metadata": {
        "id": "193a06eaGFlx"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t4_JedjWCwgi",
      "metadata": {
        "id": "t4_JedjWCwgi"
      },
      "source": [
        "## Behavior Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d9fb5c",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "71d9fb5c"
      },
      "outputs": [],
      "source": [
        "# preprocess behavior labels\n",
        "train_behavior_dataframe = (\n",
        "    train_dataframe\n",
        "    .filter(~((pl.col(\"lab_id\") == \"AdaptableSnail\") & (pl.col(\"frames_per_second\") == 25)))\n",
        "    .filter(pl.col(\"behaviors_labeled\").is_not_null())\n",
        "    .select(\n",
        "        pl.col(\"lab_id\"),\n",
        "        pl.col(\"video_id\"),\n",
        "        pl.col(\"behaviors_labeled\").map_elements(eval, return_dtype=pl.List(pl.Utf8)).alias(\"behaviors_labeled_list\"),\n",
        "    )\n",
        "    .explode(\"behaviors_labeled_list\")\n",
        "    .rename({\"behaviors_labeled_list\": \"behaviors_labeled_element\"})\n",
        "    .select(\n",
        "        pl.col(\"lab_id\"),\n",
        "        pl.col(\"video_id\"),\n",
        "        pl.col(\"behaviors_labeled_element\").str.split(\",\").list[0].str.replace_all(\"'\", \"\").alias(\"agent\"),\n",
        "        pl.col(\"behaviors_labeled_element\").str.split(\",\").list[1].str.replace_all(\"'\", \"\").alias(\"target\"),\n",
        "        pl.col(\"behaviors_labeled_element\").str.split(\",\").list[2].str.replace_all(\"'\", \"\").alias(\"behavior\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "train_self_behavior_dataframe = train_behavior_dataframe.filter(pl.col(\"behavior\").is_in(SELF_BEHAVIORS))\n",
        "train_pair_behavior_dataframe = train_behavior_dataframe.filter(pl.col(\"behavior\").is_in(PAIR_BEHAVIORS))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6858f216",
      "metadata": {
        "id": "6858f216"
      },
      "source": [
        "## Self\n",
        "\n",
        "\n",
        "*   Khoảng cách giữa các bộ phận cơ thể của agent (cm) - BODY PARTS\n",
        "\n",
        "*   Vận tốc ước tính của các bộ phận (cm/s)\n",
        "Tính vận tốc ước tính của ear_left, ear_right, tail_base trong các khoảng thời gian 500, 1000, 2000, 3000 ms.\n",
        "\n",
        "*   Độ duỗi (nose-tail base / earleft-right)\n",
        "\n",
        "*   Body angle (nose-body center vs body center-tail)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c024dc5",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5c024dc5",
        "outputId": "59c6d647-7729-4288-84e7-866751ec0a89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing self_features.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile self_features.py\n",
        "\n",
        "def make_self_features(\n",
        "    metadata: dict,\n",
        "    tracking: pl.DataFrame,\n",
        ") -> pl.DataFrame:\n",
        "    # tạo để đỡ phải truy cập nhiều lần\n",
        "    fps = metadata[\"frames_per_second\"]\n",
        "    pix_per_cm = metadata[\"pix_per_cm_approx\"]\n",
        "\n",
        "    # Helper\n",
        "    def get_window(period_ms):\n",
        "        return max(1, int(round(period_ms * fps / 1000.0)))\n",
        "\n",
        "\n",
        "    def body_parts_distance(body_part_1, body_part_2):\n",
        "        # Khoảng cách giữa các bộ phận cơ thể của agent (cm)\n",
        "        assert body_part_1 in BODY_PARTS\n",
        "        assert body_part_2 in BODY_PARTS\n",
        "        return (\n",
        "            (pl.col(f\"agent_x_{body_part_1}\") - pl.col(f\"agent_x_{body_part_2}\")).pow(2)\n",
        "            + (pl.col(f\"agent_y_{body_part_1}\") - pl.col(f\"agent_y_{body_part_2}\")).pow(2)\n",
        "        ).sqrt() / metadata[\"pix_per_cm_approx\"]\n",
        "\n",
        "    # thêm chức năng switch để chuyển giữa mean vafd std\n",
        "    def body_part_speed(body_part, period_ms, stat=\"mean\"):\n",
        "        assert body_part in BODY_PARTS\n",
        "        # Tốc độ ước tính của bộ phận (cm/s)\n",
        "        raw_speed = (\n",
        "            ((pl.col(f\"agent_x_{body_part}\").diff()).pow(2) + (pl.col(f\"agent_y_{body_part}\").diff()).pow(2)).sqrt()\n",
        "            / pix_per_cm * fps\n",
        "        )\n",
        "        w = get_window(period_ms)\n",
        "        if stat == \"mean\":\n",
        "            return raw_speed.rolling_mean(window_size=w, center=True, min_samples=1).fill_null(0.0)\n",
        "        elif stat == \"std\":\n",
        "            return raw_speed.rolling_std(window_size=w, center=True, min_samples=1).fill_null(0.0)\n",
        "        return raw_speed\n",
        "\n",
        "    def elongation():\n",
        "        # Độ giãn dài\n",
        "        d1 = body_parts_distance(\"nose\", \"tail_base\")\n",
        "        d2 = body_parts_distance(\"ear_left\", \"ear_right\")\n",
        "        return d1 / (d2 + 1e-06)\n",
        "\n",
        "    def body_angle():\n",
        "        # Góc cơ thể (deg)\n",
        "        v1x = pl.col(\"agent_x_nose\") - pl.col(\"agent_x_body_center\")\n",
        "        v1y = pl.col(\"agent_y_nose\") - pl.col(\"agent_y_body_center\")\n",
        "        v2x = pl.col(\"agent_x_tail_base\") - pl.col(\"agent_x_body_center\")\n",
        "        v2y = pl.col(\"agent_y_tail_base\") - pl.col(\"agent_y_body_center\")\n",
        "        return (v1x * v2x + v1y * v2y) / ((v1x.pow(2) + v1y.pow(2)).sqrt() * (v2x.pow(2) + v2y.pow(2)).sqrt() + 1e-06)\n",
        "\n",
        "    # [MỚI] Hàm tính Grooming Decouple\n",
        "    def grooming_decouple():\n",
        "        # Tốc độ tức thời của Mũi\n",
        "        s_nose = (((pl.col(\"agent_x_nose\").diff()).pow(2) + (pl.col(\"agent_y_nose\").diff()).pow(2)).sqrt() / pix_per_cm * fps)\n",
        "        # Tốc độ tức thời của Thân\n",
        "        s_body = (((pl.col(\"agent_x_body_center\").diff()).pow(2) + (pl.col(\"agent_y_body_center\").diff()).pow(2)).sqrt() / pix_per_cm * fps)\n",
        "\n",
        "        # Ratio: Mũi / (Thân + 0.5) -> Median 500ms\n",
        "        w = get_window(500)\n",
        "        ratio = (s_nose / (s_body + 0.5)).clip(0.0, 10.0)\n",
        "        return ratio.rolling_median(window_size=w, center=True, min_samples=1).fill_null(0.0)\n",
        "    # [MỚI] Hàm tính Nose Radial Jitter\n",
        "    def nose_radial_jitter():\n",
        "        # Khoảng cách Mũi - Thân\n",
        "        dist = body_parts_distance(\"nose\", \"body_center\")\n",
        "        # Std trong 500ms\n",
        "        w = get_window(500)\n",
        "        return dist.rolling_std(window_size=w, center=True, min_samples=1).fill_null(0.0)\n",
        "    # [MỚI] Hàm tính Vận tốc góc\n",
        "    def angular_velocity():\n",
        "        vec_x = pl.col(\"agent_x_nose\") - pl.col(\"agent_x_body_center\")\n",
        "        vec_y = pl.col(\"agent_y_nose\") - pl.col(\"agent_y_body_center\")\n",
        "        angle = pl.arctan2(vec_y, vec_x)\n",
        "        # Diff góc * FPS = Rad/s -> Smooth 300ms\n",
        "        w = get_window(300)\n",
        "        return (angle.diff().abs() * fps).rolling_mean(window_size=w, center=True, min_samples=1).fill_null(0.0)\n",
        "\n",
        "\n",
        "\n",
        "    n_mice = (\n",
        "        (metadata[\"mouse1_strain\"] is not None)\n",
        "        + (metadata[\"mouse2_strain\"] is not None)\n",
        "        + (metadata[\"mouse3_strain\"] is not None)\n",
        "        + (metadata[\"mouse4_strain\"] is not None)\n",
        "    )\n",
        "    start_frame = tracking.select(pl.col(\"video_frame\").min()).item()\n",
        "    end_frame = tracking.select(pl.col(\"video_frame\").max()).item()\n",
        "\n",
        "    result = []\n",
        "\n",
        "    pivot = tracking.pivot(\n",
        "        on=[\"bodypart\"],\n",
        "        index=[\"video_frame\", \"mouse_id\"],\n",
        "        values=[\"x\", \"y\"],\n",
        "    ).sort([\"mouse_id\", \"video_frame\"])\n",
        "    pivot_trackings = {mouse_id: pivot.filter(pl.col(\"mouse_id\") == mouse_id) for mouse_id in range(1, n_mice + 1)}\n",
        "\n",
        "    for agent_mouse_id in range(1, n_mice + 1):\n",
        "        result_element = pl.DataFrame(\n",
        "            {\n",
        "                \"video_id\": metadata[\"video_id\"],\n",
        "                \"agent_mouse_id\": agent_mouse_id,\n",
        "                \"target_mouse_id\": -1,\n",
        "                \"video_frame\": pl.arange(start_frame, end_frame + 1, eager=True),\n",
        "            },\n",
        "            schema={\n",
        "                \"video_id\": pl.Int32,\n",
        "                \"agent_mouse_id\": pl.Int8,\n",
        "                \"target_mouse_id\": pl.Int8,\n",
        "                \"video_frame\": pl.Int32,\n",
        "            },\n",
        "        )\n",
        "\n",
        "        pivot = pivot_trackings[agent_mouse_id].select(\n",
        "            pl.col(\"video_frame\"),\n",
        "            pl.exclude(\"video_frame\").name.prefix(\"agent_\"),\n",
        "        )\n",
        "        columns = pivot.columns\n",
        "        pivot = pivot.with_columns(\n",
        "            *[pl.lit(None).cast(pl.Float32).alias(f\"agent_x_{bp}\") for bp in BODY_PARTS if f\"agent_x_{bp}\" not in columns],\n",
        "            *[pl.lit(None).cast(pl.Float32).alias(f\"agent_y_{bp}\") for bp in BODY_PARTS if f\"agent_y_{bp}\" not in columns],\n",
        "        )\n",
        "\n",
        "        features = pivot.with_columns(\n",
        "            pl.lit(agent_mouse_id).alias(\"agent_mouse_id\"),\n",
        "            pl.lit(-1).alias(\"target_mouse_id\"),\n",
        "        ).select(\n",
        "            pl.col(\"video_frame\"),\n",
        "            pl.col(\"agent_mouse_id\"),\n",
        "            pl.col(\"target_mouse_id\"),\n",
        "            *[\n",
        "                body_parts_distance(body_part_1, body_part_2).alias(f\"aa__{body_part_1}__{body_part_2}__distance\")\n",
        "                for body_part_1, body_part_2 in itertools.combinations(BODY_PARTS, 2)\n",
        "            ],\n",
        "            *[\n",
        "                body_part_speed(body_part, period_ms).alias(f\"agent__{body_part}__speed_{period_ms}ms\")\n",
        "                for body_part, period_ms in itertools.product([\"ear_left\", \"ear_right\", \"tail_base\"], [500, 1000, 2000, 3000])\n",
        "            ],\n",
        "            # THÊM: body_center speed (Run/Walk)\n",
        "            *[\n",
        "                body_part_speed(\"body_center\", ms, stat=\"mean\").alias(\n",
        "                    f\"agent__body_center__speed_{ms}ms\"\n",
        "                )\n",
        "                for ms in [500, 1000, 2000]\n",
        "            ],\n",
        "            # THÊM: nose speed (Groom/Sniff)\n",
        "            *[\n",
        "                body_part_speed(\"nose\", ms, stat=\"mean\").alias(\n",
        "                    f\"agent__nose__speed_{ms}ms\"\n",
        "                )\n",
        "                for ms in [500, 1000]\n",
        "            ],\n",
        "            elongation().alias(\"agent__elongation\"),\n",
        "            body_angle().alias(\"agent__body_angle\"),\n",
        "            # các feature mới thêm\n",
        "            grooming_decouple().alias(\"agent__groom_decouple\"),\n",
        "            nose_radial_jitter().alias(\"agent__groom_nose_jitter\"),\n",
        "            angular_velocity().alias(\"agent__angular_velocity\"),\n",
        "        )\n",
        "\n",
        "        result_element = result_element.join(\n",
        "            features,\n",
        "            on=[\"video_frame\", \"agent_mouse_id\", \"target_mouse_id\"],\n",
        "            how=\"left\",\n",
        "        )\n",
        "        result.append(result_element)\n",
        "\n",
        "    return pl.concat(result, how=\"vertical\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12c5fd9a",
      "metadata": {
        "id": "12c5fd9a"
      },
      "source": [
        "## Pair\n",
        "\n",
        "*   Khoảng cách giữa các bộ phận cơ thể của agent–target (cm)  - BODY PARTS\n",
        "\n",
        "*   Vận tốc ước tính của các bộ phận của agent và target (cm/s)\n",
        "Tính vận tốc ước tính của ear_left, ear_right, tail_base trong các khoảng thời gian 500, 1000, 2000, 3000 ms cho cả agent và target.\n",
        "\n",
        "*   Độ duỗi của agent và target\n",
        "\n",
        "*   Góc cơ thể của agent và target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cab41dc0",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cab41dc0",
        "outputId": "a4a2084a-8053-4f74-b302-c43e5d8b1dae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing pair_features.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile pair_features.py\n",
        "\n",
        "def make_pair_features(\n",
        "    metadata: dict,\n",
        "    tracking: pl.DataFrame,\n",
        ") -> pl.DataFrame:\n",
        "    def body_parts_distance(agent_or_target_1, body_part_1, agent_or_target_2, body_part_2):\n",
        "        # Khoảng cách giữa các bộ phận cơ thể của agent-target (cm)\n",
        "        assert agent_or_target_1 == \"agent\" or agent_or_target_1 == \"target\"\n",
        "        assert agent_or_target_2 == \"agent\" or agent_or_target_2 == \"target\"\n",
        "        assert body_part_1 in BODY_PARTS\n",
        "        assert body_part_2 in BODY_PARTS\n",
        "        return (\n",
        "            (pl.col(f\"{agent_or_target_1}_x_{body_part_1}\") - pl.col(f\"{agent_or_target_2}_x_{body_part_2}\")).pow(2)\n",
        "            + (pl.col(f\"{agent_or_target_1}_y_{body_part_1}\") - pl.col(f\"{agent_or_target_2}_y_{body_part_2}\")).pow(2)\n",
        "        ).sqrt() / metadata[\"pix_per_cm_approx\"]\n",
        "\n",
        "    def body_part_speed(agent_or_target, body_part, period_ms):\n",
        "        # Tốc độ ước tính của bộ phận (cm/s)\n",
        "        assert agent_or_target == \"agent\" or agent_or_target == \"target\"\n",
        "        assert body_part in BODY_PARTS\n",
        "        window_frames = max(1, int(round(period_ms * metadata[\"frames_per_second\"] / 1000.0)))\n",
        "        return (\n",
        "            (\n",
        "                (pl.col(f\"{agent_or_target}_x_{body_part}\").diff()).pow(2)\n",
        "                + (pl.col(f\"{agent_or_target}_y_{body_part}\").diff()).pow(2)\n",
        "            ).sqrt()\n",
        "            / metadata[\"pix_per_cm_approx\"]\n",
        "            * metadata[\"frames_per_second\"]\n",
        "        ).rolling_mean(window_size=window_frames, center=True)\n",
        "\n",
        "    def elongation(agent_or_target):\n",
        "        # Độ giãn dài (cm)\n",
        "        assert agent_or_target == \"agent\" or agent_or_target == \"target\"\n",
        "        d1 = body_parts_distance(agent_or_target, \"nose\", agent_or_target, \"tail_base\")\n",
        "        d2 = body_parts_distance(agent_or_target, \"ear_left\", agent_or_target, \"ear_right\")\n",
        "        return d1 / (d2 + 1e-06)\n",
        "\n",
        "    def body_angle(agent_or_target):\n",
        "        # Góc cơ thể (deg)\n",
        "        assert agent_or_target == \"agent\" or agent_or_target == \"target\"\n",
        "        v1x = pl.col(f\"{agent_or_target}_x_nose\") - pl.col(f\"{agent_or_target}_x_body_center\")\n",
        "        v1y = pl.col(f\"{agent_or_target}_y_nose\") - pl.col(f\"{agent_or_target}_y_body_center\")\n",
        "        v2x = pl.col(f\"{agent_or_target}_x_tail_base\") - pl.col(f\"{agent_or_target}_x_body_center\")\n",
        "        v2y = pl.col(f\"{agent_or_target}_y_tail_base\") - pl.col(f\"{agent_or_target}_y_body_center\")\n",
        "        return (v1x * v2x + v1y * v2y) / ((v1x.pow(2) + v1y.pow(2)).sqrt() * (v2x.pow(2) + v2y.pow(2)).sqrt() + 1e-06)\n",
        "\n",
        "    def body_center_distance_rolling_agg(agg, period_ms):\n",
        "        # Đặc trưng tổng hợp khoảng cách trung tâm cơ thể di chuyển\n",
        "        assert agg in [\"mean\", \"std\", \"var\", \"min\", \"max\"] # Hàm tổng hợp\n",
        "        expr = body_parts_distance(\"agent\", \"body_center\", \"target\", \"body_center\")\n",
        "        window_frames = max(1, int(round(period_ms * metadata[\"frames_per_second\"] / 1000.0)))\n",
        "\n",
        "        if agg == \"mean\":\n",
        "            return expr.rolling_mean(window_size=window_frames, center=True, min_samples=1)\n",
        "        elif agg == \"std\":\n",
        "            return expr.rolling_std(window_size=window_frames, center=True, min_samples=1)\n",
        "        elif agg == \"var\":\n",
        "            return expr.rolling_var(window_size=window_frames, center=True, min_samples=1)\n",
        "        elif agg == \"min\":\n",
        "            return expr.rolling_min(window_size=window_frames, center=True, min_samples=1)\n",
        "        elif agg == \"max\":\n",
        "            return expr.rolling_max(window_size=window_frames, center=True, min_samples=1)\n",
        "        else:\n",
        "            raise ValueError()\n",
        "\n",
        "    # Add new feature\n",
        "    fps  = metadata[\"frames_per_second\"]\n",
        "    def body_center_distance():\n",
        "        # Khoảng cách tâm thân agent–target (cm)\n",
        "        return body_parts_distance(\"agent\", \"body_center\", \"target\", \"body_center\")\n",
        "    def body_center_radial_velocity(period_ms=300):\n",
        "        \"\"\"\n",
        "        Vận tốc hướng tâm (cm/s):\n",
        "        - < 0: lại gần (approach/chase)\n",
        "        - > 0: xa ra (avoid/escape)\n",
        "        \"\"\"\n",
        "        dist = body_center_distance()\n",
        "        window_frames = max(1, int(round(period_ms * fps / 1000.0)))\n",
        "        return dist.diff().rolling_mean(\n",
        "            window_size=window_frames,\n",
        "            center=True,\n",
        "            min_samples=1,\n",
        "        ).fill_null(0.0)\n",
        "    def relative_speed(period_ms=500):\n",
        "        \"\"\"\n",
        "        Chênh lệch tốc độ thân:\n",
        "        > 0: agent nhanh hơn\n",
        "        < 0: target nhanh hơn\n",
        "        \"\"\"\n",
        "        return (\n",
        "            body_part_speed(\"agent\", \"body_center\", period_ms)\n",
        "            - body_part_speed(\"target\", \"body_center\", period_ms)\n",
        "        )\n",
        "    def facing_score(agent_role, target_role, period_ms=500):\n",
        "        \"\"\"\n",
        "        Cosine giữa:\n",
        "        - hướng body_center→nose của agent\n",
        "        - vector agent_body_center → target_body_center\n",
        "        \"\"\"\n",
        "        vec_ag_x = pl.col(f\"{agent_role}_x_nose\") - pl.col(f\"{agent_role}_x_body_center\")\n",
        "        vec_ag_y = pl.col(f\"{agent_role}_y_nose\") - pl.col(f\"{agent_role}_y_body_center\")\n",
        "\n",
        "        vec_to_tg_x = pl.col(f\"{target_role}_x_body_center\") - pl.col(f\"{agent_role}_x_body_center\")\n",
        "        vec_to_tg_y = pl.col(f\"{target_role}_y_body_center\") - pl.col(f\"{agent_role}_y_body_center\")\n",
        "\n",
        "        dot = vec_ag_x * vec_to_tg_x + vec_ag_y * vec_to_tg_y\n",
        "        mag_ag = (vec_ag_x.pow(2) + vec_ag_y.pow(2)).sqrt()\n",
        "        mag_to = (vec_to_tg_x.pow(2) + vec_to_tg_y.pow(2)).sqrt()\n",
        "\n",
        "        cos_val = (dot / (mag_ag * mag_to + 1e-6)).clip(-1.0, 1.0)\n",
        "\n",
        "        window_frames = max(1, int(round(period_ms * fps / 1000.0)))\n",
        "        return cos_val.rolling_mean(\n",
        "            window_size=window_frames,\n",
        "            center=True,\n",
        "            min_samples=1,\n",
        "        ).fill_null(0.0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    n_mice = (\n",
        "        (metadata[\"mouse1_strain\"] is not None)\n",
        "        + (metadata[\"mouse2_strain\"] is not None)\n",
        "        + (metadata[\"mouse3_strain\"] is not None)\n",
        "        + (metadata[\"mouse4_strain\"] is not None)\n",
        "    )\n",
        "    start_frame = tracking.select(pl.col(\"video_frame\").min()).item()\n",
        "    end_frame = tracking.select(pl.col(\"video_frame\").max()).item()\n",
        "\n",
        "    result = []\n",
        "\n",
        "    pivot = tracking.pivot(\n",
        "        on=[\"bodypart\"],\n",
        "        index=[\"video_frame\", \"mouse_id\"],\n",
        "        values=[\"x\", \"y\"],\n",
        "    ).sort([\"mouse_id\", \"video_frame\"])\n",
        "    pivot_trackings = {mouse_id: pivot.filter(pl.col(\"mouse_id\") == mouse_id) for mouse_id in range(1, n_mice + 1)}\n",
        "\n",
        "    for agent_mouse_id, target_mouse_id in itertools.permutations(range(1, n_mice + 1), 2):\n",
        "        result_element = pl.DataFrame(\n",
        "            {\n",
        "                \"video_id\": metadata[\"video_id\"],\n",
        "                \"agent_mouse_id\": agent_mouse_id,\n",
        "                \"target_mouse_id\": target_mouse_id,\n",
        "                \"video_frame\": pl.arange(start_frame, end_frame + 1, eager=True),\n",
        "            },\n",
        "            schema={\n",
        "                \"video_id\": pl.Int32,\n",
        "                \"agent_mouse_id\": pl.Int8,\n",
        "                \"target_mouse_id\": pl.Int8,\n",
        "                \"video_frame\": pl.Int32,\n",
        "            },\n",
        "        )\n",
        "\n",
        "        merged_pivot = (\n",
        "            pivot_trackings[agent_mouse_id]\n",
        "            .select(\n",
        "                pl.col(\"video_frame\"),\n",
        "                pl.exclude(\"video_frame\").name.prefix(\"agent_\"),\n",
        "            )\n",
        "            .join(\n",
        "                pivot_trackings[target_mouse_id].select(\n",
        "                    pl.col(\"video_frame\"),\n",
        "                    pl.exclude(\"video_frame\").name.prefix(\"target_\"),\n",
        "                ),\n",
        "                on=\"video_frame\",\n",
        "                how=\"inner\",\n",
        "            )\n",
        "        )\n",
        "        columns = merged_pivot.columns\n",
        "        merged_pivot = merged_pivot.with_columns(\n",
        "            *[pl.lit(None).cast(pl.Float32).alias(f\"agent_x_{bp}\") for bp in BODY_PARTS if f\"agent_x_{bp}\" not in columns],\n",
        "            *[pl.lit(None).cast(pl.Float32).alias(f\"agent_y_{bp}\") for bp in BODY_PARTS if f\"agent_y_{bp}\" not in columns],\n",
        "            *[pl.lit(None).cast(pl.Float32).alias(f\"target_x_{bp}\") for bp in BODY_PARTS if f\"target_x_{bp}\" not in columns],\n",
        "            *[pl.lit(None).cast(pl.Float32).alias(f\"target_y_{bp}\") for bp in BODY_PARTS if f\"target_y_{bp}\" not in columns],\n",
        "        )\n",
        "\n",
        "        features = merged_pivot.with_columns(\n",
        "            pl.lit(agent_mouse_id).alias(\"agent_mouse_id\"),\n",
        "            pl.lit(target_mouse_id).alias(\"target_mouse_id\"),\n",
        "        ).select(\n",
        "            pl.col(\"video_frame\"),\n",
        "            pl.col(\"agent_mouse_id\"),\n",
        "            pl.col(\"target_mouse_id\"),\n",
        "            *[\n",
        "                body_parts_distance(\"agent\", agent_body_part, \"target\", target_body_part).alias(\n",
        "                    f\"at__{agent_body_part}__{target_body_part}__distance\"\n",
        "                )\n",
        "                for agent_body_part, target_body_part in itertools.product(BODY_PARTS, repeat=2)\n",
        "            ],\n",
        "            *[\n",
        "                body_part_speed(\"agent\", body_part, period_ms).alias(f\"agent__{body_part}__speed_{period_ms}ms\")\n",
        "                for body_part, period_ms in itertools.product([\"ear_left\", \"ear_right\", \"tail_base\"], [500, 1000, 2000, 3000])\n",
        "            ],\n",
        "            *[\n",
        "                body_part_speed(\"target\", body_part, period_ms).alias(f\"target__{body_part}__speed_{period_ms}ms\")\n",
        "                for body_part, period_ms in itertools.product([\"ear_left\", \"ear_right\", \"tail_base\"], [500, 1000, 2000, 3000])\n",
        "            ],\n",
        "            elongation(\"agent\").alias(\"agent__elongation\"),\n",
        "            elongation(\"target\").alias(\"target__elongation\"),\n",
        "            body_angle(\"agent\").alias(\"agent__body_angle\"),\n",
        "            body_angle(\"target\").alias(\"target__body_angle\"),\n",
        "\n",
        "            # 1) Speed body_center của agent & target (locomotion tương đối)\n",
        "            body_part_speed(\"agent\", \"body_center\", 500).alias(\"agent__body_center__speed_500ms\"),\n",
        "            body_part_speed(\"target\", \"body_center\", 500).alias(\"target__body_center__speed_500ms\"),\n",
        "\n",
        "            # 2) Khoảng cách tâm thân + rolling mean\n",
        "            body_center_distance().alias(\"at__body_center__distance\"),\n",
        "            body_center_distance_rolling_agg(\"mean\", 500).alias(\"at__body_center__distance_mean_500ms\"),\n",
        "            body_center_distance_rolling_agg(\"mean\", 1000).alias(\"at__body_center__distance_mean_1000ms\"),\n",
        "\n",
        "            # 3) Động học khoảng cách & speed tương đối\n",
        "            body_center_radial_velocity().alias(\"at__body_center__radial_velocity\"),\n",
        "            relative_speed().alias(\"pair__body_center_speed_diff_500ms\"),\n",
        "\n",
        "            # 4) Facing score: agent nhìn target & target nhìn agent\n",
        "            facing_score(\"agent\", \"target\").alias(\"pair__agent_facing_score\"),\n",
        "            facing_score(\"target\", \"agent\").alias(\"pair__target_facing_score\"),\n",
        "        )\n",
        "\n",
        "        result_element = result_element.join(\n",
        "            features,\n",
        "            on=[\"video_frame\", \"agent_mouse_id\", \"target_mouse_id\"],\n",
        "            how=\"left\",\n",
        "        )\n",
        "        result.append(result_element)\n",
        "\n",
        "    return pl.concat(result, how=\"vertical\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6607b00",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a6607b00",
        "outputId": "366e3628-aeb2-4dc3-8828-3c28742b2b3d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
            "/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:   26.6s\n",
            "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   42.1s\n",
            "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:   54.3s\n",
            "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:  1.5min\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 848 videos successfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 848 out of 848 | elapsed:  1.9min finished\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%run -i self_features.py\n",
        "%run -i pair_features.py\n",
        "\n",
        "def process_video(row):\n",
        "    \"\"\"Process a single video to extract self and pair features.\"\"\"\n",
        "    lab_id = row[\"lab_id\"]\n",
        "    video_id = row[\"video_id\"]\n",
        "\n",
        "    tracking_path = TRAIN_TRACKING_DIR / f\"{lab_id}/{video_id}.parquet\"\n",
        "    tracking = pl.read_parquet(tracking_path)\n",
        "\n",
        "    self_features = make_self_features(metadata=row, tracking=tracking)\n",
        "    pair_features = make_pair_features(metadata=row, tracking=tracking)\n",
        "\n",
        "    self_features.write_parquet(WORKING_DIR / \"self_features\" / f\"{video_id}.parquet\")\n",
        "    pair_features.write_parquet(WORKING_DIR / \"pair_features\" / f\"{video_id}.parquet\")\n",
        "\n",
        "    return video_id\n",
        "\n",
        "\n",
        "# make data\n",
        "(WORKING_DIR / \"self_features\").mkdir(exist_ok=True, parents=True)\n",
        "(WORKING_DIR / \"pair_features\").mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "rows = list(train_dataframe.filter(pl.col(\"behaviors_labeled\").is_not_null()).rows(named=True))\n",
        "results = joblib.Parallel(n_jobs=-1, verbose=5)(joblib.delayed(process_video)(row) for row in rows)\n",
        "\n",
        "print(f\"Processed {len(results)} videos successfully\")\n",
        "\n",
        "del rows, results\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v28df4dZHlW6",
      "metadata": {
        "id": "v28df4dZHlW6"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f8349ce",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5f8349ce"
      },
      "outputs": [],
      "source": [
        "def tune_threshold(oof_action, y_action):\n",
        "    thresholds = np.arange(0, 1.005, 0.005)\n",
        "    scores = [f1_score(y_action, (oof_action >= th), zero_division=0) for th in thresholds]\n",
        "    best_idx = np.argmax(scores)\n",
        "    return thresholds[best_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "677934fd",
      "metadata": {
        "id": "677934fd"
      },
      "source": [
        "*   XGB for perlab, per behavior\n",
        "*   Cross validation 3 Fold\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "098061a7",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "098061a7"
      },
      "outputs": [],
      "source": [
        "def train_validate(lab_id: str, behavior: str, indices: pl.DataFrame, features: pl.DataFrame, labels: pl.Series):\n",
        "    # Tạo đường dẫn thư mục để lưu kết quả\n",
        "    result_dir = WORKING_DIR / \"results\" / lab_id / behavior\n",
        "    # Tạo thư mục nếu không tồn tại (bao gồm cả thư mục cha)\n",
        "    result_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # Xử lý trường hợp tổng nhãn là 0 (không có mẫu dương nào)\n",
        "    if labels.sum() == 0:\n",
        "        # Lưu điểm F1 là 0\n",
        "        with open(result_dir / \"f1.txt\", \"w\") as f:\n",
        "            f.write(\"0.0\\n\")\n",
        "        # Tạo DataFrame kết quả với tất cả các giá trị dự đoán là 0\n",
        "        oof_prediction_dataframe = indices.with_columns(\n",
        "            pl.Series(\"fold\", [-1] * len(labels), dtype=pl.Int8),  # Số fold (-1 nghĩa là không sử dụng)\n",
        "            pl.Series(\"prediction\", [0.0] * len(labels), dtype=pl.Float32),  # Xác suất dự đoán\n",
        "            pl.Series(\"predicted_label\", [0] * len(labels), dtype=pl.Int8),  # Nhãn dự đoán\n",
        "        )\n",
        "        # Lưu kết quả dưới dạng parquet\n",
        "        oof_prediction_dataframe.write_parquet(result_dir / \"oof_predictions.parquet\")\n",
        "        return 0.0\n",
        "\n",
        "    # Khởi tạo mảng để lưu kết quả dự đoán Out-of-Fold\n",
        "    folds = np.ones(len(labels), dtype=np.int8) * -1  # Số fold mà mỗi mẫu thuộc về\n",
        "    oof_predictions = np.zeros(len(labels), dtype=np.float32)  # Xác suất dự đoán\n",
        "    oof_prediction_labels = np.zeros(len(labels), dtype=np.int8)  # Nhãn dự đoán (0 hoặc 1)\n",
        "\n",
        "    # Thực hiện phân tích chéo nhóm phân tầng 3 fold\n",
        "    # StratifiedGroupKFold giữ phân bố nhãn và đảm bảo cùng một nhóm (video_id) không bị chia thành nhiều fold\n",
        "    for fold, (train_idx, valid_idx) in enumerate(\n",
        "        StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=42).split(\n",
        "            X=features,  # Đặc trưng\n",
        "            y=labels,  # Nhãn\n",
        "            groups=indices.get_column(\"video_id\"),  # Tiêu chí nhóm (cùng ID video ở cùng một fold)\n",
        "        )\n",
        "    ):\n",
        "        # Tạo thư mục để lưu kết quả cho mỗi fold\n",
        "        result_dir_fold = result_dir / f\"fold_{fold}\"\n",
        "        result_dir_fold.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        # Chia thành dữ liệu huấn luyện và xác thực\n",
        "        X_train = features[train_idx]  # Đặc trưng huấn luyện\n",
        "        y_train = labels[train_idx]  # Nhãn huấn luyện\n",
        "        X_valid = features[valid_idx]  # Đặc trưng xác thực\n",
        "        y_valid = labels[valid_idx]  # Nhãn xác thực\n",
        "\n",
        "        # Tính trọng số để xử lý mất cân bằng lớp\n",
        "        # Số mẫu âm / Số mẫu dương = Trọng số cho mẫu dương\n",
        "        scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
        "\n",
        "        scale_pos_weight = float(np.clip(scale_pos_weight, 1.0, 50.0))\n",
        "\n",
        "        # Đặt các tham số XGBoost\n",
        "        params = {\n",
        "            \"objective\": \"binary:logistic\",  # Vấn đề phân loại nhị phân\n",
        "            \"eval_metric\": \"logloss\",  # Chỉ số đánh giá: mất mát logarit\n",
        "            \"device\": \"cuda\",  # Thiết bị sử dụng\n",
        "            \"tree_method\": \"hist\",  # Thuật toán dựa trên histogram nhanh\n",
        "            \"learning_rate\": 0.05,  # Tốc độ học\n",
        "            \"max_depth\": 6,  # Độ sâu tối đa của cây\n",
        "            \"min_child_weight\": 5,  # Trọng số tối thiểu của nút con\n",
        "            \"subsample\": 0.8,  # Tỷ lệ mẫu sử dụng cho mỗi cây\n",
        "            \"colsample_bytree\": 0.8,  # Tỷ lệ đặc trưng sử dụng cho mỗi cây\n",
        "            \"scale_pos_weight\": scale_pos_weight,  # Trọng số của mẫu dương\n",
        "            \"max_bin\": 64,  # Số bin của histogram\n",
        "            \"seed\": 42,  # Seed ngẫu nhiên\n",
        "        }\n",
        "\n",
        "        # Tạo ma trận dữ liệu cho XGBoost (dữ liệu huấn luyện là ma trận lượng tử hóa, dữ liệu xác thực là ma trận thông thường)\n",
        "        dtrain = xgb.QuantileDMatrix(X_train, label=y_train, feature_names=features.columns, max_bin=64)\n",
        "        dvalid = xgb.DMatrix(X_valid, label=y_valid, feature_names=features.columns)\n",
        "\n",
        "        # Từ điển để lưu kết quả đánh giá\n",
        "        evals_result = {}\n",
        "\n",
        "        # Đặt callback dừng sớm\n",
        "        # Dừng huấn luyện nếu mất mát logarit của dữ liệu xác thực không cải thiện trong 10 vòng\n",
        "        early_stopping_callback = xgb.callback.EarlyStopping(\n",
        "            rounds=10,  # Số vòng liên tiếp không cải thiện\n",
        "            metric_name=\"logloss\",  # Chỉ số cần giám sát\n",
        "            data_name=\"valid\",  # Tập dữ liệu cần giám sát\n",
        "            maximize=False,  # Chỉ số càng nhỏ càng tốt\n",
        "            save_best=True,  # Lưu mô hình tốt nhất\n",
        "        )\n",
        "\n",
        "        # Thực hiện huấn luyện mô hình\n",
        "        model = xgb.train(\n",
        "            params,  # Tham số siêu\n",
        "            dtrain=dtrain,  # Dữ liệu huấn luyện\n",
        "            num_boost_round=250,  # Số vòng tăng cường tối đa\n",
        "            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],  # Tập dữ liệu để đánh giá\n",
        "            callbacks=[early_stopping_callback],  # Callback\n",
        "            evals_result=evals_result,  # Nơi lưu kết quả đánh giá\n",
        "            verbose_eval=0,  # Tần suất ghi log (0 là không ghi)\n",
        "        )\n",
        "\n",
        "        # Thực hiện dự đoán trên dữ liệu xác thực (lấy giá trị xác suất)\n",
        "        fold_predictions = model.predict(dvalid)\n",
        "\n",
        "        # Điều chỉnh ngưỡng tối ưu để tối đa hóa điểm F1\n",
        "        threshold = tune_threshold(fold_predictions, y_valid)\n",
        "\n",
        "        # Lưu kết quả dự đoán Out-of-Fold\n",
        "        folds[valid_idx] = fold  # Số fold\n",
        "        oof_predictions[valid_idx] = fold_predictions  # Xác suất dự đoán\n",
        "        oof_prediction_labels[valid_idx] = (fold_predictions >= threshold).astype(np.int8)  # Nhị phân hóa bằng ngưỡng\n",
        "\n",
        "        # Lưu kết quả của fold này\n",
        "        # Lưu mô hình đã huấn luyện\n",
        "        model.save_model(result_dir_fold / \"model.json\")\n",
        "        # Lưu ngưỡng tối ưu\n",
        "        with open(result_dir_fold / \"threshold.txt\", \"w\") as f:\n",
        "            f.write(f\"{threshold}\\n\")\n",
        "\n",
        "        # Vẽ biểu đồ mức độ quan trọng của đặc trưng (top 20, theo gain)\n",
        "        xgb.plot_importance(model, max_num_features=20, importance_type=\"gain\", values_format=\"{v:.2f}\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(result_dir_fold / \"feature_importance.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Vẽ biểu đồ đường cong học (diễn biến mất mát logarit)\n",
        "        lgb.plot_metric(evals_result, metric=\"logloss\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(result_dir_fold / \"metric.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Giải phóng bộ nhớ\n",
        "        gc.collect()\n",
        "\n",
        "    # Tổng hợp kết quả dự đoán của tất cả các fold vào một DataFrame\n",
        "    oof_prediction_dataframe = indices.with_columns(\n",
        "        pl.Series(\"fold\", folds, dtype=pl.Int8),  # Số fold\n",
        "        pl.Series(\"prediction\", oof_predictions, dtype=pl.Float32),  # Xác suất dự đoán\n",
        "        pl.Series(\"predicted_label\", oof_prediction_labels, dtype=pl.Int8),  # Nhãn dự đoán\n",
        "    )\n",
        "\n",
        "    # Tính điểm F1 tổng thể\n",
        "    f1 = f1_score(labels, oof_prediction_labels, zero_division=0)\n",
        "    # Lưu điểm F1 vào tệp\n",
        "    with open(result_dir / \"f1.txt\", \"w\") as f:\n",
        "        f.write(f\"{f1}\\n\")\n",
        "\n",
        "    # Lưu DataFrame kết quả dự đoán\n",
        "    oof_prediction_dataframe.write_parquet(result_dir / \"oof_predictions.parquet\")\n",
        "\n",
        "    # Trả về điểm F1\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56a27d78",
      "metadata": {
        "id": "56a27d78"
      },
      "source": [
        "##Self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388bdfa3",
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "e643eba3d0e140d6ae13cf06ccf505f2"
          ]
        },
        "id": "388bdfa3",
        "outputId": "080a48de-a345-40c1-d10e-f2055093cfdd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e643eba3d0e140d6ae13cf06ccf505f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|           LAB           |   BEHAVIOR    | SAMPLES  | POSITIVE | FEATURES |    F1    | ELAPSED TIME  |\n",
            "|     AdaptableSnail      |     rear      |   660,348|    85,313|        77|      0.64|        0:00:29|\n",
            "|         CRIM13          |     rear      |   179,132|    12,042|        77|      0.36|        0:00:41|\n",
            "|         CRIM13          |   selfgroom   |   205,533|    14,472|        77|      0.36|        0:00:53|\n",
            "|      CalMS21_task1      | genitalgroom  |   102,445|     6,270|        77|      0.67|        0:01:02|\n",
            "|       ElegantMink       |     rear      |         0|         0|         0|         -|        0:01:02|\n",
            "|       ElegantMink       |   selfgroom   |         0|         0|         0|         -|        0:01:02|\n",
            "|       GroovyShrew       |     rear      |   899,280|    50,768|        77|      0.53|        0:01:36|\n",
            "|       GroovyShrew       |     rest      |   530,886|    87,573|        77|      0.68|        0:01:57|\n",
            "|       GroovyShrew       |   selfgroom   |   877,773|    22,893|        77|      0.31|        0:02:31|\n",
            "|       GroovyShrew       |     climb     |   295,943|     8,647|        77|      0.38|        0:02:46|\n",
            "|       GroovyShrew       |      dig      |   771,922|    31,267|        77|      0.41|        0:03:17|\n",
            "|       GroovyShrew       |      run      |   413,942|     1,732|        77|      0.13|        0:03:35|\n",
            "|   InvincibleJellyfish   |      dig      |   188,949|     6,768|        77|      0.29|        0:03:48|\n",
            "|   InvincibleJellyfish   |   selfgroom   |   308,326|     2,791|        77|      0.20|        0:04:04|\n",
            "|       LyricalHare       |    freeze     |   329,777|    31,660|        77|      0.52|        0:04:20|\n",
            "|       LyricalHare       |     rear      |   255,767|    18,953|        77|      0.32|        0:04:33|\n",
            "|     NiftyGoldfinch      |  biteobject   |   558,309|     2,326|        77|      0.03|        0:04:57|\n",
            "|     NiftyGoldfinch      |     climb     |   602,654|    51,687|        77|      0.57|        0:05:23|\n",
            "|     NiftyGoldfinch      |      dig      |   656,612|    40,735|        77|      0.51|        0:05:51|\n",
            "|     NiftyGoldfinch      | exploreobject |   558,859|     3,678|        77|      0.08|        0:06:15|\n",
            "|     NiftyGoldfinch      |     rear      |   602,308|    40,444|        77|      0.43|        0:06:42|\n",
            "|     NiftyGoldfinch      |   selfgroom   |   708,496|    34,621|        77|      0.45|        0:07:12|\n",
            "|     TranquilPanther     |     rear      | 1,234,586|    23,369|        77|      0.20|        0:08:02|\n",
            "|     TranquilPanther     |   selfgroom   | 1,021,199|     6,984|        77|      0.09|        0:08:44|\n",
            "|      UppityFerret       |    huddle     |   164,371|    24,148|        77|      0.65|        0:08:56|\n",
            "|      UppityFerret       |     rear      |         0|         0|         0|         -|        0:08:57|\n",
            "|      UppityFerret       |   selfgroom   |         0|         0|         0|         -|        0:08:57|\n"
          ]
        }
      ],
      "source": [
        "groups = train_self_behavior_dataframe.group_by(\"lab_id\", \"behavior\", maintain_order=True)\n",
        "total_groups = len(list(groups))\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "for idx, ((lab_id, behavior), group) in tqdm(enumerate(groups), total=total_groups):\n",
        "    if idx == 0:\n",
        "        tqdm.write(\n",
        "            f\"|{'LAB':^25}|{'BEHAVIOR':^15}|{'SAMPLES':^10}|{'POSITIVE':^10}|{'FEATURES':^10}|{'F1':^10}|{'ELAPSED TIME':^15}|\",\n",
        "            end=\"\\n\",\n",
        "        )\n",
        "\n",
        "    tqdm.write(f\"|{lab_id:^25}|{behavior:^15}|\", end=\"\")\n",
        "    index_list = []\n",
        "    feature_list = []\n",
        "    label_list = []\n",
        "\n",
        "    for row in group.rows(named=True):\n",
        "        video_id = row[\"video_id\"]\n",
        "        agent = row[\"agent\"]\n",
        "\n",
        "        agent_mouse_id = int(re.search(r\"mouse(\\d+)\", agent).group(1))\n",
        "\n",
        "        data = pl.scan_parquet(WORKING_DIR / \"self_features\" / f\"{video_id}.parquet\").filter(\n",
        "            (pl.col(\"agent_mouse_id\") == agent_mouse_id)\n",
        "        )\n",
        "        index = data.select(INDEX_COLS).collect(engine=\"streaming\")\n",
        "        feature = data.select(pl.exclude(INDEX_COLS)).collect(engine=\"streaming\")\n",
        "\n",
        "        # read annotation\n",
        "        annotation_path = TRAIN_ANNOTATION_DIR / lab_id / f\"{video_id}.parquet\"\n",
        "        if annotation_path.exists():\n",
        "            annotation = (\n",
        "                pl.scan_parquet(annotation_path)\n",
        "                .filter((pl.col(\"action\") == behavior) & (pl.col(\"agent_id\") == agent_mouse_id))\n",
        "                .collect()\n",
        "            )\n",
        "        else:\n",
        "            annotation = pl.DataFrame(\n",
        "                schema={\n",
        "                    \"agent_id\": pl.Int8,\n",
        "                    \"target_id\": pl.Int8,\n",
        "                    \"action\": str,\n",
        "                    \"start_frame\": pl.Int16,\n",
        "                    \"stop_frame\": pl.Int16,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        label_frames = set()\n",
        "        for annotation_row in annotation.rows(named=True):\n",
        "            label_frames.update(range(annotation_row[\"start_frame\"], annotation_row[\"stop_frame\"]))\n",
        "        label = index.select(pl.col(\"video_frame\").is_in(label_frames).cast(pl.Int8).alias(\"label\"))\n",
        "\n",
        "        if label.get_column(\"label\").sum() == 0:\n",
        "            continue\n",
        "\n",
        "        index_list.append(index)\n",
        "        feature_list.append(feature)\n",
        "        label_list.append(label.get_column(\"label\"))\n",
        "\n",
        "    if not index_list:\n",
        "        elapsed_time = datetime.timedelta(seconds=int(time.perf_counter() - start_time))\n",
        "        tqdm.write(f\"{0:>10,}|{0:>10,}|{0:>10,}|{'-':>10}|{str(elapsed_time):>15}|\", end=\"\\n\")\n",
        "        continue\n",
        "\n",
        "    indices = pl.concat(index_list, how=\"vertical\")\n",
        "    features = pl.concat(feature_list, how=\"vertical\")\n",
        "    labels = pl.concat(label_list, how=\"vertical\")\n",
        "\n",
        "    del index_list, feature_list, label_list\n",
        "    gc.collect()\n",
        "\n",
        "    tqdm.write(f\"{len(indices):>10,}|{labels.sum():>10,}|{len(features.columns):>10,}|\", end=\"\")\n",
        "\n",
        "    f1 = train_validate(lab_id, behavior, indices, features, labels)\n",
        "    tqdm.write(f\"{f1:>10.2f}|\", end=\"\")\n",
        "\n",
        "    elapsed_time = datetime.timedelta(seconds=int(time.perf_counter() - start_time))\n",
        "    tqdm.write(f\"{str(elapsed_time):>15}|\", end=\"\\n\")\n",
        "\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b977e906",
      "metadata": {
        "id": "b977e906"
      },
      "source": [
        "##Pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f796732",
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "91e2a461250a421eaab182d239862788"
          ]
        },
        "id": "0f796732",
        "outputId": "17f7d58a-d60d-486e-d6cd-6f182230d1f0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91e2a461250a421eaab182d239862788",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/104 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|           LAB           |   BEHAVIOR    | SAMPLES  | POSITIVE | FEATURES |    F1    | ELAPSED TIME  |\n",
            "|     AdaptableSnail      |   approach    | 1,524,536|     8,083|       158|      0.41|        0:01:20|\n",
            "|     AdaptableSnail      |    attack     |   637,048|     8,295|       158|      0.57|        0:01:54|\n",
            "|     AdaptableSnail      |     avoid     | 1,849,071|    14,820|       158|      0.32|        0:03:30|\n",
            "|     AdaptableSnail      |     chase     |   648,358|     3,575|       158|      0.54|        0:04:05|\n",
            "|     AdaptableSnail      |  chaseattack  |   317,584|     1,522|       158|      0.55|        0:04:25|\n",
            "|     AdaptableSnail      |    submit     |   424,181|     8,478|       158|      0.40|        0:04:52|\n",
            "|    BoisterousParrot     |   shepherd    | 9,504,414|    29,451|       158|      0.50|        0:12:47|\n",
            "|         CRIM13          |   approach    |   205,533|    10,178|       158|      0.49|        0:13:02|\n",
            "|         CRIM13          |    attack     |    71,906|     7,594|       158|      0.67|        0:13:10|\n",
            "|         CRIM13          |   disengage   |   205,533|    12,021|       158|      0.44|        0:13:25|\n",
            "|         CRIM13          |     mount     |   125,494|    10,340|       158|      0.67|        0:13:36|\n",
            "|         CRIM13          |     sniff     |   205,533|    30,218|       158|      0.68|        0:13:51|\n",
            "|  CalMS21_supplemental   |    attack     | 1,546,866|   179,193|       158|      0.83|        0:15:03|\n",
            "|  CalMS21_supplemental   |     sniff     | 5,179,395| 1,146,803|       158|      0.69|        0:19:31|\n",
            "|  CalMS21_supplemental   | sniffgenital  | 3,809,136|   531,890|       158|      0.49|        0:22:38|\n",
            "|  CalMS21_supplemental   |     mount     | 2,033,659|    75,346|       158|      0.66|        0:24:13|\n",
            "|  CalMS21_supplemental   |   approach    |   368,607|     9,639|       158|      0.49|        0:24:37|\n",
            "|  CalMS21_supplemental   |dominancemount | 1,234,065|    17,556|       158|      0.52|        0:25:36|\n",
            "|  CalMS21_supplemental   |   sniffbody   |   368,607|    51,978|       158|      0.61|        0:25:59|\n",
            "|  CalMS21_supplemental   |   sniffface   |   368,607|    29,017|       158|      0.72|        0:26:23|\n",
            "|  CalMS21_supplemental   | attemptmount  |   371,365|     2,630|       158|      0.16|        0:26:47|\n",
            "|  CalMS21_supplemental   |   intromit    | 1,057,613|   236,167|       158|      0.92|        0:27:39|\n",
            "|      CalMS21_task1      |   approach    |   308,898|     6,783|       158|      0.39|        0:28:00|\n",
            "|      CalMS21_task1      |     mount     |   528,763|    50,518|       158|      0.78|        0:28:31|\n",
            "|      CalMS21_task1      |   sniffbody   |   370,347|    48,288|       158|      0.52|        0:28:56|\n",
            "|      CalMS21_task1      |   sniffface   |   377,998|    22,538|       158|      0.56|        0:29:20|\n",
            "|      CalMS21_task1      | sniffgenital  |   656,994|    54,891|       158|      0.70|        0:29:55|\n",
            "|      CalMS21_task1      |    attack     |   276,601|    34,836|       158|      0.76|        0:30:15|\n",
            "|      CalMS21_task1      |   intromit    |    76,790|    11,339|       158|      0.75|        0:30:25|\n",
            "|      CalMS21_task1      |     sniff     |   569,289|   126,182|       158|      0.75|        0:30:56|\n",
            "|      CalMS21_task2      |    attack     |   765,949|    69,339|       158|      0.77|        0:31:37|\n",
            "|      CalMS21_task2      |     mount     |   784,342|    96,787|       158|      0.88|        0:32:19|\n",
            "|      CalMS21_task2      |     sniff     | 1,519,939|   332,234|       158|      0.80|        0:33:32|\n",
            "|     CautiousGiraffe     |   approach    |         0|         0|         0|         -|        0:33:33|\n",
            "|     CautiousGiraffe     |     chase     |    75,000|       858|       158|      0.44|        0:33:42|\n",
            "|     CautiousGiraffe     |    escape     |    60,000|     5,212|       158|      0.79|        0:33:51|\n",
            "|     CautiousGiraffe     |reciprocalsniff|   300,000|    23,298|       158|      0.73|        0:34:11|\n",
            "|     CautiousGiraffe     |   sniffbody   |   135,000|     3,008|       158|      0.35|        0:34:26|\n",
            "|     CautiousGiraffe     | sniffgenital  |   150,000|     5,300|       158|      0.53|        0:34:40|\n",
            "|     CautiousGiraffe     |     sniff     |   150,000|     4,933|       158|      0.47|        0:34:55|\n",
            "|      DeliriousFly       |     sniff     |   647,998|    49,388|       158|      0.47|        0:35:31|\n",
            "|      DeliriousFly       |    attack     |   647,998|    15,300|       158|      0.56|        0:36:08|\n",
            "|      DeliriousFly       |   dominance   |   647,998|    76,308|       158|      0.64|        0:36:45|\n",
            "|       ElegantMink       |    attack     |   111,956|     8,146|       158|      0.77|        0:36:57|\n",
            "|       ElegantMink       |   intromit    |   545,849|    48,602|       158|      0.71|        0:37:27|\n",
            "|       ElegantMink       |     mount     |   636,591|    15,273|       158|      0.39|        0:38:02|\n",
            "|       ElegantMink       |     sniff     |   883,208|    84,073|       158|      0.50|        0:38:47|\n",
            "|       ElegantMink       | sniffgenital  |         0|         0|         0|         -|        0:38:49|\n",
            "|       ElegantMink       | attemptmount  |   328,974|     2,159|       158|      0.14|        0:39:09|\n",
            "|       ElegantMink       |   allogroom   |   306,819|     4,286|       158|      0.17|        0:39:29|\n",
            "|       ElegantMink       |   ejaculate   |   162,670|     1,611|       158|      0.13|        0:39:43|\n",
            "|       GroovyShrew       |   intromit    |         0|         0|         0|         -|        0:39:44|\n",
            "|       GroovyShrew       |     mount     |         0|         0|         0|         -|        0:39:44|\n",
            "|       GroovyShrew       |     sniff     |   899,280|    82,623|       158|      0.65|        0:40:25|\n",
            "|       GroovyShrew       | sniffgenital  |   718,252|    15,113|       158|      0.53|        0:41:00|\n",
            "|       GroovyShrew       |   approach    |   829,428|    18,984|       158|      0.39|        0:41:40|\n",
            "|       GroovyShrew       |    defend     |   181,931|       935|       158|      0.07|        0:41:55|\n",
            "|       GroovyShrew       |    escape     |   715,426|     6,241|       158|      0.23|        0:42:30|\n",
            "|       GroovyShrew       | attemptmount  |    98,248|     1,149|       158|      0.31|        0:42:42|\n",
            "|   InvincibleJellyfish   |   allogroom   |   188,416|     3,294|       158|      0.39|        0:42:57|\n",
            "|   InvincibleJellyfish   |    attack     |   291,242|    21,800|       158|      0.68|        0:43:18|\n",
            "|   InvincibleJellyfish   |dominancegroom |   239,759|     4,724|       158|      0.25|        0:43:37|\n",
            "|   InvincibleJellyfish   |    escape     |   291,246|     3,374|       158|      0.08|        0:43:59|\n",
            "|   InvincibleJellyfish   |     sniff     |   565,839|    72,554|       158|      0.55|        0:44:32|\n",
            "|   InvincibleJellyfish   | sniffgenital  |   857,669|    32,606|       158|      0.44|        0:45:18|\n",
            "|      JovialSwallow      |    attack     |   252,000|    10,207|       158|      0.56|        0:45:38|\n",
            "|      JovialSwallow      |     chase     |    90,000|     1,082|       158|      0.04|        0:45:52|\n",
            "|      JovialSwallow      |     sniff     |   468,000|    88,173|       158|      0.59|        0:46:22|\n",
            "|       LyricalHare       |   approach    |   328,566|     8,053|       158|      0.20|        0:46:44|\n",
            "|       LyricalHare       |    attack     |   588,649|    72,608|       158|      0.77|        0:47:15|\n",
            "|       LyricalHare       |    defend     |   849,942|    62,735|       158|      0.57|        0:47:57|\n",
            "|       LyricalHare       |    escape     |   515,755|    41,307|       158|      0.67|        0:48:26|\n",
            "|       LyricalHare       |     sniff     |   297,196|    27,471|       158|      0.70|        0:48:46|\n",
            "|     NiftyGoldfinch      |   approach    |   708,496|    24,248|       158|      0.51|        0:49:24|\n",
            "|     NiftyGoldfinch      |    attack     |   582,788|    12,325|       158|      0.61|        0:49:57|\n",
            "|     NiftyGoldfinch      |     chase     |   326,896|     6,791|       158|      0.67|        0:50:20|\n",
            "|     NiftyGoldfinch      |    defend     |   708,496|    16,352|       158|      0.45|        0:50:59|\n",
            "|     NiftyGoldfinch      |    escape     |   708,496|    26,501|       158|      0.66|        0:51:38|\n",
            "|     NiftyGoldfinch      |    flinch     |   708,496|     1,677|       158|      0.09|        0:52:16|\n",
            "|     NiftyGoldfinch      |    follow     |   708,496|     5,567|       158|      0.43|        0:52:54|\n",
            "|     NiftyGoldfinch      |     sniff     |   708,496|    30,505|       158|      0.50|        0:53:33|\n",
            "|     NiftyGoldfinch      |   sniffface   |   708,496|    19,972|       158|      0.63|        0:54:11|\n",
            "|     NiftyGoldfinch      | sniffgenital  |   585,829|     2,245|       158|      0.26|        0:54:43|\n",
            "|     NiftyGoldfinch      |    tussle     |   212,582|     5,056|       158|      0.40|        0:55:02|\n",
            "|     PleasantMeerkat     |    attack     | 1,130,613|     3,923|       158|      0.12|        0:56:05|\n",
            "|     PleasantMeerkat     |     chase     |   698,633|     1,777|       158|      0.09|        0:56:48|\n",
            "|     PleasantMeerkat     |    escape     |   806,627|     2,029|       158|      0.17|        0:57:36|\n",
            "|     PleasantMeerkat     |    follow     |   833,637|    34,577|       158|      0.72|        0:58:25|\n",
            "|    ReflectiveManatee    |     sniff     |   606,038|   137,550|       158|      0.89|        0:59:02|\n",
            "|    ReflectiveManatee    |    attack     |   515,338|    37,716|       158|      0.84|        0:59:35|\n",
            "|     SparklingTapir      |    attack     |   279,533|    39,416|       158|      0.67|        0:59:57|\n",
            "|     SparklingTapir      |    defend     |   198,020|     9,960|       158|      0.62|        1:00:16|\n",
            "|     SparklingTapir      |    escape     |   180,020|     5,640|       158|      0.73|        1:00:35|\n",
            "|     SparklingTapir      |     mount     |   162,000|     9,327|       158|      0.84|        1:00:53|\n",
            "|     SparklingTapir      | sniffgenital  |         0|         0|         0|         -|        1:00:54|\n",
            "|     TranquilPanther     |   intromit    | 1,061,791|    51,856|       158|      0.55|        1:01:46|\n",
            "|     TranquilPanther     |     mount     | 1,061,791|    27,340|       158|      0.41|        1:02:42|\n",
            "|     TranquilPanther     |     sniff     | 1,234,586|    31,352|       158|      0.46|        1:03:44|\n",
            "|     TranquilPanther     | sniffgenital  | 1,183,089|    21,942|       158|      0.49|        1:04:45|\n",
            "|      UppityFerret       |reciprocalsniff|   328,742|    17,384|       158|      0.62|        1:05:13|\n",
            "|      UppityFerret       |     sniff     |         0|         0|         0|         -|        1:05:14|\n",
            "|      UppityFerret       | sniffgenital  |   613,716|    39,944|       158|      0.52|        1:05:54|\n",
            "|      UppityFerret       |   intromit    |         0|         0|         0|         -|        1:05:56|\n",
            "|      UppityFerret       |     mount     |         0|         0|         0|         -|        1:05:56|\n"
          ]
        }
      ],
      "source": [
        "groups = train_pair_behavior_dataframe.group_by(\"lab_id\", \"behavior\", maintain_order=True)\n",
        "total_groups = len(list(groups))\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "for idx, ((lab_id, behavior), group) in tqdm(enumerate(groups), total=total_groups):\n",
        "    if idx == 0:\n",
        "        tqdm.write(\n",
        "            f\"|{'LAB':^25}|{'BEHAVIOR':^15}|{'SAMPLES':^10}|{'POSITIVE':^10}|{'FEATURES':^10}|{'F1':^10}|{'ELAPSED TIME':^15}|\",\n",
        "            end=\"\\n\",\n",
        "        )\n",
        "\n",
        "    tqdm.write(f\"|{lab_id:^25}|{behavior:^15}|\", end=\"\")\n",
        "    index_list = []\n",
        "    feature_list = []\n",
        "    label_list = []\n",
        "\n",
        "    for row in group.rows(named=True):\n",
        "        video_id = row[\"video_id\"]\n",
        "        agent = row[\"agent\"]\n",
        "        target = row[\"target\"]\n",
        "\n",
        "        agent_mouse_id = int(re.search(r\"mouse(\\d+)\", agent).group(1))\n",
        "        target_mouse_id = int(re.search(r\"mouse(\\d+)\", target).group(1))\n",
        "\n",
        "        data = pl.scan_parquet(WORKING_DIR / \"pair_features\" / f\"{video_id}.parquet\").filter(\n",
        "            (pl.col(\"agent_mouse_id\") == agent_mouse_id) & (pl.col(\"target_mouse_id\") == target_mouse_id)\n",
        "        )\n",
        "        index = data.select(INDEX_COLS).collect(engine=\"streaming\")\n",
        "        feature = data.select(pl.exclude(INDEX_COLS)).collect(engine=\"streaming\")\n",
        "\n",
        "        # read annotation\n",
        "        annotation_path = TRAIN_ANNOTATION_DIR / lab_id / f\"{video_id}.parquet\"\n",
        "        if annotation_path.exists():\n",
        "            annotation = (\n",
        "                pl.scan_parquet(annotation_path)\n",
        "                .filter(\n",
        "                    (pl.col(\"action\") == behavior)\n",
        "                    & (pl.col(\"agent_id\") == agent_mouse_id)\n",
        "                    & (pl.col(\"target_id\") == target_mouse_id)\n",
        "                )\n",
        "                .collect()\n",
        "            )\n",
        "        else:\n",
        "            annotation = pl.DataFrame(\n",
        "                schema={\n",
        "                    \"agent_id\": pl.Int8,\n",
        "                    \"target_id\": pl.Int8,\n",
        "                    \"action\": str,\n",
        "                    \"start_frame\": pl.Int16,\n",
        "                    \"stop_frame\": pl.Int16,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        label_frames = set()\n",
        "        for annotation_row in annotation.rows(named=True):\n",
        "            label_frames.update(range(annotation_row[\"start_frame\"], annotation_row[\"stop_frame\"]))\n",
        "        label = index.select(pl.col(\"video_frame\").is_in(label_frames).cast(pl.Int8).alias(\"label\"))\n",
        "\n",
        "        if label.get_column(\"label\").sum() == 0:\n",
        "            continue\n",
        "\n",
        "        index_list.append(index)\n",
        "        feature_list.append(feature)\n",
        "        label_list.append(label.get_column(\"label\"))\n",
        "\n",
        "    if not index_list:\n",
        "        elapsed_time = datetime.timedelta(seconds=int(time.perf_counter() - start_time))\n",
        "        tqdm.write(f\"{0:>10,}|{0:>10,}|{0:>10,}|{'-':>10}|{str(elapsed_time):>15}|\", end=\"\\n\")\n",
        "        continue\n",
        "\n",
        "    indices = pl.concat(index_list, how=\"vertical\")\n",
        "    features = pl.concat(feature_list, how=\"vertical\")\n",
        "    labels = pl.concat(label_list, how=\"vertical\")\n",
        "\n",
        "    del index_list, feature_list, label_list\n",
        "    gc.collect()\n",
        "\n",
        "    tqdm.write(f\"{len(indices):>10,}|{labels.sum():>10,}|{len(features.columns):>10,}|\", end=\"\")\n",
        "\n",
        "    f1 = train_validate(lab_id, behavior, indices, features, labels)\n",
        "    tqdm.write(f\"{f1:>10.2f}|\", end=\"\")\n",
        "\n",
        "    elapsed_time = datetime.timedelta(seconds=int(time.perf_counter() - start_time))\n",
        "    tqdm.write(f\"{str(elapsed_time):>15}|\", end=\"\\n\")\n",
        "\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88a0a099",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "88a0a099",
        "outputId": "ede46f49-1699-4791-906d-4efca804b465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing robustify.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile robustify.py\n",
        "\n",
        "def robustify(submission: pl.DataFrame, dataset: pl.DataFrame, train_test: str = \"train\"):\n",
        "    traintest_directory = INPUT_DIR / f\"{train_test}_tracking\"\n",
        "\n",
        "    old_submission = submission.clone()\n",
        "    submission = submission.filter(pl.col(\"start_frame\") < pl.col(\"stop_frame\"))\n",
        "    if len(submission) != len(old_submission):\n",
        "        print(\"ERROR: Dropped frames with start >= stop\")\n",
        "\n",
        "    old_submission = submission.clone()\n",
        "    group_list = []\n",
        "    for _, group in submission.group_by(\"video_id\", \"agent_id\", \"target_id\"):\n",
        "        group = group.sort(\"start_frame\")\n",
        "        mask = np.ones(len(group), dtype=bool)\n",
        "        last_stop_frame = 0\n",
        "        for i, row in enumerate(group.rows(named=True)):\n",
        "            if row[\"start_frame\"] < last_stop_frame:\n",
        "                mask[i] = False\n",
        "            else:\n",
        "                last_stop_frame = row[\"stop_frame\"]\n",
        "        group_list.append(group.filter(pl.Series(\"mask\", mask)))\n",
        "\n",
        "    submission = pl.concat(group_list)\n",
        "\n",
        "    if len(submission) != len(old_submission):\n",
        "        print(\"ERROR: Dropped duplicate frames\")\n",
        "\n",
        "    # ========= 💡 3. MERGE SMALL GAPS GIỮA 2 EVENT CÙNG ACTION =========\n",
        "    MAX_GAP_FRAMES = 2\n",
        "\n",
        "    merged_groups = []\n",
        "    before_merge = len(submission)\n",
        "\n",
        "    # Group theo (video, agent, target), DUYỆT THEO THỨ TỰ GLOBAL\n",
        "    for _, group in submission.group_by([\"video_id\", \"agent_id\", \"target_id\"]):\n",
        "        group = group.sort(\"start_frame\")\n",
        "        rows = list(group.rows(named=True))\n",
        "        if not rows:\n",
        "            continue\n",
        "\n",
        "        current = dict(rows[0])\n",
        "        merged = []\n",
        "\n",
        "        for row in rows[1:]:\n",
        "            # Chỉ merge nếu CÙNG action và gap nhỏ\n",
        "            gap = row[\"start_frame\"] - current[\"stop_frame\"]\n",
        "            if (row[\"action\"] == current[\"action\"]) and (gap <= MAX_GAP_FRAMES):\n",
        "                if row[\"stop_frame\"] > current[\"stop_frame\"]:\n",
        "                    current[\"stop_frame\"] = row[\"stop_frame\"]\n",
        "            else:\n",
        "                merged.append(current)\n",
        "                current = dict(row)\n",
        "\n",
        "        merged.append(current)\n",
        "        merged_groups.append(pl.DataFrame(merged, schema=submission.schema))\n",
        "\n",
        "    if merged_groups:\n",
        "        submission = pl.concat(merged_groups)\n",
        "\n",
        "    after_merge = len(submission)\n",
        "    if after_merge != before_merge:\n",
        "        print(f\"INFO: Merged small gaps, events: {before_merge} -> {after_merge}\")\n",
        "    # ========= 💡 THÊM BƯỚC LỌC EVENT QUÁ NGẮN Ở ĐÂY =========\n",
        "    MIN_LEN_FRAMES = 3\n",
        "    SHORT_OK = [\"flinch\", \"ejaculate\", \"attemptmount\", \"allogroom\", \"tussle\"]\n",
        "\n",
        "    submission = submission.with_columns(\n",
        "        (pl.col(\"stop_frame\") - pl.col(\"start_frame\")).alias(\"length\")\n",
        "    )\n",
        "\n",
        "    before = len(submission)\n",
        "\n",
        "    submission = submission.filter(\n",
        "        pl.when(pl.col(\"action\").is_in(SHORT_OK))\n",
        "          .then(pl.col(\"length\") >= 1)                 # các action “nhấp nháy” không lọc theo min_len\n",
        "          .otherwise(pl.col(\"length\") >= MIN_LEN_FRAMES)  # còn lại phải dài >= 3 frame\n",
        "    )\n",
        "\n",
        "    submission = submission.drop(\"length\")\n",
        "    after = len(submission)\n",
        "\n",
        "    if after != before:\n",
        "        print(f\"INFO: Dropped {before - after} short events (action-dependent)\")\n",
        "\n",
        "    # =========================================================\n",
        "\n",
        "\n",
        "    s_list = []\n",
        "    for row in dataset.rows(named=True):\n",
        "        lab_id = row[\"lab_id\"]\n",
        "        video_id = row[\"video_id\"]\n",
        "        if row[\"behaviors_labeled\"] is None:\n",
        "            continue\n",
        "\n",
        "        if video_id in submission.get_column(\"video_id\").to_list():\n",
        "            continue\n",
        "\n",
        "        if isinstance(row[\"behaviors_labeled\"], str):\n",
        "            continue\n",
        "\n",
        "        print(f\"Video {video_id} has no predictions.\")\n",
        "\n",
        "        path = traintest_directory / f\"/{lab_id}/{video_id}.parquet\"\n",
        "        vid = pd.read_parquet(path)\n",
        "\n",
        "        vid_behaviors = json.loads(row[\"behaviors_labeled\"])\n",
        "        vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n",
        "        vid_behaviors = [b.split(\",\") for b in vid_behaviors]\n",
        "        vid_behaviors = pd.DataFrame(vid_behaviors, columns=[\"agent\", \"target\", \"action\"])\n",
        "\n",
        "        start_frame = vid.video_frame.min()\n",
        "        stop_frame = vid.video_frame.max() + 1\n",
        "\n",
        "        for (agent, target), actions in vid_behaviors.groupby([\"agent\", \"target\"]):\n",
        "            batch_length = int(np.ceil((stop_frame - start_frame) / len(actions)))\n",
        "            for i, action_row in enumerate(actions.itertuples(index=False)):\n",
        "                batch_start = start_frame + i * batch_length\n",
        "                batch_stop = min(batch_start + batch_length, stop_frame)\n",
        "                s_list.append((video_id, agent, target, action_row[\"action\"], batch_start, batch_stop))\n",
        "\n",
        "    if len(s_list) > 0:\n",
        "        submission = pd.concat(\n",
        "            [\n",
        "                submission,\n",
        "                pd.DataFrame(s_list, columns=[\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]),\n",
        "            ]\n",
        "        )\n",
        "        print(\"ERROR: Filled empty videos\")\n",
        "\n",
        "    return submission"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5728781",
      "metadata": {
        "id": "e5728781"
      },
      "source": [
        "## Tổng hợp các giá trị dự đoán trên dữ liệu kiểm chứng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a1ccc3c",
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "6722404137804da3976ebdfc6d923109"
          ]
        },
        "id": "9a1ccc3c",
        "outputId": "ac588550-1de3-4b7f-c3b4-862e3fd85915"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6722404137804da3976ebdfc6d923109",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1378 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERROR: Dropped frames with start >= stop\n",
            "INFO: Merged small gaps, events: 433232 -> 329016\n",
            "INFO: Dropped 115973 short events (action-dependent)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Danh sách để lưu kết quả dự đoán Out-of-Fold của từng nhóm\n",
        "group_oof_predictions = []\n",
        "\n",
        "# Nhóm dữ liệu theo lab_id, video_id, agent, target\n",
        "# maintain_order=True để giữ nguyên thứ tự ban đầu\n",
        "groups = train_behavior_dataframe.group_by(\"lab_id\", \"video_id\", \"agent\", \"target\", maintain_order=True)\n",
        "\n",
        "# Thực hiện xử lý cho từng nhóm (hiển thị thanh tiến trình)\n",
        "for (lab_id, video_id, agent, target), group in tqdm(groups, total=len(list(groups))):\n",
        "    # Trích xuất ID chuột từ agent (chủ thể hành động)\n",
        "    # Ví dụ: \"mouse1\" → 1\n",
        "    agent_mouse_id = int(re.search(r\"mouse(\\d+)\", agent).group(1))\n",
        "\n",
        "    # Trích xuất ID chuột từ target (đối tượng hành động)\n",
        "    # Nếu là \"self\" (chính nó) thì là -1, ngược lại lấy ID chuột\n",
        "    target_mouse_id = -1 if target == \"self\" else int(re.search(r\"mouse(\\d+)\", target).group(1))\n",
        "\n",
        "    # Danh sách để lưu kết quả dự đoán của từng hành vi trong nhóm này\n",
        "    prediction_dataframe_list = []\n",
        "\n",
        "    # Xử lý từng hàng (từng hành vi) trong nhóm\n",
        "    for row in group.rows(named=True):\n",
        "        behavior = row[\"behavior\"]  # Loại hành vi (ví dụ: \"grooming\", \"sniffing\")\n",
        "\n",
        "        # Xây dựng đường dẫn đến tệp kết quả dự đoán OOF của hành vi này\n",
        "        oof_path = WORKING_DIR / \"results\" / lab_id / behavior / \"oof_predictions.parquet\"\n",
        "\n",
        "        # Bỏ qua nếu tệp không tồn tại\n",
        "        if not oof_path.exists():\n",
        "            continue\n",
        "\n",
        "        # Đọc kết quả dự đoán và lọc theo video_id, agent, target tương ứng\n",
        "        prediction = (\n",
        "            pl.scan_parquet(oof_path)  # Đọc chậm (hiệu quả bộ nhớ)\n",
        "            .filter(\n",
        "                (pl.col(\"video_id\") == video_id)  # ID video khớp\n",
        "                & (pl.col(\"agent_mouse_id\") == agent_mouse_id)  # Chủ thể hành động khớp\n",
        "                & (pl.col(\"target_mouse_id\") == target_mouse_id)  # Đối tượng hành động khớp\n",
        "            )\n",
        "            .select(\n",
        "                *INDEX_COLS,  # Chọn các cột chỉ mục\n",
        "                # Tính điểm cho hành vi này bằng cách nhân xác suất dự đoán với nhãn dự đoán\n",
        "                # Nếu nhãn dự đoán là 0 thì điểm cũng là 0\n",
        "                (pl.col(\"prediction\") * pl.col(\"predicted_label\")).alias(behavior)\n",
        "            )\n",
        "            .collect()  # Thực sự đọc và thực thi dữ liệu\n",
        "        )\n",
        "\n",
        "        # Bỏ qua nếu không có hàng nào sau khi lọc (không có dữ liệu phù hợp)\n",
        "        if len(prediction) == 0:\n",
        "            continue\n",
        "\n",
        "        # Thêm kết quả dự đoán của hành vi này vào danh sách\n",
        "        prediction_dataframe_list.append(prediction)\n",
        "\n",
        "    # Bỏ qua nếu không có kết quả dự đoán nào cho nhóm này\n",
        "    if not prediction_dataframe_list:\n",
        "        continue\n",
        "\n",
        "    # Kết hợp các kết quả dự đoán của nhiều hành vi theo chiều ngang\n",
        "    # how=\"align\" để sắp xếp và kết hợp dựa trên các cột chỉ mục\n",
        "    prediction_dataframe = pl.concat(prediction_dataframe_list, how=\"align\")\n",
        "\n",
        "    # Lấy tên các cột không phải là cột chỉ mục (tên từng hành vi)\n",
        "    cols = prediction_dataframe.select(pl.exclude(INDEX_COLS)).columns\n",
        "\n",
        "    # Chọn hành vi có độ tin cậy cao nhất cho mỗi khung hình\n",
        "    prediction_labels_dataframe = prediction_dataframe.with_columns(\n",
        "        pl.struct(pl.exclude(INDEX_COLS))  # Tập hợp điểm của tất cả các hành vi thành một cấu trúc\n",
        "        .map_elements(\n",
        "            # Hàm được thực thi cho mỗi hàng\n",
        "            lambda row: \"none\" if sum(row.values()) == 0  # Nếu tất cả các điểm là 0 thì \"none\"\n",
        "                       else (cols[np.argmax(list(row.values()))]),  # Chọn hành vi có điểm cao nhất\n",
        "            return_dtype=pl.String,\n",
        "        )\n",
        "        .alias(\"prediction\")  # Đặt tên cột mới là \"prediction\"\n",
        "    ).select(INDEX_COLS + [\"prediction\"])  # Chọn chỉ các cột chỉ mục và cột dự đoán\n",
        "\n",
        "    # Gom các hành vi giống nhau liên tiếp lại và xác định khung hình bắt đầu và kết thúc của hành vi\n",
        "    group_oof_prediction = (\n",
        "        prediction_labels_dataframe\n",
        "        .filter((pl.col(\"prediction\") != pl.col(\"prediction\").shift(1)))  # Chỉ trích xuất các hành vi khác với hàng trước (điểm biên)\n",
        "        .with_columns(pl.col(\"video_frame\").shift(-1).alias(\"stop_frame\"))  # Đặt điểm biên tiếp theo làm khung hình kết thúc\n",
        "        .filter(pl.col(\"prediction\") != \"none\")  # Loại trừ \"none\" (không có hành vi)\n",
        "        .select(\n",
        "            pl.col(\"video_id\"),  # ID video\n",
        "            (\"mouse\" + pl.col(\"agent_mouse_id\").cast(str)).alias(\"agent_id\"),  # Chuyển đổi sang định dạng \"mouse1\"\n",
        "            # Nếu target_mouse_id là -1 thì là \"self\", ngược lại chuyển đổi sang định dạng \"mouse2\"\n",
        "            pl.when(pl.col(\"target_mouse_id\") == -1)\n",
        "            .then(pl.lit(\"self\"))\n",
        "            .otherwise(\"mouse\" + pl.col(\"target_mouse_id\").cast(str))\n",
        "            .alias(\"target_id\"),\n",
        "            pl.col(\"prediction\").alias(\"action\"),  # Tên hành vi\n",
        "            pl.col(\"video_frame\").alias(\"start_frame\"),  # Khung hình bắt đầu\n",
        "            pl.col(\"stop_frame\"),  # Khung hình kết thúc\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Thêm kết quả dự đoán của nhóm này vào danh sách\n",
        "    group_oof_predictions.append(group_oof_prediction)\n",
        "\n",
        "%run -i robustify.py\n",
        "\n",
        "oof_predictions = pl.concat(group_oof_predictions, how=\"vertical\")\n",
        "oof_predictions = robustify(oof_predictions, train_dataframe, train_test=\"train\")\n",
        "oof_predictions.with_row_index(\"row_id\").write_csv(WORKING_DIR / \"oof_predictions.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "863524d2",
      "metadata": {
        "id": "863524d2"
      },
      "source": [
        "##Tính điểm (score) dựa trên dữ liệu kiểm chứng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f208b832",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f208b832",
        "outputId": "f3558a20-489c-4c6e-e256-b6c9c7f43181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PERFORMANCE METRICS\n",
            "============================================================\n",
            "Overall F1 Score: 0.5197\n",
            "Total predictions: 213043\n",
            "  - Single behaviors: 44547\n",
            "  - Pair behaviors: 168496\n",
            "\n",
            "Per-Action Performance Summary:\n",
            "------------------------------------------------------------\n",
            "Action               Mode       Count      Avg F1    \n",
            "------------------------------------------------------------\n",
            "allogroom            pair       17         0.1545    \n",
            "approach             pair       258        0.3987    \n",
            "attack               pair       369        0.5933    \n",
            "attemptmount         pair       42         0.0976    \n",
            "avoid                pair       95         0.2875    \n",
            "biteobject           single     16         0.0171    \n",
            "chase                pair       83         0.2645    \n",
            "chaseattack          pair       12         0.3808    \n",
            "climb                single     30         0.4074    \n",
            "defend               pair       64         0.4042    \n",
            "dig                  single     60         0.3446    \n",
            "disengage            pair       20         0.4401    \n",
            "dominance            pair       6          0.6172    \n",
            "dominancegroom       pair       14         0.2355    \n",
            "dominancemount       pair       63         0.4508    \n",
            "ejaculate            pair       3          0.4214    \n",
            "escape               pair       125        0.3168    \n",
            "exploreobject        single     17         0.0776    \n",
            "flinch               pair       22         0.0794    \n",
            "follow               pair       53         0.4516    \n",
            "freeze               single     9          0.3450    \n",
            "genitalgroom         single     17         0.6654    \n",
            "huddle               single     11         0.4601    \n",
            "intromit             pair       81         0.6864    \n",
            "mount                pair       247        0.6280    \n",
            "rear                 single     137        0.4116    \n",
            "reciprocalsniff      pair       42         0.6755    \n",
            "rest                 single     21         0.4364    \n",
            "run                  single     19         0.1483    \n",
            "selfgroom            single     108        0.2771    \n",
            "shepherd             pair       16         0.4269    \n",
            "sniff                pair       622        0.6304    \n",
            "sniffbody            pair       109        0.4877    \n",
            "sniffface            pair       119        0.5425    \n",
            "sniffgenital         pair       462        0.4846    \n",
            "submit               pair       23         0.2633    \n",
            "tussle               pair       6          0.2525    \n",
            "\n",
            "Single behaviors: 11 actions, Avg F1: 0.3264\n",
            "Pair behaviors: 26 actions, Avg F1: 0.4105\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def compute_validation_metrics(submission, verbose=True):\n",
        "    \"\"\"Compute and display validation metrics for single vs pair behaviors.\"\"\"\n",
        "    # solution_df\n",
        "    dataset = pl.read_csv(INPUT_DIR / \"train.csv\").to_pandas()\n",
        "\n",
        "    solution = []\n",
        "    for _, row in dataset.iterrows():\n",
        "        lab_id = row[\"lab_id\"]\n",
        "        if lab_id.startswith(\"MABe22\"):\n",
        "            continue\n",
        "\n",
        "        video_id = row[\"video_id\"]\n",
        "        path = TRAIN_ANNOTATION_DIR / lab_id / f\"{video_id}.parquet\"\n",
        "        try:\n",
        "            annot = pd.read_parquet(path)\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "\n",
        "        annot[\"lab_id\"] = lab_id\n",
        "        annot[\"video_id\"] = video_id\n",
        "        annot[\"behaviors_labeled\"] = row[\"behaviors_labeled\"]\n",
        "        annot[\"target_id\"] = np.where(\n",
        "            annot.target_id != annot.agent_id, annot[\"target_id\"].apply(lambda s: f\"mouse{s}\"), \"self\"\n",
        "        )\n",
        "        annot[\"agent_id\"] = annot[\"agent_id\"].apply(lambda s: f\"mouse{s}\")\n",
        "        solution.append(annot)\n",
        "\n",
        "    solution = pd.concat(solution)\n",
        "\n",
        "    try:\n",
        "        # Separate single and pair behaviors\n",
        "        submission_single = submission[submission[\"target_id\"] == \"self\"].copy()\n",
        "        submission_pair = submission[submission[\"target_id\"] != \"self\"].copy()\n",
        "\n",
        "        # Filter solution to match submission videos\n",
        "        solution_videos = set(submission[\"video_id\"].unique())\n",
        "        solution = solution[solution[\"video_id\"].isin(solution_videos)]\n",
        "\n",
        "        if len(solution) == 0:\n",
        "            return\n",
        "\n",
        "        # Compute overall F1 score\n",
        "        overall_f1 = score(solution, submission, \"row_id\", beta=1.0)\n",
        "        print(f\"\\n{'=' * 60}\")\n",
        "        print(\"PERFORMANCE METRICS\")\n",
        "        print(f\"{'=' * 60}\")\n",
        "        print(f\"Overall F1 Score: {overall_f1:.4f}\")\n",
        "        print(f\"Total predictions: {len(submission)}\")\n",
        "        print(f\"  - Single behaviors: {len(submission_single)}\")\n",
        "        print(f\"  - Pair behaviors: {len(submission_pair)}\")\n",
        "\n",
        "        # Compute per-action F1 scores using existing scoring function\n",
        "        solution_pl = pl.DataFrame(solution)\n",
        "        submission_pl = pl.DataFrame(submission)\n",
        "\n",
        "        # Add label_key and prediction_key\n",
        "        solution_pl = solution_pl.with_columns(\n",
        "            pl.concat_str(\n",
        "                [\n",
        "                    pl.col(\"video_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"agent_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"target_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"action\"),\n",
        "                ],\n",
        "                separator=\"_\",\n",
        "            ).alias(\"label_key\"),\n",
        "        )\n",
        "        submission_pl = submission_pl.with_columns(\n",
        "            pl.concat_str(\n",
        "                [\n",
        "                    pl.col(\"video_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"agent_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"target_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"action\"),\n",
        "                ],\n",
        "                separator=\"_\",\n",
        "            ).alias(\"prediction_key\"),\n",
        "        )\n",
        "\n",
        "        # Group by action and compute metrics\n",
        "        action_stats = defaultdict(lambda: {\"single\": {\"count\": 0, \"f1\": 0.0}, \"pair\": {\"count\": 0, \"f1\": 0.0}})\n",
        "\n",
        "        for lab in solution_pl[\"lab_id\"].unique():\n",
        "            lab_solution = solution_pl.filter(pl.col(\"lab_id\") == lab).clone()\n",
        "            lab_videos = set(lab_solution[\"video_id\"].unique())\n",
        "            lab_submission = submission_pl.filter(pl.col(\"video_id\").is_in(lab_videos)).clone()\n",
        "\n",
        "            # Compute per-action F1 using same logic as single_lab_f1\n",
        "            label_frames = defaultdict(set)\n",
        "            prediction_frames = defaultdict(set)\n",
        "\n",
        "            for row in lab_solution.to_dicts():\n",
        "                label_frames[row[\"label_key\"]].update(range(row[\"start_frame\"], row[\"stop_frame\"]))\n",
        "\n",
        "            for row in lab_submission.to_dicts():\n",
        "                key = row[\"prediction_key\"]\n",
        "                prediction_frames[key].update(range(row[\"start_frame\"], row[\"stop_frame\"]))\n",
        "\n",
        "            for key in set(list(label_frames.keys()) + list(prediction_frames.keys())):\n",
        "                action = key.split(\"_\")[-1]\n",
        "                mode = \"single\" if \"self\" in key else \"pair\"\n",
        "\n",
        "                pred_frames = prediction_frames.get(key, set())\n",
        "                label_frames_set = label_frames.get(key, set())\n",
        "\n",
        "                tp = len(pred_frames & label_frames_set)\n",
        "                fn = len(label_frames_set - pred_frames)\n",
        "                fp = len(pred_frames - label_frames_set)\n",
        "\n",
        "                if tp + fn + fp > 0:\n",
        "                    f1 = (1 + 1**2) * tp / ((1 + 1**2) * tp + 1**2 * fn + fp)\n",
        "                    action_stats[action][mode][\"count\"] += 1\n",
        "                    action_stats[action][mode][\"f1\"] += f1\n",
        "\n",
        "        # Print per-action summary\n",
        "        print(\"\\nPer-Action Performance Summary:\")\n",
        "        print(f\"{'-' * 60}\")\n",
        "        print(f\"{'Action':<20} {'Mode':<10} {'Count':<10} {'Avg F1':<10}\")\n",
        "        print(f\"{'-' * 60}\")\n",
        "\n",
        "        for action in sorted(action_stats.keys()):\n",
        "            for mode in [\"single\", \"pair\"]:\n",
        "                stats = action_stats[action][mode]\n",
        "                if stats[\"count\"] > 0:\n",
        "                    avg_f1 = stats[\"f1\"] / stats[\"count\"]\n",
        "                    print(f\"{action:<20} {mode:<10} {stats['count']:<10} {avg_f1:<10.4f}\")\n",
        "\n",
        "        # Summary by mode\n",
        "        single_actions = [a for a in action_stats.keys() if action_stats[a][\"single\"][\"count\"] > 0]\n",
        "        pair_actions = [a for a in action_stats.keys() if action_stats[a][\"pair\"][\"count\"] > 0]\n",
        "\n",
        "        if single_actions:\n",
        "            single_avg_f1 = np.mean(\n",
        "                [\n",
        "                    action_stats[a][\"single\"][\"f1\"] / action_stats[a][\"single\"][\"count\"]\n",
        "                    for a in single_actions\n",
        "                    if action_stats[a][\"single\"][\"count\"] > 0\n",
        "                ]\n",
        "            )\n",
        "            print(f\"\\nSingle behaviors: {len(single_actions)} actions, Avg F1: {single_avg_f1:.4f}\")\n",
        "\n",
        "        if pair_actions:\n",
        "            pair_avg_f1 = np.mean(\n",
        "                [\n",
        "                    action_stats[a][\"pair\"][\"f1\"] / action_stats[a][\"pair\"][\"count\"]\n",
        "                    for a in pair_actions\n",
        "                    if action_stats[a][\"pair\"][\"count\"] > 0\n",
        "                ]\n",
        "            )\n",
        "            print(f\"Pair behaviors: {len(pair_actions)} actions, Avg F1: {pair_avg_f1:.4f}\")\n",
        "\n",
        "        print(f\"{'=' * 60}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            error_msg = str(e)\n",
        "            if len(error_msg) > 200:\n",
        "                error_msg = error_msg[:200] + \"...\"\n",
        "            print(f\"\\nWarning: Could not compute validation metrics: {error_msg}\")\n",
        "            if verbose:\n",
        "                print(f\"Traceback: {traceback.format_exc()[:300]}\")\n",
        "\n",
        "compute_validation_metrics(submission=pd.read_csv(WORKING_DIR / \"oof_predictions.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wX5pTLJ0fiEI",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wX5pTLJ0fiEI",
        "outputId": "3cfcd75c-b967-49f4-d972-4411d5a0dbf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PERFORMANCE METRICS\n",
            "============================================================\n",
            "Overall F1 Score: 0.5197\n",
            "Total predictions: 213043\n",
            "  - Single behaviors: 44547\n",
            "  - Pair behaviors: 168496\n",
            "\n",
            "Per-Action Performance Summary:\n",
            "------------------------------------------------------------\n",
            "Action               Mode       Count      Avg F1    \n",
            "------------------------------------------------------------\n",
            "allogroom            pair       17         0.1545    \n",
            "approach             pair       258        0.3987    \n",
            "attack               pair       369        0.5933    \n",
            "attemptmount         pair       42         0.0976    \n",
            "avoid                pair       95         0.2875    \n",
            "biteobject           single     16         0.0171    \n",
            "chase                pair       83         0.2645    \n",
            "chaseattack          pair       12         0.3808    \n",
            "climb                single     30         0.4074    \n",
            "defend               pair       64         0.4042    \n",
            "dig                  single     60         0.3446    \n",
            "disengage            pair       20         0.4401    \n",
            "dominance            pair       6          0.6172    \n",
            "dominancegroom       pair       14         0.2355    \n",
            "dominancemount       pair       63         0.4508    \n",
            "ejaculate            pair       3          0.4214    \n",
            "escape               pair       125        0.3168    \n",
            "exploreobject        single     17         0.0776    \n",
            "flinch               pair       22         0.0794    \n",
            "follow               pair       53         0.4516    \n",
            "freeze               single     9          0.3450    \n",
            "genitalgroom         single     17         0.6654    \n",
            "huddle               single     11         0.4601    \n",
            "intromit             pair       81         0.6864    \n",
            "mount                pair       247        0.6280    \n",
            "rear                 single     137        0.4116    \n",
            "reciprocalsniff      pair       42         0.6755    \n",
            "rest                 single     21         0.4364    \n",
            "run                  single     19         0.1483    \n",
            "selfgroom            single     108        0.2771    \n",
            "shepherd             pair       16         0.4269    \n",
            "sniff                pair       622        0.6304    \n",
            "sniffbody            pair       109        0.4877    \n",
            "sniffface            pair       119        0.5425    \n",
            "sniffgenital         pair       462        0.4846    \n",
            "submit               pair       23         0.2633    \n",
            "tussle               pair       6          0.2525    \n",
            "\n",
            "Single behaviors: 11 actions, Avg F1: 0.3264\n",
            "Pair behaviors: 26 actions, Avg F1: 0.4105\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def compute_validation_metrics(submission, verbose=True):\n",
        "    \"\"\"Compute and display validation metrics for single vs pair behaviors.\"\"\"\n",
        "    # solution_df\n",
        "    dataset = pl.read_csv(INPUT_DIR / \"train.csv\").to_pandas()\n",
        "\n",
        "    solution = []\n",
        "    for _, row in dataset.iterrows():\n",
        "        lab_id = row[\"lab_id\"]\n",
        "        if lab_id.startswith(\"MABe22\"):\n",
        "            continue\n",
        "\n",
        "        video_id = row[\"video_id\"]\n",
        "        path = TRAIN_ANNOTATION_DIR / lab_id / f\"{video_id}.parquet\"\n",
        "        try:\n",
        "            annot = pd.read_parquet(path)\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "\n",
        "        annot[\"lab_id\"] = lab_id\n",
        "        annot[\"video_id\"] = video_id\n",
        "        annot[\"behaviors_labeled\"] = row[\"behaviors_labeled\"]\n",
        "        annot[\"target_id\"] = np.where(\n",
        "            annot.target_id != annot.agent_id, annot[\"target_id\"].apply(lambda s: f\"mouse{s}\"), \"self\"\n",
        "        )\n",
        "        annot[\"agent_id\"] = annot[\"agent_id\"].apply(lambda s: f\"mouse{s}\")\n",
        "        solution.append(annot)\n",
        "\n",
        "    solution = pd.concat(solution)\n",
        "\n",
        "    try:\n",
        "        # Separate single and pair behaviors\n",
        "        submission_single = submission[submission[\"target_id\"] == \"self\"].copy()\n",
        "        submission_pair = submission[submission[\"target_id\"] != \"self\"].copy()\n",
        "\n",
        "        # Filter solution to match submission videos\n",
        "        solution_videos = set(submission[\"video_id\"].unique())\n",
        "        solution = solution[solution[\"video_id\"].isin(solution_videos)]\n",
        "\n",
        "        if len(solution) == 0:\n",
        "            return\n",
        "\n",
        "        # Compute overall F1 score\n",
        "        overall_f1 = score(solution, submission, \"row_id\", beta=1.0)\n",
        "        print(f\"\\n{'=' * 60}\")\n",
        "        print(\"PERFORMANCE METRICS\")\n",
        "        print(f\"{'=' * 60}\")\n",
        "        print(f\"Overall F1 Score: {overall_f1:.4f}\")\n",
        "        print(f\"Total predictions: {len(submission)}\")\n",
        "        print(f\"  - Single behaviors: {len(submission_single)}\")\n",
        "        print(f\"  - Pair behaviors: {len(submission_pair)}\")\n",
        "\n",
        "        # Compute per-action F1 scores using existing scoring function\n",
        "        solution_pl = pl.DataFrame(solution)\n",
        "        submission_pl = pl.DataFrame(submission)\n",
        "\n",
        "        # Add label_key and prediction_key\n",
        "        solution_pl = solution_pl.with_columns(\n",
        "            pl.concat_str(\n",
        "                [\n",
        "                    pl.col(\"video_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"agent_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"target_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"action\"),\n",
        "                ],\n",
        "                separator=\"_\",\n",
        "            ).alias(\"label_key\"),\n",
        "        )\n",
        "        submission_pl = submission_pl.with_columns(\n",
        "            pl.concat_str(\n",
        "                [\n",
        "                    pl.col(\"video_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"agent_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"target_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"action\"),\n",
        "                ],\n",
        "                separator=\"_\",\n",
        "            ).alias(\"prediction_key\"),\n",
        "        )\n",
        "\n",
        "        # Group by action and compute metrics\n",
        "        action_stats = defaultdict(lambda: {\"single\": {\"count\": 0, \"f1\": 0.0}, \"pair\": {\"count\": 0, \"f1\": 0.0}})\n",
        "\n",
        "        for lab in solution_pl[\"lab_id\"].unique():\n",
        "            lab_solution = solution_pl.filter(pl.col(\"lab_id\") == lab).clone()\n",
        "            lab_videos = set(lab_solution[\"video_id\"].unique())\n",
        "            lab_submission = submission_pl.filter(pl.col(\"video_id\").is_in(lab_videos)).clone()\n",
        "\n",
        "            # Compute per-action F1 using same logic as single_lab_f1\n",
        "            label_frames = defaultdict(set)\n",
        "            prediction_frames = defaultdict(set)\n",
        "\n",
        "            for row in lab_solution.to_dicts():\n",
        "                label_frames[row[\"label_key\"]].update(range(row[\"start_frame\"], row[\"stop_frame\"]))\n",
        "\n",
        "            for row in lab_submission.to_dicts():\n",
        "                key = row[\"prediction_key\"]\n",
        "                prediction_frames[key].update(range(row[\"start_frame\"], row[\"stop_frame\"]))\n",
        "\n",
        "            for key in set(list(label_frames.keys()) + list(prediction_frames.keys())):\n",
        "                action = key.split(\"_\")[-1]\n",
        "                mode = \"single\" if \"self\" in key else \"pair\"\n",
        "\n",
        "                pred_frames = prediction_frames.get(key, set())\n",
        "                label_frames_set = label_frames.get(key, set())\n",
        "\n",
        "                tp = len(pred_frames & label_frames_set)\n",
        "                fn = len(label_frames_set - pred_frames)\n",
        "                fp = len(pred_frames - label_frames_set)\n",
        "\n",
        "                if tp + fn + fp > 0:\n",
        "                    f1 = (1 + 1**2) * tp / ((1 + 1**2) * tp + 1**2 * fn + fp)\n",
        "                    action_stats[action][mode][\"count\"] += 1\n",
        "                    action_stats[action][mode][\"f1\"] += f1\n",
        "\n",
        "        # Print per-action summary\n",
        "        print(\"\\nPer-Action Performance Summary:\")\n",
        "        print(f\"{'-' * 60}\")\n",
        "        print(f\"{'Action':<20} {'Mode':<10} {'Count':<10} {'Avg F1':<10}\")\n",
        "        print(f\"{'-' * 60}\")\n",
        "\n",
        "        for action in sorted(action_stats.keys()):\n",
        "            for mode in [\"single\", \"pair\"]:\n",
        "                stats = action_stats[action][mode]\n",
        "                if stats[\"count\"] > 0:\n",
        "                    avg_f1 = stats[\"f1\"] / stats[\"count\"]\n",
        "                    print(f\"{action:<20} {mode:<10} {stats['count']:<10} {avg_f1:<10.4f}\")\n",
        "\n",
        "        # Summary by mode\n",
        "        single_actions = [a for a in action_stats.keys() if action_stats[a][\"single\"][\"count\"] > 0]\n",
        "        pair_actions = [a for a in action_stats.keys() if action_stats[a][\"pair\"][\"count\"] > 0]\n",
        "\n",
        "        if single_actions:\n",
        "            single_avg_f1 = np.mean(\n",
        "                [\n",
        "                    action_stats[a][\"single\"][\"f1\"] / action_stats[a][\"single\"][\"count\"]\n",
        "                    for a in single_actions\n",
        "                    if action_stats[a][\"single\"][\"count\"] > 0\n",
        "                ]\n",
        "            )\n",
        "            print(f\"\\nSingle behaviors: {len(single_actions)} actions, Avg F1: {single_avg_f1:.4f}\")\n",
        "\n",
        "        if pair_actions:\n",
        "            pair_avg_f1 = np.mean(\n",
        "                [\n",
        "                    action_stats[a][\"pair\"][\"f1\"] / action_stats[a][\"pair\"][\"count\"]\n",
        "                    for a in pair_actions\n",
        "                    if action_stats[a][\"pair\"][\"count\"] > 0\n",
        "                ]\n",
        "            )\n",
        "            print(f\"Pair behaviors: {len(pair_actions)} actions, Avg F1: {pair_avg_f1:.4f}\")\n",
        "\n",
        "        print(f\"{'=' * 60}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            error_msg = str(e)\n",
        "            if len(error_msg) > 200:\n",
        "                error_msg = error_msg[:200] + \"...\"\n",
        "            print(f\"\\nWarning: Could not compute validation metrics: {error_msg}\")\n",
        "            if verbose:\n",
        "                print(f\"Traceback: {traceback.format_exc()[:300]}\")\n",
        "\n",
        "compute_validation_metrics(submission=pd.read_csv(WORKING_DIR / \"oof_predictions.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A4k33SJ6blca",
      "metadata": {
        "id": "A4k33SJ6blca"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8S00Hli3cNmY",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8S00Hli3cNmY",
        "outputId": "edffc7eb-409b-4ca8-cf80-70ddcea34d65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Đang kiểm tra file và thư mục...\n",
            "  ✅ Tìm thấy: metric.py\n",
            "  ✅ Tìm thấy: pair_features.py\n",
            "  ✅ Tìm thấy: robustify.py\n",
            "  ✅ Tìm thấy: self_features.py\n",
            "  ✅ Tìm thấy: working/results\n",
            "  ✅ Tìm thấy: working/self_features\n",
            "  ✅ Tìm thấy: working/pair_features\n",
            "  ✅ Tìm thấy: working/oof_predictions.csv\n",
            "\n",
            "📦 Đang nén các file vào: mabe_artifacts.zip ...\n",
            "\n",
            "✅ Đã tạo file zip thành công!\n",
            "🚀 Đã chuyển file zip vào thư mục upload_kaggle/mabe_artifacts.zip\n",
            "☁️ Đang chuẩn bị upload lên Kaggle...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# --- CẤU HÌNH ---\n",
        "# 1. Định nghĩa danh sách các file code bạn muốn gửi kèm\n",
        "py_files = [\"metric.py\", \"pair_features.py\", \"robustify.py\", \"self_features.py\"]\n",
        "\n",
        "# 2. Định nghĩa danh sách các thư mục/file dữ liệu cần nén\n",
        "# Không cần dùng chuỗi, dùng list để kiểm tra dễ dàng hơn\n",
        "data_paths = [\n",
        "    \"working/results\",\n",
        "    \"working/self_features\",\n",
        "    \"working/pair_features\",\n",
        "    \"working/oof_predictions.csv\"\n",
        "]\n",
        "\n",
        "# Kết hợp tất cả các đường dẫn cần nén\n",
        "all_paths_to_check = py_files + data_paths\n",
        "files_to_zip = []\n",
        "\n",
        "# --- BƯỚC 1: KIỂM TRA ĐƯỜNG DẪN TỒN TẠI ---\n",
        "print(\"🔍 Đang kiểm tra file và thư mục...\")\n",
        "for path in all_paths_to_check:\n",
        "    if os.path.exists(path):\n",
        "        files_to_zip.append(path)\n",
        "        print(f\"  ✅ Tìm thấy: {path}\")\n",
        "    else:\n",
        "        print(f\"  ❌ KHÔNG tìm thấy: {path} (Sẽ bị bỏ qua)\")\n",
        "\n",
        "if not files_to_zip:\n",
        "    print(\"⛔ LỖI: Không tìm thấy bất kỳ file nào để nén! Hãy kiểm tra lại đường dẫn.\")\n",
        "else:\n",
        "    zip_filename = \"mabe_artifacts.zip\"\n",
        "\n",
        "    # Xóa file zip cũ nếu có\n",
        "    if os.path.exists(zip_filename):\n",
        "        os.remove(zip_filename)\n",
        "\n",
        "    print(f\"\\n📦 Đang nén các file vào: {zip_filename} ...\")\n",
        "\n",
        "    # --- BƯỚC 2: TIẾN HÀNH NÉN SỬ DỤNG THƯ VIỆN CHUẨN CỦA PYTHON (zipfile) ---\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for file_path in files_to_zip:\n",
        "                # Nếu là thư mục, đi bộ qua nó và thêm các file đệ quy\n",
        "                if os.path.isdir(file_path):\n",
        "                    for root, _, files in os.walk(file_path):\n",
        "                        for file in files:\n",
        "                            full_path = os.path.join(root, file)\n",
        "                            # Đảm bảo tên file trong zip không có tiền tố 'working/'\n",
        "                            # ví dụ: working/results/... sẽ thành results/...\n",
        "                            arcname = full_path.replace('working/', '', 1) if full_path.startswith('working/') else full_path\n",
        "                            zipf.write(full_path, arcname=arcname)\n",
        "                # Nếu là file, thêm trực tiếp\n",
        "                elif os.path.isfile(file_path):\n",
        "                    arcname = file_path.replace('working/', '', 1) if file_path.startswith('working/') else file_path\n",
        "                    zipf.write(file_path, arcname=arcname)\n",
        "\n",
        "        print(\"\\n✅ Đã tạo file zip thành công!\")\n",
        "\n",
        "        # --- BƯỚC 3: DI CHUYỂN VÀ CHUẨN BỊ UPLOAD ---\n",
        "        upload_dir = \"upload_kaggle\"\n",
        "        os.makedirs(upload_dir, exist_ok=True)\n",
        "\n",
        "        # Di chuyển file\n",
        "        destination = f\"{upload_dir}/{zip_filename}\"\n",
        "        shutil.move(zip_filename, destination)\n",
        "        print(f\"🚀 Đã chuyển file zip vào thư mục {destination}\")\n",
        "\n",
        "        print(\"☁️ Đang chuẩn bị upload lên Kaggle...\")\n",
        "        # Lệnh upload (Bạn có thể bỏ comment dòng dưới để chạy)\n",
        "        # !kaggle datasets version -p {upload_dir} -m \"Updated code and results\" -r zip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n⛔ LỖI khi nén file: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rczoi-AALQ3q",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Rczoi-AALQ3q",
        "outputId": "55b607a1-0d67-47b6-ba8d-2861841e1104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Đã tạo file metadata cho dataset: tuanvqt/mabe-artifacts\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# --- CẤU HÌNH DATASET ---\n",
        "# Thay dòng bên dưới bằng username Kaggle của bạn\n",
        "KAGGLE_USERNAME = \"tuanvqt\"\n",
        "DATASET_SLUG = \"mabe-artifacts\" # Tên định danh dataset (viết liền không dấu)\n",
        "DATASET_TITLE = \"MABE Artifacts\" # Tên hiển thị của dataset\n",
        "\n",
        "metadata = {\n",
        "    \"title\": DATASET_TITLE,\n",
        "    \"id\": f\"{KAGGLE_USERNAME}/{DATASET_SLUG}\",\n",
        "    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n",
        "}\n",
        "\n",
        "upload_dir = \"upload_kaggle\"\n",
        "os.makedirs(upload_dir, exist_ok=True)\n",
        "\n",
        "# Ghi file dataset-metadata.json vào thư mục upload_kaggle\n",
        "with open(f\"{upload_dir}/dataset-metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "\n",
        "print(f\"✅ Đã tạo file metadata cho dataset: {KAGGLE_USERNAME}/{DATASET_SLUG}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xCnDLVJJgjeq",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xCnDLVJJgjeq",
        "outputId": "c06d8457-4879-4843-9fb0-02e5f89a13d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Đang kiểm tra dataset: tuanvqt/mabe-artifacts ...\n",
            "🆕 Dataset 'tuanvqt/mabe-artifacts' CHƯA TỒN TẠI (hoặc bạn chưa có quyền).\n",
            "🚀 Hệ thống sẽ chạy lệnh: TẠO MỚI (create)\n",
            "------------------------------\n",
            "Starting upload for file mabe_artifacts.zip\n",
            "100% 15.3G/15.3G [11:23<00:00, 24.0MB/s]\n",
            "Upload successful: mabe_artifacts.zip (15GB)\n",
            "Your private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/tuanvqt/mabe-artifacts\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "# --- CẤU HÌNH ---\n",
        "upload_dir = \"upload_kaggle\"\n",
        "metadata_path = f\"{upload_dir}/dataset-metadata.json\"\n",
        "\n",
        "# 1. Đọc ID dataset từ file metadata\n",
        "if not os.path.exists(metadata_path):\n",
        "    print(\"❌ Lỗi: Không tìm thấy file dataset-metadata.json. Hãy tạo nó trước!\")\n",
        "else:\n",
        "    with open(metadata_path, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "        dataset_id = meta['id']\n",
        "\n",
        "    print(f\"🔍 Đang kiểm tra dataset: {dataset_id} ...\")\n",
        "\n",
        "    # 2. Khởi tạo API để kiểm tra\n",
        "    api = KaggleApi()\n",
        "    api.authenticate()\n",
        "\n",
        "    try:\n",
        "        # Thử lấy thông tin dataset\n",
        "        # Nếu dòng này chạy thành công -> Dataset đã tồn tại\n",
        "        api.dataset_view(dataset_id)\n",
        "\n",
        "        print(f\"✅ Dataset '{dataset_id}' ĐÃ TỒN TẠI.\")\n",
        "        print(\"🚀 Hệ thống sẽ chạy lệnh: UPDATE (version)\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Chạy lệnh update (ghi đè version cũ)\n",
        "        !kaggle datasets version -p $upload_dir -m \"Auto update via script\" -r zip\n",
        "\n",
        "    except Exception as e:\n",
        "        # Nếu lỗi (thường là 404 Not Found) -> Dataset chưa có\n",
        "        print(f\"🆕 Dataset '{dataset_id}' CHƯA TỒN TẠI (hoặc bạn chưa có quyền).\")\n",
        "        print(\"🚀 Hệ thống sẽ chạy lệnh: TẠO MỚI (create)\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Chạy lệnh tạo mới\n",
        "        !kaggle datasets create -p $upload_dir -r zip"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 13874099,
          "sourceId": 59156,
          "sourceType": "competition"
        },
        {
          "sourceId": 262477103,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 279806245,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 31153,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 16813.530603,
      "end_time": "2025-11-25T05:43:26.989111",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-11-25T01:03:13.458508",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}