{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7cc0b7e",
      "metadata": {
        "id": "f7cc0b7e"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "VY0FvEMEDqyg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "VY0FvEMEDqyg",
        "outputId": "abe63846-a840-41fe-b0c5-ab01bd0c2f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hãy chọn file kaggle.json từ máy tính của bạn...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6ae286ce-e7ef-4c7e-827a-8855ffadbdfe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6ae286ce-e7ef-4c7e-827a-8855ffadbdfe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Đã cài đặt xong! Giờ bạn có thể dùng lệnh kaggle.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# 1. Hiện nút upload file\n",
        "print(\"Hãy chọn file kaggle.json từ máy tính của bạn...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 2. Tạo thư mục ẩn .kaggle (nếu chưa có)\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# 3. Di chuyển file vào đúng chỗ\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# 4. Phân quyền (QUAN TRỌNG: nếu không làm bước này Kaggle sẽ báo lỗi bảo mật)\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"Đã cài đặt xong! Giờ bạn có thể dùng lệnh kaggle.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fB_khskIHUQa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB_khskIHUQa",
        "outputId": "b368e980-c3b9-4d38-f255-a7f9f106d7ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Downloading MABe-mouse-behavior-detection.zip to /content\n",
            "100% 2.62G/2.63G [00:02<00:00, 848MB/s] \n",
            "100% 2.63G/2.63G [00:02<00:00, 1.38GB/s]\n",
            "Unzipping files...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"MABe-mouse-behavior-detection.zip\"):\n",
        "    print(\"Downloading dataset...\")\n",
        "    !kaggle competitions download -c MABe-mouse-behavior-detection\n",
        "else:\n",
        "    print(\"Zip already exists, skip download.\")\n",
        "\n",
        "if not os.path.exists(\"./input\"):\n",
        "    print(\"Unzipping files...\")\n",
        "    !unzip -q MABe-mouse-behavior-detection.zip -d ./input\n",
        "else:\n",
        "    print(\"Input folder already exists, skip unzip.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "E54FbxqaLrmA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E54FbxqaLrmA",
        "outputId": "f39e8f70-ab7e-423a-f1ba-315c9fe8abd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (13.6.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.3)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x) (0.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U lightgbm cupy-cuda12x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "g1AM36ZMHYev",
      "metadata": {
        "id": "g1AM36ZMHYev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6398b4ee-bcbd-474a-e67d-65ad951810ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q polars xgboost scikit-learn catboost optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "KuXZDyINKSsD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuXZDyINKSsD",
        "outputId": "5edc0e84-f4e7-45bc-d710-0b694e8534fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing metric.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile metric.py\n",
        "\"\"\"F Beta customized for the data format of the MABe challenge.\"\"\"\n",
        "\n",
        "import json\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "\n",
        "class HostVisibleError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "def single_lab_f1(lab_solution: pl.DataFrame, lab_submission: pl.DataFrame, beta: float = 1) -> float:\n",
        "    label_frames: defaultdict[str, set[int]] = defaultdict(set)\n",
        "    prediction_frames: defaultdict[str, set[int]] = defaultdict(set)\n",
        "\n",
        "    for row in lab_solution.to_dicts():\n",
        "        label_frames[row['label_key']].update(range(row['start_frame'], row['stop_frame']))\n",
        "\n",
        "    for video in lab_solution['video_id'].unique():\n",
        "        active_labels: str = lab_solution.filter(pl.col('video_id') == video)['behaviors_labeled'].first()  # ty: ignore\n",
        "        active_labels: set[str] = set(json.loads(active_labels))\n",
        "        predicted_mouse_pairs: defaultdict[str, set[int]] = defaultdict(set)\n",
        "\n",
        "        for row in lab_submission.filter(pl.col('video_id') == video).to_dicts():\n",
        "            # Since the labels are sparse, we can't evaluate prediction keys not in the active labels.\n",
        "            if ','.join([str(row['agent_id']), str(row['target_id']), row['action']]) not in active_labels:\n",
        "                continue\n",
        "\n",
        "            new_frames = set(range(row['start_frame'], row['stop_frame']))\n",
        "            # Ignore truly redundant predictions.\n",
        "            new_frames = new_frames.difference(prediction_frames[row['prediction_key']])\n",
        "            prediction_pair = ','.join([str(row['agent_id']), str(row['target_id'])])\n",
        "            if predicted_mouse_pairs[prediction_pair].intersection(new_frames):\n",
        "                # A single agent can have multiple targets per frame (ex: evading all other mice) but only one action per target per frame.\n",
        "                raise HostVisibleError('Multiple predictions for the same frame from one agent/target pair')\n",
        "            prediction_frames[row['prediction_key']].update(new_frames)\n",
        "            predicted_mouse_pairs[prediction_pair].update(new_frames)\n",
        "\n",
        "    tps = defaultdict(int)\n",
        "    fns = defaultdict(int)\n",
        "    fps = defaultdict(int)\n",
        "    for key, pred_frames in prediction_frames.items():\n",
        "        action = key.split('_')[-1]\n",
        "        matched_label_frames = label_frames[key]\n",
        "        tps[action] += len(pred_frames.intersection(matched_label_frames))\n",
        "        fns[action] += len(matched_label_frames.difference(pred_frames))\n",
        "        fps[action] += len(pred_frames.difference(matched_label_frames))\n",
        "\n",
        "    distinct_actions = set()\n",
        "    for key, frames in label_frames.items():\n",
        "        action = key.split('_')[-1]\n",
        "        distinct_actions.add(action)\n",
        "        if key not in prediction_frames:\n",
        "            fns[action] += len(frames)\n",
        "\n",
        "    action_f1s = []\n",
        "    for action in distinct_actions:\n",
        "        if tps[action] + fns[action] + fps[action] == 0:\n",
        "            action_f1s.append(0)\n",
        "        else:\n",
        "            action_f1s.append((1 + beta**2) * tps[action] / ((1 + beta**2) * tps[action] + beta**2 * fns[action] + fps[action]))\n",
        "    return sum(action_f1s) / len(action_f1s)\n",
        "\n",
        "\n",
        "def mouse_fbeta(solution: pd.DataFrame, submission: pd.DataFrame, beta: float = 1) -> float:\n",
        "    \"\"\"\n",
        "    Doctests:\n",
        "    >>> solution = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ... ])\n",
        "    >>> submission = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10},\n",
        "    ... ])\n",
        "    >>> mouse_fbeta(solution, submission)\n",
        "    1.0\n",
        "\n",
        "    >>> solution = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ... ])\n",
        "    >>> submission = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 0, 'stop_frame': 10}, # Wrong action\n",
        "    ... ])\n",
        "    >>> mouse_fbeta(solution, submission)\n",
        "    0.0\n",
        "\n",
        "    >>> solution = pd.DataFrame([\n",
        "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 15, 'stop_frame': 24, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ... ])\n",
        "    >>> submission = pd.DataFrame([\n",
        "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9},\n",
        "    ... ])\n",
        "    >>> \"%.12f\" % mouse_fbeta(solution, submission)\n",
        "    '0.500000000000'\n",
        "\n",
        "    >>> solution = pd.DataFrame([\n",
        "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 15, 'stop_frame': 24, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ...     {'video_id': 345, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9, 'lab_id': 2, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ...     {'video_id': 345, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 15, 'stop_frame': 24, 'lab_id': 2, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ... ])\n",
        "    >>> submission = pd.DataFrame([\n",
        "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9},\n",
        "    ... ])\n",
        "    >>> \"%.12f\" % mouse_fbeta(solution, submission)\n",
        "    '0.250000000000'\n",
        "\n",
        "    >>> # Overlapping solution events, one prediction matching both.\n",
        "    >>> solution = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 10, 'stop_frame': 20, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ... ])\n",
        "    >>> submission = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 20},\n",
        "    ... ])\n",
        "    >>> mouse_fbeta(solution, submission)\n",
        "    1.0\n",
        "\n",
        "    >>> solution = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 30, 'stop_frame': 40, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
        "    ... ])\n",
        "    >>> submission = pd.DataFrame([\n",
        "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 40},\n",
        "    ... ])\n",
        "    >>> mouse_fbeta(solution, submission)\n",
        "    0.6666666666666666\n",
        "    \"\"\"\n",
        "    if len(solution) == 0 or len(submission) == 0:\n",
        "        raise ValueError('Missing solution or submission data')\n",
        "\n",
        "    expected_cols = ['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\n",
        "\n",
        "    for col in expected_cols:\n",
        "        if col not in solution.columns:\n",
        "            raise ValueError(f'Solution is missing column {col}')\n",
        "        if col not in submission.columns:\n",
        "            raise ValueError(f'Submission is missing column {col}')\n",
        "\n",
        "    solution: pl.DataFrame = pl.DataFrame(solution)\n",
        "    submission: pl.DataFrame = pl.DataFrame(submission)\n",
        "    assert (solution['start_frame'] <= solution['stop_frame']).all()\n",
        "    assert (submission['start_frame'] <= submission['stop_frame']).all()\n",
        "    solution_videos = set(solution['video_id'].unique())\n",
        "    # Need to align based on video IDs as we can't rely on the row IDs for handling public/private splits.\n",
        "    submission = submission.filter(pl.col('video_id').is_in(solution_videos))\n",
        "\n",
        "    solution = solution.with_columns(\n",
        "        pl.concat_str(\n",
        "            [\n",
        "                pl.col('video_id').cast(pl.Utf8),\n",
        "                pl.col('agent_id').cast(pl.Utf8),\n",
        "                pl.col('target_id').cast(pl.Utf8),\n",
        "                pl.col('action'),\n",
        "            ],\n",
        "            separator='_',\n",
        "        ).alias('label_key'),\n",
        "    )\n",
        "    submission = submission.with_columns(\n",
        "        pl.concat_str(\n",
        "            [\n",
        "                pl.col('video_id').cast(pl.Utf8),\n",
        "                pl.col('agent_id').cast(pl.Utf8),\n",
        "                pl.col('target_id').cast(pl.Utf8),\n",
        "                pl.col('action'),\n",
        "            ],\n",
        "            separator='_',\n",
        "        ).alias('prediction_key'),\n",
        "    )\n",
        "\n",
        "    lab_scores = []\n",
        "    for lab in solution['lab_id'].unique():\n",
        "        lab_solution = solution.filter(pl.col('lab_id') == lab).clone()\n",
        "        lab_videos = set(lab_solution['video_id'].unique())\n",
        "        lab_submission = submission.filter(pl.col('video_id').is_in(lab_videos)).clone()\n",
        "        lab_scores.append(single_lab_f1(lab_solution, lab_submission, beta=beta))\n",
        "\n",
        "    return sum(lab_scores) / len(lab_scores)\n",
        "\n",
        "\n",
        "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, beta: float = 1) -> float:\n",
        "    \"\"\"\n",
        "    F1 score for the MABe Challenge\n",
        "    \"\"\"\n",
        "    solution = solution.drop(row_id_column_name, axis='columns', errors='ignore')\n",
        "    submission = submission.drop(row_id_column_name, axis='columns', errors='ignore')\n",
        "    return mouse_fbeta(solution, submission, beta=beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68JjLSX1DhI_",
      "metadata": {
        "id": "68JjLSX1DhI_"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8b91cb2c",
      "metadata": {
        "id": "8b91cb2c"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import gc\n",
        "import itertools\n",
        "import json\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import traceback\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from metric import score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "428bb520",
      "metadata": {
        "id": "428bb520"
      },
      "outputs": [],
      "source": [
        "# const\n",
        "# INPUT_DIR = Path(\"/kaggle/input/MABe-mouse-behavior-detection\")\n",
        "# WORKING_DIR = Path(\"/kaggle/working\")\n",
        "\n",
        "INPUT_DIR = Path(\"/content/input\")\n",
        "WORKING_DIR = Path(\"/content/working\")\n",
        "\n",
        "\n",
        "TRAIN_TRACKING_DIR = INPUT_DIR / \"train_tracking\"\n",
        "TRAIN_ANNOTATION_DIR = INPUT_DIR / \"train_annotation\"\n",
        "TEST_TRACKING_DIR = INPUT_DIR / \"test_tracking\"\n",
        "\n",
        "\n",
        "INDEX_COLS = [\n",
        "    \"video_id\",\n",
        "    \"agent_mouse_id\",\n",
        "    \"target_mouse_id\",\n",
        "    \"video_frame\",\n",
        "]\n",
        "\n",
        "BODY_PARTS = [\n",
        "    \"ear_left\",\n",
        "    \"ear_right\",\n",
        "    \"nose\",\n",
        "    \"neck\",\n",
        "    \"body_center\",\n",
        "    \"lateral_left\",\n",
        "    \"lateral_right\",\n",
        "    \"hip_left\",\n",
        "    \"hip_right\",\n",
        "    \"tail_base\",\n",
        "    \"tail_tip\",\n",
        "]\n",
        "\n",
        "SELF_BEHAVIORS = [\n",
        "    \"biteobject\",\n",
        "    \"climb\",\n",
        "    \"dig\",\n",
        "    \"exploreobject\",\n",
        "    \"freeze\",\n",
        "    \"genitalgroom\",\n",
        "    \"huddle\",\n",
        "    \"rear\",\n",
        "    \"rest\",\n",
        "    \"run\",\n",
        "    \"selfgroom\",\n",
        "]\n",
        "\n",
        "PAIR_BEHAVIORS = [\n",
        "    \"allogroom\",\n",
        "    \"approach\",\n",
        "    \"attack\",\n",
        "    \"attemptmount\",\n",
        "    \"avoid\",\n",
        "    \"chase\",\n",
        "    \"chaseattack\",\n",
        "    \"defend\",\n",
        "    \"disengage\",\n",
        "    \"dominance\",\n",
        "    \"dominancegroom\",\n",
        "    \"dominancemount\",\n",
        "    \"ejaculate\",\n",
        "    \"escape\",\n",
        "    \"flinch\",\n",
        "    \"follow\",\n",
        "    \"intromit\",\n",
        "    \"mount\",\n",
        "    \"reciprocalsniff\",\n",
        "    \"shepherd\",\n",
        "    \"sniff\",\n",
        "    \"sniffbody\",\n",
        "    \"sniffface\",\n",
        "    \"sniffgenital\",\n",
        "    \"submit\",\n",
        "    \"tussle\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "18f4383f",
      "metadata": {
        "id": "18f4383f"
      },
      "outputs": [],
      "source": [
        "# read data\n",
        "train_dataframe = pl.read_csv(INPUT_DIR / \"train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193a06eaGFlx",
      "metadata": {
        "id": "193a06eaGFlx"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t4_JedjWCwgi",
      "metadata": {
        "id": "t4_JedjWCwgi"
      },
      "source": [
        "## Behavior Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "71d9fb5c",
      "metadata": {
        "id": "71d9fb5c"
      },
      "outputs": [],
      "source": [
        "# preprocess behavior labels\n",
        "train_behavior_dataframe = (\n",
        "    train_dataframe\n",
        "    .filter(pl.col(\"behaviors_labeled\").is_not_null())\n",
        "    .select(\n",
        "        pl.col(\"lab_id\"),\n",
        "        pl.col(\"video_id\"),\n",
        "        pl.col(\"behaviors_labeled\").map_elements(eval, return_dtype=pl.List(pl.Utf8)).alias(\"behaviors_labeled_list\"),\n",
        "    )\n",
        "    .explode(\"behaviors_labeled_list\")\n",
        "    .rename({\"behaviors_labeled_list\": \"behaviors_labeled_element\"})\n",
        "    .select(\n",
        "        pl.col(\"lab_id\"),\n",
        "        pl.col(\"video_id\"),\n",
        "        pl.col(\"behaviors_labeled_element\").str.split(\",\").list[0].str.replace_all(\"'\", \"\").alias(\"agent\"),\n",
        "        pl.col(\"behaviors_labeled_element\").str.split(\",\").list[1].str.replace_all(\"'\", \"\").alias(\"target\"),\n",
        "        pl.col(\"behaviors_labeled_element\").str.split(\",\").list[2].str.replace_all(\"'\", \"\").alias(\"behavior\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "train_self_behavior_dataframe = train_behavior_dataframe.filter(pl.col(\"behavior\").is_in(SELF_BEHAVIORS))\n",
        "train_pair_behavior_dataframe = train_behavior_dataframe.filter(pl.col(\"behavior\").is_in(PAIR_BEHAVIORS))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6858f216",
      "metadata": {
        "id": "6858f216"
      },
      "source": [
        "## Self\n",
        "\n",
        "\n",
        "*   Khoảng cách giữa các bộ phận cơ thể của agent (cm) - BODY PARTS\n",
        "\n",
        "*   Vận tốc ước tính của các bộ phận (cm/s)\n",
        "Tính vận tốc ước tính của ear_left, ear_right, tail_base trong các khoảng thời gian 500, 1000, 2000, 3000 ms.\n",
        "\n",
        "*   Độ duỗi (nose-tail base / earleft-right)\n",
        "\n",
        "*   Body angle (nose-body center vs body center-tail)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5c024dc5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c024dc5",
        "outputId": "027c9716-5c8f-4f6e-94ac-fcb2005d6313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing self_features.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile self_features.py\n",
        "\n",
        "def make_self_features(\n",
        "    metadata: dict,\n",
        "    tracking: pl.DataFrame,\n",
        ") -> pl.DataFrame:\n",
        "    # tạo để đỡ phải truy cập nhiều lần\n",
        "    fps = metadata[\"frames_per_second\"]\n",
        "    pix_per_cm = metadata[\"pix_per_cm_approx\"]\n",
        "\n",
        "    # Helper\n",
        "    def get_window(period_ms):\n",
        "        return max(1, int(round(period_ms * fps / 1000.0)))\n",
        "\n",
        "\n",
        "    def body_parts_distance(body_part_1, body_part_2):\n",
        "        # Khoảng cách giữa các bộ phận cơ thể của agent (cm)\n",
        "        assert body_part_1 in BODY_PARTS\n",
        "        assert body_part_2 in BODY_PARTS\n",
        "        return (\n",
        "            (pl.col(f\"agent_x_{body_part_1}\") - pl.col(f\"agent_x_{body_part_2}\")).pow(2)\n",
        "            + (pl.col(f\"agent_y_{body_part_1}\") - pl.col(f\"agent_y_{body_part_2}\")).pow(2)\n",
        "        ).sqrt() / metadata[\"pix_per_cm_approx\"]\n",
        "\n",
        "    # thêm chức năng switch để chuyển giữa mean vafd std\n",
        "    def body_part_speed(body_part, period_ms, stat=\"mean\"):\n",
        "        assert body_part in BODY_PARTS\n",
        "        # Tốc độ ước tính của bộ phận (cm/s)\n",
        "        raw_speed = (\n",
        "            ((pl.col(f\"agent_x_{body_part}\").diff()).pow(2) + (pl.col(f\"agent_y_{body_part}\").diff()).pow(2)).sqrt()\n",
        "            / pix_per_cm * fps\n",
        "        )\n",
        "        w = get_window(period_ms)\n",
        "        if stat == \"mean\":\n",
        "            return raw_speed.rolling_mean(window_size=w, center=True, min_samples=1).fill_null(0.0)\n",
        "        elif stat == \"std\":\n",
        "            return raw_speed.rolling_std(window_size=w, center=True, min_samples=1).fill_null(0.0)\n",
        "        return raw_speed\n",
        "\n",
        "    def elongation():\n",
        "        # Độ giãn dài\n",
        "        d1 = body_parts_distance(\"nose\", \"tail_base\")\n",
        "        d2 = body_parts_distance(\"ear_left\", \"ear_right\")\n",
        "        return d1 / (d2 + 1e-06)\n",
        "\n",
        "    def body_angle():\n",
        "        # Góc cơ thể (deg)\n",
        "        v1x = pl.col(\"agent_x_nose\") - pl.col(\"agent_x_body_center\")\n",
        "        v1y = pl.col(\"agent_y_nose\") - pl.col(\"agent_y_body_center\")\n",
        "        v2x = pl.col(\"agent_x_tail_base\") - pl.col(\"agent_x_body_center\")\n",
        "        v2y = pl.col(\"agent_y_tail_base\") - pl.col(\"agent_y_body_center\")\n",
        "        return (v1x * v2x + v1y * v2y) / ((v1x.pow(2) + v1y.pow(2)).sqrt() * (v2x.pow(2) + v2y.pow(2)).sqrt() + 1e-06)\n",
        "\n",
        "    # [MỚI] Hàm tính Grooming Decouple\n",
        "    def grooming_decouple():\n",
        "        # Tốc độ tức thời của Mũi\n",
        "        s_nose = (((pl.col(\"agent_x_nose\").diff()).pow(2) + (pl.col(\"agent_y_nose\").diff()).pow(2)).sqrt() / pix_per_cm * fps)\n",
        "        # Tốc độ tức thời của Thân\n",
        "        s_body = (((pl.col(\"agent_x_body_center\").diff()).pow(2) + (pl.col(\"agent_y_body_center\").diff()).pow(2)).sqrt() / pix_per_cm * fps)\n",
        "\n",
        "        # Ratio: Mũi / (Thân + 0.5) -> Median 500ms\n",
        "        w = get_window(500)\n",
        "        ratio = (s_nose / (s_body + 0.5)).clip(0.0, 10.0)\n",
        "        return ratio.rolling_median(window_size=w, center=True, min_samples=1).fill_null(0.0)\n",
        "    # [MỚI] Hàm tính Nose Radial Jitter\n",
        "    def nose_radial_jitter():\n",
        "        # Khoảng cách Mũi - Thân\n",
        "        dist = body_parts_distance(\"nose\", \"body_center\")\n",
        "        # Std trong 500ms\n",
        "        w = get_window(500)\n",
        "        return dist.rolling_std(window_size=w, center=True, min_samples=1).fill_null(0.0)\n",
        "    # [MỚI] Hàm tính Vận tốc góc\n",
        "    def angular_velocity():\n",
        "        vec_x = pl.col(\"agent_x_nose\") - pl.col(\"agent_x_body_center\")\n",
        "        vec_y = pl.col(\"agent_y_nose\") - pl.col(\"agent_y_body_center\")\n",
        "        angle = pl.arctan2(vec_y, vec_x)\n",
        "        # Diff góc * FPS = Rad/s -> Smooth 300ms\n",
        "        w = get_window(300)\n",
        "        return (angle.diff().abs() * fps).rolling_mean(window_size=w, center=True, min_samples=1).fill_null(0.0)\n",
        "\n",
        "\n",
        "\n",
        "    n_mice = (\n",
        "        (metadata[\"mouse1_strain\"] is not None)\n",
        "        + (metadata[\"mouse2_strain\"] is not None)\n",
        "        + (metadata[\"mouse3_strain\"] is not None)\n",
        "        + (metadata[\"mouse4_strain\"] is not None)\n",
        "    )\n",
        "    start_frame = tracking.select(pl.col(\"video_frame\").min()).item()\n",
        "    end_frame = tracking.select(pl.col(\"video_frame\").max()).item()\n",
        "\n",
        "    result = []\n",
        "\n",
        "    pivot = tracking.pivot(\n",
        "        on=[\"bodypart\"],\n",
        "        index=[\"video_frame\", \"mouse_id\"],\n",
        "        values=[\"x\", \"y\"],\n",
        "    ).sort([\"mouse_id\", \"video_frame\"])\n",
        "    pivot_trackings = {mouse_id: pivot.filter(pl.col(\"mouse_id\") == mouse_id) for mouse_id in range(1, n_mice + 1)}\n",
        "\n",
        "    for agent_mouse_id in range(1, n_mice + 1):\n",
        "        result_element = pl.DataFrame(\n",
        "            {\n",
        "                \"video_id\": metadata[\"video_id\"],\n",
        "                \"agent_mouse_id\": agent_mouse_id,\n",
        "                \"target_mouse_id\": -1,\n",
        "                \"video_frame\": pl.arange(start_frame, end_frame + 1, eager=True),\n",
        "            },\n",
        "            schema={\n",
        "                \"video_id\": pl.Int32,\n",
        "                \"agent_mouse_id\": pl.Int8,\n",
        "                \"target_mouse_id\": pl.Int8,\n",
        "                \"video_frame\": pl.Int32,\n",
        "            },\n",
        "        )\n",
        "\n",
        "        pivot = pivot_trackings[agent_mouse_id].select(\n",
        "            pl.col(\"video_frame\"),\n",
        "            pl.exclude(\"video_frame\").name.prefix(\"agent_\"),\n",
        "        )\n",
        "        columns = pivot.columns\n",
        "        pivot = pivot.with_columns(\n",
        "            *[pl.lit(None).cast(pl.Float32).alias(f\"agent_x_{bp}\") for bp in BODY_PARTS if f\"agent_x_{bp}\" not in columns],\n",
        "            *[pl.lit(None).cast(pl.Float32).alias(f\"agent_y_{bp}\") for bp in BODY_PARTS if f\"agent_y_{bp}\" not in columns],\n",
        "        )\n",
        "\n",
        "        features = pivot.with_columns(\n",
        "            pl.lit(agent_mouse_id).alias(\"agent_mouse_id\"),\n",
        "            pl.lit(-1).alias(\"target_mouse_id\"),\n",
        "        ).select(\n",
        "            pl.col(\"video_frame\"),\n",
        "            pl.col(\"agent_mouse_id\"),\n",
        "            pl.col(\"target_mouse_id\"),\n",
        "            *[\n",
        "                body_parts_distance(body_part_1, body_part_2).alias(f\"aa__{body_part_1}__{body_part_2}__distance\")\n",
        "                for body_part_1, body_part_2 in itertools.combinations(BODY_PARTS, 2)\n",
        "            ],\n",
        "            *[\n",
        "                body_part_speed(body_part, period_ms).alias(f\"agent__{body_part}__speed_{period_ms}ms\")\n",
        "                for body_part, period_ms in itertools.product([\"ear_left\", \"ear_right\", \"tail_base\"], [500, 1000, 2000, 3000])\n",
        "            ],\n",
        "            # THÊM: body_center speed (Run/Walk)\n",
        "            *[\n",
        "                body_part_speed(\"body_center\", ms, stat=\"mean\").alias(\n",
        "                    f\"agent__body_center__speed_{ms}ms\"\n",
        "                )\n",
        "                for ms in [500, 1000, 2000]\n",
        "            ],\n",
        "            # THÊM: nose speed (Groom/Sniff)\n",
        "            *[\n",
        "                body_part_speed(\"nose\", ms, stat=\"mean\").alias(\n",
        "                    f\"agent__nose__speed_{ms}ms\"\n",
        "                )\n",
        "                for ms in [500, 1000]\n",
        "            ],\n",
        "            elongation().alias(\"agent__elongation\"),\n",
        "            body_angle().alias(\"agent__body_angle\"),\n",
        "            # các feature mới thêm\n",
        "            grooming_decouple().alias(\"agent__groom_decouple\"),\n",
        "            nose_radial_jitter().alias(\"agent__groom_nose_jitter\"),\n",
        "            angular_velocity().alias(\"agent__angular_velocity\"),\n",
        "        )\n",
        "\n",
        "        result_element = result_element.join(\n",
        "            features,\n",
        "            on=[\"video_frame\", \"agent_mouse_id\", \"target_mouse_id\"],\n",
        "            how=\"left\",\n",
        "        )\n",
        "        result.append(result_element)\n",
        "\n",
        "    return pl.concat(result, how=\"vertical\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12c5fd9a",
      "metadata": {
        "id": "12c5fd9a"
      },
      "source": [
        "## Pair\n",
        "\n",
        "*   Khoảng cách giữa các bộ phận cơ thể của agent–target (cm)  - BODY PARTS\n",
        "\n",
        "*   Vận tốc ước tính của các bộ phận của agent và target (cm/s)\n",
        "Tính vận tốc ước tính của ear_left, ear_right, tail_base trong các khoảng thời gian 500, 1000, 2000, 3000 ms cho cả agent và target.\n",
        "\n",
        "*   Độ duỗi của agent và target\n",
        "\n",
        "*   Góc cơ thể của agent và target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cab41dc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cab41dc0",
        "outputId": "8546b9f3-e690-4614-f692-33719db8966a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pair_features.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile pair_features.py\n",
        "\n",
        "def make_pair_features(\n",
        "    metadata: dict,\n",
        "    tracking: pl.DataFrame,\n",
        ") -> pl.DataFrame:\n",
        "    def body_parts_distance(agent_or_target_1, body_part_1, agent_or_target_2, body_part_2):\n",
        "        # Khoảng cách giữa các bộ phận cơ thể của agent-target (cm)\n",
        "        assert agent_or_target_1 == \"agent\" or agent_or_target_1 == \"target\"\n",
        "        assert agent_or_target_2 == \"agent\" or agent_or_target_2 == \"target\"\n",
        "        assert body_part_1 in BODY_PARTS\n",
        "        assert body_part_2 in BODY_PARTS\n",
        "        return (\n",
        "            (pl.col(f\"{agent_or_target_1}_x_{body_part_1}\") - pl.col(f\"{agent_or_target_2}_x_{body_part_2}\")).pow(2)\n",
        "            + (pl.col(f\"{agent_or_target_1}_y_{body_part_1}\") - pl.col(f\"{agent_or_target_2}_y_{body_part_2}\")).pow(2)\n",
        "        ).sqrt() / metadata[\"pix_per_cm_approx\"]\n",
        "\n",
        "    def body_part_speed(agent_or_target, body_part, period_ms):\n",
        "        # Tốc độ ước tính của bộ phận (cm/s)\n",
        "        assert agent_or_target == \"agent\" or agent_or_target == \"target\"\n",
        "        assert body_part in BODY_PARTS\n",
        "        window_frames = max(1, int(round(period_ms * metadata[\"frames_per_second\"] / 1000.0)))\n",
        "        return (\n",
        "            (\n",
        "                (pl.col(f\"{agent_or_target}_x_{body_part}\").diff()).pow(2)\n",
        "                + (pl.col(f\"{agent_or_target}_y_{body_part}\").diff()).pow(2)\n",
        "            ).sqrt()\n",
        "            / metadata[\"pix_per_cm_approx\"]\n",
        "            * metadata[\"frames_per_second\"]\n",
        "        ).rolling_mean(window_size=window_frames, center=True, min_samples=1).fill_null(0.0)\n",
        "\n",
        "    def elongation(agent_or_target):\n",
        "        # Độ giãn dài (cm)\n",
        "        assert agent_or_target == \"agent\" or agent_or_target == \"target\"\n",
        "        d1 = body_parts_distance(agent_or_target, \"nose\", agent_or_target, \"tail_base\")\n",
        "        d2 = body_parts_distance(agent_or_target, \"ear_left\", agent_or_target, \"ear_right\")\n",
        "        return d1 / (d2 + 1e-06)\n",
        "\n",
        "    def body_angle(agent_or_target):\n",
        "        # Cosine of body angle (alignment)\n",
        "        assert agent_or_target == \"agent\" or agent_or_target == \"target\"\n",
        "        v1x = pl.col(f\"{agent_or_target}_x_nose\") - pl.col(f\"{agent_or_target}_x_body_center\")\n",
        "        v1y = pl.col(f\"{agent_or_target}_y_nose\") - pl.col(f\"{agent_or_target}_y_body_center\")\n",
        "        v2x = pl.col(f\"{agent_or_target}_x_tail_base\") - pl.col(f\"{agent_or_target}_x_body_center\")\n",
        "        v2y = pl.col(f\"{agent_or_target}_y_tail_base\") - pl.col(f\"{agent_or_target}_y_body_center\")\n",
        "        return (v1x * v2x + v1y * v2y) / ((v1x.pow(2) + v1y.pow(2)).sqrt() * (v2x.pow(2) + v2y.pow(2)).sqrt() + 1e-06)\n",
        "\n",
        "    def body_center_distance_rolling_agg(agg, period_ms):\n",
        "        # Đặc trưng tổng hợp khoảng cách trung tâm cơ thể di chuyển\n",
        "        assert agg in [\"mean\", \"std\", \"var\", \"min\", \"max\"] # Hàm tổng hợp\n",
        "        expr = body_parts_distance(\"agent\", \"body_center\", \"target\", \"body_center\")\n",
        "        window_frames = max(1, int(round(period_ms * metadata[\"frames_per_second\"] / 1000.0)))\n",
        "\n",
        "        if agg == \"mean\":\n",
        "            return expr.rolling_mean(window_size=window_frames, center=True, min_samples=1)\n",
        "        elif agg == \"std\":\n",
        "            return expr.rolling_std(window_size=window_frames, center=True, min_samples=1)\n",
        "        elif agg == \"var\":\n",
        "            return expr.rolling_var(window_size=window_frames, center=True, min_samples=1)\n",
        "        elif agg == \"min\":\n",
        "            return expr.rolling_min(window_size=window_frames, center=True, min_samples=1)\n",
        "        elif agg == \"max\":\n",
        "            return expr.rolling_max(window_size=window_frames, center=True, min_samples=1)\n",
        "        else:\n",
        "            raise ValueError()\n",
        "\n",
        "    # Add new feature\n",
        "    fps  = metadata[\"frames_per_second\"]\n",
        "    def body_center_distance():\n",
        "        # Khoảng cách tâm thân agent–target (cm)\n",
        "        return body_parts_distance(\"agent\", \"body_center\", \"target\", \"body_center\")\n",
        "    def body_center_radial_velocity(period_ms=300):\n",
        "        \"\"\"\n",
        "        Vận tốc hướng tâm (cm/s):\n",
        "        - < 0: lại gần (approach/chase)\n",
        "        - > 0: xa ra (avoid/escape)\n",
        "        \"\"\"\n",
        "        dist = body_center_distance()\n",
        "        window_frames = max(1, int(round(period_ms * fps / 1000.0)))\n",
        "        return (dist.diff()*fps).rolling_mean(\n",
        "            window_size=window_frames,\n",
        "            center=True,\n",
        "            min_samples=1,\n",
        "        ).fill_null(0.0)\n",
        "    def relative_speed(period_ms=500):\n",
        "        \"\"\"\n",
        "        Chênh lệch tốc độ thân:\n",
        "        > 0: agent nhanh hơn\n",
        "        < 0: target nhanh hơn\n",
        "        \"\"\"\n",
        "        return (\n",
        "            body_part_speed(\"agent\", \"body_center\", period_ms)\n",
        "            - body_part_speed(\"target\", \"body_center\", period_ms)\n",
        "        )\n",
        "    def facing_score(agent_role, target_role, period_ms=500):\n",
        "        \"\"\"\n",
        "        Cosine giữa:\n",
        "        - hướng body_center→nose của agent\n",
        "        - vector agent_body_center → target_body_center\n",
        "        \"\"\"\n",
        "        vec_ag_x = pl.col(f\"{agent_role}_x_nose\") - pl.col(f\"{agent_role}_x_body_center\")\n",
        "        vec_ag_y = pl.col(f\"{agent_role}_y_nose\") - pl.col(f\"{agent_role}_y_body_center\")\n",
        "\n",
        "        vec_to_tg_x = pl.col(f\"{target_role}_x_body_center\") - pl.col(f\"{agent_role}_x_body_center\")\n",
        "        vec_to_tg_y = pl.col(f\"{target_role}_y_body_center\") - pl.col(f\"{agent_role}_y_body_center\")\n",
        "\n",
        "        dot = vec_ag_x * vec_to_tg_x + vec_ag_y * vec_to_tg_y\n",
        "        mag_ag = (vec_ag_x.pow(2) + vec_ag_y.pow(2)).sqrt()\n",
        "        mag_to = (vec_to_tg_x.pow(2) + vec_to_tg_y.pow(2)).sqrt()\n",
        "\n",
        "        cos_val = (dot / (mag_ag * mag_to + 1e-6)).clip(-1.0, 1.0)\n",
        "\n",
        "        window_frames = max(1, int(round(period_ms * fps / 1000.0)))\n",
        "        return cos_val.rolling_mean(\n",
        "            window_size=window_frames,\n",
        "            center=True,\n",
        "            min_samples=1,\n",
        "        ).fill_null(0.0)\n",
        "\n",
        "\n",
        "    n_mice = (\n",
        "        (metadata[\"mouse1_strain\"] is not None)\n",
        "        + (metadata[\"mouse2_strain\"] is not None)\n",
        "        + (metadata[\"mouse3_strain\"] is not None)\n",
        "        + (metadata[\"mouse4_strain\"] is not None)\n",
        "    )\n",
        "    start_frame = tracking.select(pl.col(\"video_frame\").min()).item()\n",
        "    end_frame = tracking.select(pl.col(\"video_frame\").max()).item()\n",
        "\n",
        "    result = []\n",
        "\n",
        "    pivot = tracking.pivot(\n",
        "        on=[\"bodypart\"],\n",
        "        index=[\"video_frame\", \"mouse_id\"],\n",
        "        values=[\"x\", \"y\"],\n",
        "    ).sort([\"mouse_id\", \"video_frame\"])\n",
        "    pivot_trackings = {mouse_id: pivot.filter(pl.col(\"mouse_id\") == mouse_id) for mouse_id in range(1, n_mice + 1)}\n",
        "\n",
        "    for agent_mouse_id, target_mouse_id in itertools.permutations(range(1, n_mice + 1), 2):\n",
        "        result_element = pl.DataFrame(\n",
        "            {\n",
        "                \"video_id\": metadata[\"video_id\"],\n",
        "                \"agent_mouse_id\": agent_mouse_id,\n",
        "                \"target_mouse_id\": target_mouse_id,\n",
        "                \"video_frame\": pl.arange(start_frame, end_frame + 1, eager=True),\n",
        "            },\n",
        "            schema={\n",
        "                \"video_id\": pl.Int32,\n",
        "                \"agent_mouse_id\": pl.Int8,\n",
        "                \"target_mouse_id\": pl.Int8,\n",
        "                \"video_frame\": pl.Int32,\n",
        "            },\n",
        "        )\n",
        "\n",
        "        merged_pivot = (\n",
        "            pivot_trackings[agent_mouse_id]\n",
        "            .select(\n",
        "                pl.col(\"video_frame\"),\n",
        "                pl.exclude(\"video_frame\").name.prefix(\"agent_\"),\n",
        "            )\n",
        "            .join(\n",
        "                pivot_trackings[target_mouse_id].select(\n",
        "                    pl.col(\"video_frame\"),\n",
        "                    pl.exclude(\"video_frame\").name.prefix(\"target_\"),\n",
        "                ),\n",
        "                on=\"video_frame\",\n",
        "                how=\"inner\",\n",
        "            )\n",
        "        )\n",
        "        columns = merged_pivot.columns\n",
        "        merged_pivot = merged_pivot.with_columns(\n",
        "            *[pl.lit(None).cast(pl.Float32).alias(f\"agent_x_{bp}\") for bp in BODY_PARTS if f\"agent_x_{bp}\" not in columns],\n",
        "            *[pl.lit(None).cast(pl.Float32).alias(f\"agent_y_{bp}\") for bp in BODY_PARTS if f\"agent_y_{bp}\" not in columns],\n",
        "            *[pl.lit(None).cast(pl.Float32).alias(f\"target_x_{bp}\") for bp in BODY_PARTS if f\"target_x_{bp}\" not in columns],\n",
        "            *[pl.lit(None).cast(pl.Float32).alias(f\"target_y_{bp}\") for bp in BODY_PARTS if f\"target_y_{bp}\" not in columns],\n",
        "        )\n",
        "\n",
        "        features = merged_pivot.with_columns(\n",
        "            pl.lit(agent_mouse_id).alias(\"agent_mouse_id\"),\n",
        "            pl.lit(target_mouse_id).alias(\"target_mouse_id\"),\n",
        "        ).select(\n",
        "            pl.col(\"video_frame\"),\n",
        "            pl.col(\"agent_mouse_id\"),\n",
        "            pl.col(\"target_mouse_id\"),\n",
        "            *[\n",
        "                body_parts_distance(\"agent\", agent_body_part, \"target\", target_body_part).alias(\n",
        "                    f\"at__{agent_body_part}__{target_body_part}__distance\"\n",
        "                )\n",
        "                for agent_body_part, target_body_part in itertools.product(BODY_PARTS, repeat=2)\n",
        "            ],\n",
        "            *[\n",
        "                body_part_speed(\"agent\", body_part, period_ms).alias(f\"agent__{body_part}__speed_{period_ms}ms\")\n",
        "                for body_part, period_ms in itertools.product([\"ear_left\", \"ear_right\", \"tail_base\"], [500, 1000, 2000, 3000])\n",
        "            ],\n",
        "            *[\n",
        "                body_part_speed(\"target\", body_part, period_ms).alias(f\"target__{body_part}__speed_{period_ms}ms\")\n",
        "                for body_part, period_ms in itertools.product([\"ear_left\", \"ear_right\", \"tail_base\"], [500, 1000, 2000, 3000])\n",
        "            ],\n",
        "            elongation(\"agent\").alias(\"agent__elongation\"),\n",
        "            elongation(\"target\").alias(\"target__elongation\"),\n",
        "            body_angle(\"agent\").alias(\"agent__body_angle\"),\n",
        "            body_angle(\"target\").alias(\"target__body_angle\"),\n",
        "\n",
        "            # 1) Speed body_center của agent & target (locomotion tương đối)\n",
        "            body_part_speed(\"agent\", \"body_center\", 500).alias(\"agent__body_center__speed_500ms\"),\n",
        "            body_part_speed(\"target\", \"body_center\", 500).alias(\"target__body_center__speed_500ms\"),\n",
        "\n",
        "            # 2) Khoảng cách tâm thân + rolling mean\n",
        "            body_center_distance().alias(\"at__body_center__distance\"),\n",
        "            body_center_distance_rolling_agg(\"mean\", 500).alias(\"at__body_center__distance_mean_500ms\"),\n",
        "            body_center_distance_rolling_agg(\"mean\", 1000).alias(\"at__body_center__distance_mean_1000ms\"),\n",
        "\n",
        "            # 3) Động học khoảng cách & speed tương đối\n",
        "            body_center_radial_velocity().alias(\"at__body_center__radial_velocity\"),\n",
        "            relative_speed().alias(\"pair__body_center_speed_diff_500ms\"),\n",
        "\n",
        "            # 4) Facing score: agent nhìn target & target nhìn agent\n",
        "            facing_score(\"agent\", \"target\").alias(\"pair__agent_facing_score\"),\n",
        "            facing_score(\"target\", \"agent\").alias(\"pair__target_facing_score\"),\n",
        "        )\n",
        "\n",
        "        result_element = result_element.join(\n",
        "            features,\n",
        "            on=[\"video_frame\", \"agent_mouse_id\", \"target_mouse_id\"],\n",
        "            how=\"left\",\n",
        "        )\n",
        "        result.append(result_element)\n",
        "\n",
        "    return pl.concat(result, how=\"vertical\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a6607b00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6607b00",
        "outputId": "59d65ad5-6957-4f5a-cabf-f611d57b6e9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
            "/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:   27.2s\n",
            "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   40.3s\n",
            "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:   52.2s\n",
            "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:  1.4min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 848 videos successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 848 out of 848 | elapsed:  1.9min finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "%run -i self_features.py\n",
        "%run -i pair_features.py\n",
        "\n",
        "def process_video(row):\n",
        "    \"\"\"Process a single video to extract self and pair features.\"\"\"\n",
        "    lab_id = row[\"lab_id\"]\n",
        "    video_id = row[\"video_id\"]\n",
        "\n",
        "    tracking_path = TRAIN_TRACKING_DIR / f\"{lab_id}/{video_id}.parquet\"\n",
        "    tracking = pl.read_parquet(tracking_path)\n",
        "\n",
        "    self_features = make_self_features(metadata=row, tracking=tracking)\n",
        "    pair_features = make_pair_features(metadata=row, tracking=tracking)\n",
        "\n",
        "    self_features.write_parquet(WORKING_DIR / \"self_features\" / f\"{video_id}.parquet\")\n",
        "    pair_features.write_parquet(WORKING_DIR / \"pair_features\" / f\"{video_id}.parquet\")\n",
        "\n",
        "    return video_id\n",
        "\n",
        "\n",
        "# make data\n",
        "(WORKING_DIR / \"self_features\").mkdir(exist_ok=True, parents=True)\n",
        "(WORKING_DIR / \"pair_features\").mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "rows = list(train_dataframe.filter(pl.col(\"behaviors_labeled\").is_not_null()).rows(named=True))\n",
        "results = joblib.Parallel(n_jobs=-1, verbose=5)(joblib.delayed(process_video)(row) for row in rows)\n",
        "\n",
        "print(f\"Processed {len(results)} videos successfully\")\n",
        "\n",
        "del rows, results\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v28df4dZHlW6",
      "metadata": {
        "id": "v28df4dZHlW6"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEEDS = [42, 43, 44]"
      ],
      "metadata": {
        "id": "l2yRA6piV27q"
      },
      "id": "l2yRA6piV27q",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5f8349ce",
      "metadata": {
        "id": "5f8349ce"
      },
      "outputs": [],
      "source": [
        "def tune_threshold(oof_action, y_action):\n",
        "    thresholds = np.arange(0, 1.005, 0.005)\n",
        "    scores = [f1_score(y_action, (oof_action >= th), zero_division=0) for th in thresholds]\n",
        "    best_idx = np.argmax(scores)\n",
        "    return thresholds[best_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "677934fd",
      "metadata": {
        "id": "677934fd"
      },
      "source": [
        "*   XGB for perlab, per behavior\n",
        "*   Cross validation 3 Fold\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "098061a7",
      "metadata": {
        "id": "098061a7"
      },
      "outputs": [],
      "source": [
        "def train_validate(lab_id: str, behavior: str, indices: pl.DataFrame, features: pl.DataFrame, labels: pl.Series, seed: int = 42):\n",
        "    # Tạo đường dẫn thư mục để lưu kết quả\n",
        "    result_dir = WORKING_DIR / \"results\" / lab_id / behavior / f\"seed_{seed}\"\n",
        "    # Tạo thư mục nếu không tồn tại (bao gồm cả thư mục cha)\n",
        "    result_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # Xử lý trường hợp tổng nhãn là 0 (không có mẫu dương nào)\n",
        "    if labels.sum() == 0:\n",
        "        # Lưu điểm F1 là 0\n",
        "        with open(result_dir / \"f1.txt\", \"w\") as f:\n",
        "            f.write(\"0.0\\n\")\n",
        "        # Tạo DataFrame kết quả với tất cả các giá trị dự đoán là 0\n",
        "        oof_prediction_dataframe = indices.with_columns(\n",
        "            pl.Series(\"fold\", [-1] * len(labels), dtype=pl.Int8),  # Số fold (-1 nghĩa là không sử dụng)\n",
        "            pl.Series(\"prediction\", [0.0] * len(labels), dtype=pl.Float32),  # Xác suất dự đoán\n",
        "            pl.Series(\"predicted_label\", [0] * len(labels), dtype=pl.Int8),  # Nhãn dự đoán\n",
        "        )\n",
        "        # Lưu kết quả dưới dạng parquet\n",
        "        oof_prediction_dataframe.write_parquet(result_dir / \"oof_predictions.parquet\")\n",
        "        return 0.0\n",
        "\n",
        "    # Khởi tạo mảng để lưu kết quả dự đoán Out-of-Fold\n",
        "    folds = np.ones(len(labels), dtype=np.int8) * -1  # Số fold mà mỗi mẫu thuộc về\n",
        "    oof_predictions = np.zeros(len(labels), dtype=np.float32)  # Xác suất dự đoán\n",
        "    oof_prediction_labels = np.zeros(len(labels), dtype=np.int8)  # Nhãn dự đoán (0 hoặc 1)\n",
        "\n",
        "    # Thực hiện phân tích chéo nhóm phân tầng 3 fold\n",
        "    # StratifiedGroupKFold giữ phân bố nhãn và đảm bảo cùng một nhóm (video_id) không bị chia thành nhiều fold\n",
        "    cv = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=seed)\n",
        "\n",
        "    for fold, (train_idx, valid_idx) in enumerate(\n",
        "        cv.split(\n",
        "            X=features,\n",
        "            y=labels,\n",
        "            groups=indices.get_column(\"video_id\"),\n",
        "        )\n",
        "    ):\n",
        "        # Tạo thư mục để lưu kết quả cho mỗi fold\n",
        "        result_dir_fold = result_dir / f\"fold_{fold}\"\n",
        "        result_dir_fold.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        # Chia thành dữ liệu huấn luyện và xác thực\n",
        "        X_train = features[train_idx]  # Đặc trưng huấn luyện\n",
        "        y_train = labels[train_idx]  # Nhãn huấn luyện\n",
        "        X_valid = features[valid_idx]  # Đặc trưng xác thực\n",
        "        y_valid = labels[valid_idx]  # Nhãn xác thực\n",
        "\n",
        "        # Tính trọng số để xử lý mất cân bằng lớp\n",
        "        # Số mẫu âm / Số mẫu dương = Trọng số cho mẫu dương\n",
        "        scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
        "\n",
        "        # Đặt các tham số XGBoost\n",
        "        params = {\n",
        "            \"objective\": \"binary:logistic\",  # Vấn đề phân loại nhị phân\n",
        "            \"eval_metric\": \"logloss\",  # Chỉ số đánh giá: mất mát logarit\n",
        "            \"device\": \"cuda\",  # Thiết bị sử dụng\n",
        "            \"tree_method\": \"hist\",  # Thuật toán dựa trên histogram nhanh\n",
        "            \"learning_rate\": 0.05,  # Tốc độ học\n",
        "            \"max_depth\": 6,  # Độ sâu tối đa của cây\n",
        "            \"min_child_weight\": 5,  # Trọng số tối thiểu của nút con\n",
        "            \"subsample\": 0.8,  # Tỷ lệ mẫu sử dụng cho mỗi cây\n",
        "            \"colsample_bytree\": 0.8,  # Tỷ lệ đặc trưng sử dụng cho mỗi cây\n",
        "            \"scale_pos_weight\": scale_pos_weight,  # Trọng số của mẫu dương\n",
        "            \"max_bin\": 64,  # Số bin của histogram\n",
        "            \"seed\": seed,  # Seed ngẫu nhiên\n",
        "        }\n",
        "\n",
        "        # Tạo ma trận dữ liệu cho XGBoost (dữ liệu huấn luyện là ma trận lượng tử hóa, dữ liệu xác thực là ma trận thông thường)\n",
        "        dtrain = xgb.QuantileDMatrix(X_train, label=y_train, feature_names=features.columns, max_bin=64)\n",
        "        dvalid = xgb.DMatrix(X_valid, label=y_valid, feature_names=features.columns)\n",
        "\n",
        "        # Từ điển để lưu kết quả đánh giá\n",
        "        evals_result = {}\n",
        "\n",
        "        # Đặt callback dừng sớm\n",
        "        # Dừng huấn luyện nếu mất mát logarit của dữ liệu xác thực không cải thiện trong 10 vòng\n",
        "        early_stopping_callback = xgb.callback.EarlyStopping(\n",
        "            rounds=10,  # Số vòng liên tiếp không cải thiện\n",
        "            metric_name=\"logloss\",  # Chỉ số cần giám sát\n",
        "            data_name=\"valid\",  # Tập dữ liệu cần giám sát\n",
        "            maximize=False,  # Chỉ số càng nhỏ càng tốt\n",
        "            save_best=True,  # Lưu mô hình tốt nhất\n",
        "        )\n",
        "\n",
        "        # Thực hiện huấn luyện mô hình\n",
        "        model = xgb.train(\n",
        "            params,  # Tham số siêu\n",
        "            dtrain=dtrain,  # Dữ liệu huấn luyện\n",
        "            num_boost_round=250,  # Số vòng tăng cường tối đa\n",
        "            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],  # Tập dữ liệu để đánh giá\n",
        "            callbacks=[early_stopping_callback],  # Callback\n",
        "            evals_result=evals_result,  # Nơi lưu kết quả đánh giá\n",
        "            verbose_eval=0,  # Tần suất ghi log (0 là không ghi)\n",
        "        )\n",
        "\n",
        "        # Thực hiện dự đoán trên dữ liệu xác thực (lấy giá trị xác suất)\n",
        "        fold_predictions = model.predict(dvalid)\n",
        "\n",
        "        # Điều chỉnh ngưỡng tối ưu để tối đa hóa điểm F1\n",
        "        threshold = tune_threshold(fold_predictions, y_valid)\n",
        "\n",
        "        # Lưu kết quả dự đoán Out-of-Fold\n",
        "        folds[valid_idx] = fold  # Số fold\n",
        "        oof_predictions[valid_idx] = fold_predictions  # Xác suất dự đoán\n",
        "        oof_prediction_labels[valid_idx] = (fold_predictions >= threshold).astype(np.int8)  # Nhị phân hóa bằng ngưỡng\n",
        "\n",
        "        # Lưu kết quả của fold này\n",
        "        # Lưu mô hình đã huấn luyện\n",
        "        model.save_model(result_dir_fold / \"model.json\")\n",
        "        # Lưu ngưỡng tối ưu\n",
        "        with open(result_dir_fold / \"threshold.txt\", \"w\") as f:\n",
        "            f.write(f\"{threshold}\\n\")\n",
        "\n",
        "        # Vẽ biểu đồ mức độ quan trọng của đặc trưng (top 20, theo gain)\n",
        "        xgb.plot_importance(model, max_num_features=20, importance_type=\"gain\", values_format=\"{v:.2f}\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(result_dir_fold / \"feature_importance.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Vẽ biểu đồ đường cong học (diễn biến mất mát logarit)\n",
        "        lgb.plot_metric(evals_result, metric=\"logloss\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(result_dir_fold / \"metric.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Giải phóng bộ nhớ\n",
        "        gc.collect()\n",
        "\n",
        "    # Tổng hợp kết quả dự đoán của tất cả các fold vào một DataFrame\n",
        "    oof_prediction_dataframe = indices.with_columns(\n",
        "        pl.Series(\"fold\", folds, dtype=pl.Int8),  # Số fold\n",
        "        pl.Series(\"prediction\", oof_predictions, dtype=pl.Float32),  # Xác suất dự đoán\n",
        "        pl.Series(\"predicted_label\", oof_prediction_labels, dtype=pl.Int8),  # Nhãn dự đoán\n",
        "    )\n",
        "\n",
        "    # Tính điểm F1 tổng thể\n",
        "    f1 = f1_score(labels, oof_prediction_labels, zero_division=0)\n",
        "    # Lưu điểm F1 vào tệp\n",
        "    with open(result_dir / \"f1.txt\", \"w\") as f:\n",
        "        f.write(f\"{f1}\\n\")\n",
        "\n",
        "    # Lưu DataFrame kết quả dự đoán\n",
        "    oof_prediction_dataframe.write_parquet(result_dir / \"oof_predictions.parquet\")\n",
        "\n",
        "    # Trả về điểm F1\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56a27d78",
      "metadata": {
        "id": "56a27d78"
      },
      "source": [
        "##Self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388bdfa3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "b2658c052a45486894079a0420ddf50c",
            "9a834095a8684e1d909ef24401afcba9",
            "56dce4ede24744709ee8fef11d9b2094",
            "3c09bb04950d414088dc96a1d22564bf",
            "41e3edf2053b40f983be14ecd1ae34cb",
            "9d640c0939984454a3f12b284ae30e9b",
            "0f727d38eace448baec3cd90aa46f9d7",
            "854be417941142f99d8c71e8a8e51463",
            "20b02712e78a49b39baca79fc92bb7fc",
            "5dbb3b4b2b3c405fba008be5931f9f2a",
            "413c16781533472bba5ba35c51bfe7d7"
          ]
        },
        "id": "388bdfa3",
        "outputId": "5d061f81-5786-43d6-b0ea-c8d8426f56ac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/27 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2658c052a45486894079a0420ddf50c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|           LAB           |   BEHAVIOR    | SAMPLES  | POSITIVE | FEATURES |    F1    | ELAPSED TIME  |\n",
            "|     AdaptableSnail      |     rear      |   660,348|    85,313|        77|      0.64|        0:01:24|\n",
            "|         CRIM13          |     rear      |   179,132|    12,042|        77|      0.34|        0:01:57|\n",
            "|         CRIM13          |   selfgroom   |   205,533|    14,472|        77|      0.37|        0:02:32|\n",
            "|      CalMS21_task1      | genitalgroom  |   102,445|     6,270|        77|      0.65|        0:02:57|\n",
            "|       ElegantMink       |     rear      |         0|         0|         0|         -|        0:02:57|\n",
            "|       ElegantMink       |   selfgroom   |         0|         0|         0|         -|        0:02:57|\n",
            "|       GroovyShrew       |     rear      |   899,280|    50,768|        77|      0.53|        0:04:36|\n",
            "|       GroovyShrew       |     rest      |   530,886|    87,573|        77|      0.69|        0:05:36|\n",
            "|       GroovyShrew       |   selfgroom   |   877,773|    22,893|        77|"
          ]
        }
      ],
      "source": [
        "groups = train_self_behavior_dataframe.group_by(\"lab_id\", \"behavior\", maintain_order=True)\n",
        "total_groups = len(list(groups))\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "for idx, ((lab_id, behavior), group) in tqdm(enumerate(groups), total=total_groups):\n",
        "    if idx == 0:\n",
        "        tqdm.write(\n",
        "            f\"|{'LAB':^25}|{'BEHAVIOR':^15}|{'SAMPLES':^10}|{'POSITIVE':^10}|{'FEATURES':^10}|{'F1':^10}|{'ELAPSED TIME':^15}|\",\n",
        "            end=\"\\n\",\n",
        "        )\n",
        "\n",
        "    tqdm.write(f\"|{lab_id:^25}|{behavior:^15}|\", end=\"\")\n",
        "    index_list = []\n",
        "    feature_list = []\n",
        "    label_list = []\n",
        "\n",
        "    for row in group.rows(named=True):\n",
        "        video_id = row[\"video_id\"]\n",
        "        agent = row[\"agent\"]\n",
        "\n",
        "        agent_mouse_id = int(re.search(r\"mouse(\\d+)\", agent).group(1))\n",
        "\n",
        "        data = pl.scan_parquet(WORKING_DIR / \"self_features\" / f\"{video_id}.parquet\").filter(\n",
        "            (pl.col(\"agent_mouse_id\") == agent_mouse_id)\n",
        "        )\n",
        "        index = data.select(INDEX_COLS).collect(engine=\"streaming\")\n",
        "        feature = data.select(pl.exclude(INDEX_COLS)).collect(engine=\"streaming\")\n",
        "\n",
        "        # read annotation\n",
        "        annotation_path = TRAIN_ANNOTATION_DIR / lab_id / f\"{video_id}.parquet\"\n",
        "        if annotation_path.exists():\n",
        "            annotation = (\n",
        "                pl.scan_parquet(annotation_path)\n",
        "                .filter((pl.col(\"action\") == behavior) & (pl.col(\"agent_id\") == agent_mouse_id))\n",
        "                .collect()\n",
        "            )\n",
        "        else:\n",
        "            annotation = pl.DataFrame(\n",
        "                schema={\n",
        "                    \"agent_id\": pl.Int8,\n",
        "                    \"target_id\": pl.Int8,\n",
        "                    \"action\": str,\n",
        "                    \"start_frame\": pl.Int16,\n",
        "                    \"stop_frame\": pl.Int16,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        label_frames = set()\n",
        "        for annotation_row in annotation.rows(named=True):\n",
        "            label_frames.update(range(annotation_row[\"start_frame\"], annotation_row[\"stop_frame\"]))\n",
        "        label = index.select(pl.col(\"video_frame\").is_in(label_frames).cast(pl.Int8).alias(\"label\"))\n",
        "\n",
        "        if label.get_column(\"label\").sum() == 0:\n",
        "            continue\n",
        "\n",
        "        index_list.append(index)\n",
        "        feature_list.append(feature)\n",
        "        label_list.append(label.get_column(\"label\"))\n",
        "\n",
        "    if not index_list:\n",
        "        elapsed_time = datetime.timedelta(seconds=int(time.perf_counter() - start_time))\n",
        "        tqdm.write(f\"{0:>10,}|{0:>10,}|{0:>10,}|{'-':>10}|{str(elapsed_time):>15}|\", end=\"\\n\")\n",
        "        continue\n",
        "\n",
        "    indices = pl.concat(index_list, how=\"vertical\")\n",
        "    features = pl.concat(feature_list, how=\"vertical\")\n",
        "    labels = pl.concat(label_list, how=\"vertical\")\n",
        "\n",
        "    del index_list, feature_list, label_list\n",
        "    gc.collect()\n",
        "\n",
        "    tqdm.write(f\"{len(indices):>10,}|{labels.sum():>10,}|{len(features.columns):>10,}|\", end=\"\")\n",
        "\n",
        "    f1_list = []\n",
        "    for seed in SEEDS:\n",
        "        f1 = train_validate(lab_id, behavior, indices, features, labels, seed=seed)\n",
        "        f1_list.append(f1)\n",
        "    mean_f1 = float(np.mean(f1_list))\n",
        "    tqdm.write(f\"{mean_f1:>10.2f}|\", end=\"\")\n",
        "\n",
        "    elapsed_time = datetime.timedelta(seconds=int(time.perf_counter() - start_time))\n",
        "    tqdm.write(f\"{str(elapsed_time):>15}|\", end=\"\\n\")\n",
        "\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b977e906",
      "metadata": {
        "id": "b977e906"
      },
      "source": [
        "##Pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f796732",
      "metadata": {
        "id": "0f796732"
      },
      "outputs": [],
      "source": [
        "groups = train_pair_behavior_dataframe.group_by(\"lab_id\", \"behavior\", maintain_order=True)\n",
        "total_groups = len(list(groups))\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "for idx, ((lab_id, behavior), group) in tqdm(enumerate(groups), total=total_groups):\n",
        "    if idx == 0:\n",
        "        tqdm.write(\n",
        "            f\"|{'LAB':^25}|{'BEHAVIOR':^15}|{'SAMPLES':^10}|{'POSITIVE':^10}|{'FEATURES':^10}|{'F1':^10}|{'ELAPSED TIME':^15}|\",\n",
        "            end=\"\\n\",\n",
        "        )\n",
        "\n",
        "    tqdm.write(f\"|{lab_id:^25}|{behavior:^15}|\", end=\"\")\n",
        "    index_list = []\n",
        "    feature_list = []\n",
        "    label_list = []\n",
        "\n",
        "    for row in group.rows(named=True):\n",
        "        video_id = row[\"video_id\"]\n",
        "        agent = row[\"agent\"]\n",
        "        target = row[\"target\"]\n",
        "\n",
        "        agent_mouse_id = int(re.search(r\"mouse(\\d+)\", agent).group(1))\n",
        "        target_mouse_id = int(re.search(r\"mouse(\\d+)\", target).group(1))\n",
        "\n",
        "        data = pl.scan_parquet(WORKING_DIR / \"pair_features\" / f\"{video_id}.parquet\").filter(\n",
        "            (pl.col(\"agent_mouse_id\") == agent_mouse_id) & (pl.col(\"target_mouse_id\") == target_mouse_id)\n",
        "        )\n",
        "        index = data.select(INDEX_COLS).collect(engine=\"streaming\")\n",
        "        feature = data.select(pl.exclude(INDEX_COLS)).collect(engine=\"streaming\")\n",
        "\n",
        "        # read annotation\n",
        "        annotation_path = TRAIN_ANNOTATION_DIR / lab_id / f\"{video_id}.parquet\"\n",
        "        if annotation_path.exists():\n",
        "            annotation = (\n",
        "                pl.scan_parquet(annotation_path)\n",
        "                .filter(\n",
        "                    (pl.col(\"action\") == behavior)\n",
        "                    & (pl.col(\"agent_id\") == agent_mouse_id)\n",
        "                    & (pl.col(\"target_id\") == target_mouse_id)\n",
        "                )\n",
        "                .collect()\n",
        "            )\n",
        "        else:\n",
        "            annotation = pl.DataFrame(\n",
        "                schema={\n",
        "                    \"agent_id\": pl.Int8,\n",
        "                    \"target_id\": pl.Int8,\n",
        "                    \"action\": str,\n",
        "                    \"start_frame\": pl.Int16,\n",
        "                    \"stop_frame\": pl.Int16,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        label_frames = set()\n",
        "        for annotation_row in annotation.rows(named=True):\n",
        "            label_frames.update(range(annotation_row[\"start_frame\"], annotation_row[\"stop_frame\"]))\n",
        "        label = index.select(pl.col(\"video_frame\").is_in(label_frames).cast(pl.Int8).alias(\"label\"))\n",
        "\n",
        "        if label.get_column(\"label\").sum() == 0:\n",
        "            continue\n",
        "\n",
        "        index_list.append(index)\n",
        "        feature_list.append(feature)\n",
        "        label_list.append(label.get_column(\"label\"))\n",
        "\n",
        "    if not index_list:\n",
        "        elapsed_time = datetime.timedelta(seconds=int(time.perf_counter() - start_time))\n",
        "        tqdm.write(f\"{0:>10,}|{0:>10,}|{0:>10,}|{'-':>10}|{str(elapsed_time):>15}|\", end=\"\\n\")\n",
        "        continue\n",
        "\n",
        "    indices = pl.concat(index_list, how=\"vertical\")\n",
        "    features = pl.concat(feature_list, how=\"vertical\")\n",
        "    labels = pl.concat(label_list, how=\"vertical\")\n",
        "\n",
        "    del index_list, feature_list, label_list\n",
        "    gc.collect()\n",
        "\n",
        "    tqdm.write(f\"{len(indices):>10,}|{labels.sum():>10,}|{len(features.columns):>10,}|\", end=\"\")\n",
        "\n",
        "    f1_list = []\n",
        "    for seed in SEEDS:\n",
        "        f1 = train_validate(lab_id, behavior, indices, features, labels, seed=seed)\n",
        "        f1_list.append(f1)\n",
        "    mean_f1 = float(np.mean(f1_list))\n",
        "    tqdm.write(f\"{mean_f1:>10.2f}|\", end=\"\")\n",
        "\n",
        "    elapsed_time = datetime.timedelta(seconds=int(time.perf_counter() - start_time))\n",
        "    tqdm.write(f\"{str(elapsed_time):>15}|\", end=\"\\n\")\n",
        "\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88a0a099",
      "metadata": {
        "id": "88a0a099"
      },
      "outputs": [],
      "source": [
        "%%writefile robustify.py\n",
        "\n",
        "def robustify(submission: pl.DataFrame, dataset: pl.DataFrame, train_test: str = \"train\"):\n",
        "    traintest_directory = INPUT_DIR / f\"{train_test}_tracking\"\n",
        "\n",
        "    old_submission = submission.clone()\n",
        "    submission = submission.filter(pl.col(\"start_frame\") < pl.col(\"stop_frame\"))\n",
        "    if len(submission) != len(old_submission):\n",
        "        print(\"ERROR: Dropped frames with start >= stop\")\n",
        "\n",
        "    old_submission = submission.clone()\n",
        "    group_list = []\n",
        "    for _, group in submission.group_by(\"video_id\", \"agent_id\", \"target_id\"):\n",
        "        group = group.sort(\"start_frame\")\n",
        "        mask = np.ones(len(group), dtype=bool)\n",
        "        last_stop_frame = 0\n",
        "        for i, row in enumerate(group.rows(named=True)):\n",
        "            if row[\"start_frame\"] < last_stop_frame:\n",
        "                mask[i] = False\n",
        "            else:\n",
        "                last_stop_frame = row[\"stop_frame\"]\n",
        "        group_list.append(group.filter(pl.Series(\"mask\", mask)))\n",
        "\n",
        "    submission = pl.concat(group_list)\n",
        "\n",
        "    if len(submission) != len(old_submission):\n",
        "        print(\"ERROR: Dropped duplicate frames\")\n",
        "\n",
        "    # ========= 💡 3. MERGE SMALL GAPS GIỮA 2 EVENT CÙNG ACTION =========\n",
        "    MAX_GAP_FRAMES = 1\n",
        "\n",
        "    merged_groups = []\n",
        "    before_merge = len(submission)\n",
        "\n",
        "    # Group theo (video, agent, target), DUYỆT THEO THỨ TỰ GLOBAL\n",
        "    for _, group in submission.group_by([\"video_id\", \"agent_id\", \"target_id\"]):\n",
        "        group = group.sort(\"start_frame\")\n",
        "        rows = list(group.rows(named=True))\n",
        "        if not rows:\n",
        "            continue\n",
        "\n",
        "        current = dict(rows[0])\n",
        "        merged = []\n",
        "\n",
        "        for row in rows[1:]:\n",
        "            # Chỉ merge nếu CÙNG action và gap nhỏ\n",
        "            gap = row[\"start_frame\"] - current[\"stop_frame\"] - 1\n",
        "            if (row[\"action\"] == current[\"action\"]) and (gap <= MAX_GAP_FRAMES):\n",
        "                if row[\"stop_frame\"] > current[\"stop_frame\"]:\n",
        "                    current[\"stop_frame\"] = row[\"stop_frame\"]\n",
        "            else:\n",
        "                merged.append(current)\n",
        "                current = dict(row)\n",
        "\n",
        "        merged.append(current)\n",
        "        merged_groups.append(pl.DataFrame(merged, schema=submission.schema))\n",
        "\n",
        "    if merged_groups:\n",
        "        submission = pl.concat(merged_groups)\n",
        "\n",
        "    after_merge = len(submission)\n",
        "    if after_merge != before_merge:\n",
        "        print(f\"INFO: Merged small gaps, events: {before_merge} -> {after_merge}\")\n",
        "    # =========  THÊM BƯỚC LỌC EVENT QUÁ NGẮN Ở ĐÂY =========\n",
        "    MIN_LEN_FRAMES = 3\n",
        "    SHORT_OK = [\"flinch\"]\n",
        "\n",
        "    submission = submission.with_columns(\n",
        "        (pl.col(\"stop_frame\") - pl.col(\"start_frame\")).alias(\"length\")\n",
        "    )\n",
        "\n",
        "    before = len(submission)\n",
        "\n",
        "    submission = submission.filter(\n",
        "        pl.when(pl.col(\"action\").is_in(SHORT_OK))\n",
        "          .then(pl.col(\"length\") >= 1)                 # các action “nhấp nháy” không lọc theo min_len\n",
        "          .otherwise(pl.col(\"length\") >= MIN_LEN_FRAMES)  # còn lại phải dài >= 3 frame\n",
        "    )\n",
        "\n",
        "    submission = submission.drop(\"length\")\n",
        "    after = len(submission)\n",
        "\n",
        "    if after != before:\n",
        "        print(f\"INFO: Dropped {before - after} short events (action-dependent)\")\n",
        "\n",
        "    # =========================================================\n",
        "\n",
        "\n",
        "    s_list = []\n",
        "    for row in dataset.rows(named=True):\n",
        "        lab_id = row[\"lab_id\"]\n",
        "        video_id = row[\"video_id\"]\n",
        "        if row[\"behaviors_labeled\"] is None:\n",
        "            continue\n",
        "\n",
        "        if video_id in submission.get_column(\"video_id\").to_list():\n",
        "            continue\n",
        "\n",
        "        if isinstance(row[\"behaviors_labeled\"], str):\n",
        "            continue\n",
        "\n",
        "        print(f\"Video {video_id} has no predictions.\")\n",
        "\n",
        "        path = traintest_directory / f\"/{lab_id}/{video_id}.parquet\"\n",
        "        vid = pd.read_parquet(path)\n",
        "\n",
        "        vid_behaviors = json.loads(row[\"behaviors_labeled\"])\n",
        "        vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n",
        "        vid_behaviors = [b.split(\",\") for b in vid_behaviors]\n",
        "        vid_behaviors = pd.DataFrame(vid_behaviors, columns=[\"agent\", \"target\", \"action\"])\n",
        "\n",
        "        start_frame = vid.video_frame.min()\n",
        "        stop_frame = vid.video_frame.max() + 1\n",
        "\n",
        "        for (agent, target), actions in vid_behaviors.groupby([\"agent\", \"target\"]):\n",
        "            batch_length = int(np.ceil((stop_frame - start_frame) / len(actions)))\n",
        "            for i, action_row in enumerate(actions.itertuples(index=False)):\n",
        "                batch_start = start_frame + i * batch_length\n",
        "                batch_stop = min(batch_start + batch_length, stop_frame)\n",
        "                s_list.append((video_id, agent, target, action_row[\"action\"], batch_start, batch_stop))\n",
        "\n",
        "    if len(s_list) > 0:\n",
        "        submission = pd.concat(\n",
        "            [\n",
        "                submission,\n",
        "                pd.DataFrame(s_list, columns=[\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]),\n",
        "            ]\n",
        "        )\n",
        "        print(\"ERROR: Filled empty videos\")\n",
        "\n",
        "    return submission"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5728781",
      "metadata": {
        "id": "e5728781"
      },
      "source": [
        "## Tổng hợp các giá trị dự đoán trên dữ liệu kiểm chứng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a1ccc3c",
      "metadata": {
        "id": "9a1ccc3c"
      },
      "outputs": [],
      "source": [
        "# Danh sách để lưu kết quả dự đoán Out-of-Fold của từng nhóm\n",
        "group_oof_predictions: list[pl.DataFrame] = []\n",
        "\n",
        "# Nhóm dữ liệu theo lab_id, video_id, agent, target\n",
        "# Dùng list(...) để dùng lại groups + có total cho tqdm\n",
        "groups = list(\n",
        "    train_behavior_dataframe.group_by(\n",
        "        \"lab_id\", \"video_id\", \"agent\", \"target\",\n",
        "        maintain_order=True,\n",
        "    )\n",
        ")\n",
        "\n",
        "for (lab_id, video_id, agent, target), group in tqdm(groups, total=len(groups)):\n",
        "    # Trích xuất ID chuột từ agent (chủ thể hành động), ví dụ: \"mouse1\" → 1\n",
        "    agent_mouse_id = int(re.search(r\"mouse(\\d+)\", agent).group(1))\n",
        "\n",
        "    # Trích xuất ID chuột từ target (đối tượng hành động)\n",
        "    # Nếu là \"self\" thì -1, ngược lại lấy ID chuột\n",
        "    target_mouse_id = -1 if target == \"self\" else int(re.search(r\"mouse(\\d+)\", target).group(1))\n",
        "\n",
        "    # Danh sách để lưu score cho từng behavior trong nhóm này\n",
        "    prediction_dataframe_list: list[pl.DataFrame] = []\n",
        "\n",
        "    # Lấy danh sách behavior duy nhất trong group\n",
        "    unique_behaviors = group.get_column(\"behavior\").unique().to_list()\n",
        "\n",
        "    # Xử lý từng behavior trong nhóm\n",
        "    for behavior in unique_behaviors:\n",
        "        # ===== Ensemble OOF qua nhiều seed cho behavior này =====\n",
        "        pred_dfs_per_seed: list[pl.DataFrame] = []\n",
        "\n",
        "        for seed in SEEDS:\n",
        "            # Đường dẫn OOF cho seed này\n",
        "            oof_path = (\n",
        "                WORKING_DIR\n",
        "                / \"results\"\n",
        "                / lab_id\n",
        "                / behavior\n",
        "                / f\"seed_{seed}\"\n",
        "                / \"oof_predictions.parquet\"\n",
        "            )\n",
        "\n",
        "            if not oof_path.exists():\n",
        "                continue\n",
        "\n",
        "            # Đọc OOF của seed, lọc đúng video/agent/target\n",
        "            df_seed = (\n",
        "                pl.scan_parquet(oof_path)\n",
        "                .filter(\n",
        "                    (pl.col(\"video_id\") == video_id)\n",
        "                    & (pl.col(\"agent_mouse_id\") == agent_mouse_id)\n",
        "                    & (pl.col(\"target_mouse_id\") == target_mouse_id)\n",
        "                )\n",
        "                .select(\n",
        "                    *INDEX_COLS,\n",
        "                    # Score seed = prob * predicted_label\n",
        "                    # (dưới threshold = 0, trên threshold = prob)\n",
        "                    (pl.col(\"prediction\") * pl.col(\"predicted_label\")).alias(f\"{behavior}_seed_{seed}\")\n",
        "                )\n",
        "                .collect()\n",
        "            )\n",
        "\n",
        "            if df_seed.height > 0:\n",
        "                pred_dfs_per_seed.append(df_seed)\n",
        "\n",
        "        # Không có seed nào có OOF cho behavior này → bỏ qua\n",
        "        if not pred_dfs_per_seed:\n",
        "            continue\n",
        "\n",
        "        # Gộp các seed theo INDEX_COLS\n",
        "        pred_merged = pl.concat(pred_dfs_per_seed, how=\"align\")\n",
        "\n",
        "        # Các cột score theo seed cho behavior này\n",
        "        seed_cols = [c for c in pred_merged.columns if c.startswith(f\"{behavior}_seed_\")]\n",
        "\n",
        "        # Mean score qua seed → 1 score ensemble cho behavior này\n",
        "        behavior_scores = (\n",
        "            pred_merged\n",
        "            .with_columns(\n",
        "                pl.mean_horizontal(pl.col(seed_cols)).alias(behavior)\n",
        "            )\n",
        "            .select(*INDEX_COLS, behavior)\n",
        "        )\n",
        "\n",
        "        prediction_dataframe_list.append(behavior_scores)\n",
        "\n",
        "    # Bỏ qua nếu không có behavior nào có OOF cho nhóm này\n",
        "    if not prediction_dataframe_list:\n",
        "        continue\n",
        "\n",
        "    # Kết hợp score của nhiều behavior theo chiều ngang (align theo INDEX_COLS)\n",
        "    prediction_dataframe = pl.concat(prediction_dataframe_list, how=\"align\")\n",
        "\n",
        "    # Lấy tên các cột không phải INDEX_COLS (tức là tên behavior)\n",
        "    cols = prediction_dataframe.select(pl.exclude(INDEX_COLS)).columns\n",
        "\n",
        "    # Chọn behavior có score cao nhất mỗi frame\n",
        "    prediction_labels_dataframe = (\n",
        "        prediction_dataframe\n",
        "        .with_columns(\n",
        "            pl.struct(pl.exclude(INDEX_COLS))\n",
        "            .map_elements(\n",
        "                lambda row: (\n",
        "                    \"none\"\n",
        "                    if sum(row.values()) == 0\n",
        "                    else cols[int(np.argmax(list(row.values())))]\n",
        "                ),\n",
        "                return_dtype=pl.String,\n",
        "            )\n",
        "            .alias(\"prediction\")\n",
        "        )\n",
        "        .select(INDEX_COLS + [\"prediction\"])\n",
        "    )\n",
        "\n",
        "    # Gom các đoạn liên tiếp cùng prediction thành segments\n",
        "    group_oof_prediction = (\n",
        "        prediction_labels_dataframe\n",
        "        # lấy các điểm biên khi prediction thay đổi\n",
        "        .filter(pl.col(\"prediction\") != pl.col(\"prediction\").shift(1))\n",
        "        # frame kế tiếp là stop_frame\n",
        "        .with_columns(\n",
        "            pl.col(\"video_frame\").shift(-1).alias(\"stop_frame\")\n",
        "        )\n",
        "        # bỏ \"none\"\n",
        "        .filter(pl.col(\"prediction\") != \"none\")\n",
        "        .select(\n",
        "            pl.col(\"video_id\"),\n",
        "            # agent_id: \"mouse1\", \"mouse2\", ...\n",
        "            (\"mouse\" + pl.col(\"agent_mouse_id\").cast(pl.Utf8)).alias(\"agent_id\"),\n",
        "            # target_id: \"self\" hoặc \"mouseX\"\n",
        "            pl.when(pl.col(\"target_mouse_id\") == -1)\n",
        "            .then(pl.lit(\"self\"))\n",
        "            .otherwise(\"mouse\" + pl.col(\"target_mouse_id\").cast(pl.Utf8))\n",
        "            .alias(\"target_id\"),\n",
        "            pl.col(\"prediction\").alias(\"action\"),\n",
        "            pl.col(\"video_frame\").alias(\"start_frame\"),\n",
        "            pl.col(\"stop_frame\"),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    group_oof_predictions.append(group_oof_prediction)\n",
        "\n",
        "# ====== robustify + lưu file OOF CSV dùng cho compute_validation_metrics ======\n",
        "%run -i robustify.py\n",
        "\n",
        "oof_predictions = pl.concat(group_oof_predictions, how=\"vertical\")\n",
        "oof_predictions = robustify(oof_predictions, train_dataframe, train_test=\"train\")\n",
        "oof_predictions.with_row_index(\"row_id\").write_csv(WORKING_DIR / \"oof_predictions.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "863524d2",
      "metadata": {
        "id": "863524d2"
      },
      "source": [
        "##Tính điểm (score) dựa trên dữ liệu kiểm chứng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f208b832",
      "metadata": {
        "id": "f208b832"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_validation_metrics(submission, verbose=True):\n",
        "    \"\"\"Compute and display validation metrics for single vs pair behaviors.\"\"\"\n",
        "    # solution_df\n",
        "    dataset = pl.read_csv(INPUT_DIR / \"train.csv\").to_pandas()\n",
        "\n",
        "    solution = []\n",
        "    for _, row in dataset.iterrows():\n",
        "        lab_id = row[\"lab_id\"]\n",
        "        if lab_id.startswith(\"MABe22\"):\n",
        "            continue\n",
        "\n",
        "        video_id = row[\"video_id\"]\n",
        "        path = TRAIN_ANNOTATION_DIR / lab_id / f\"{video_id}.parquet\"\n",
        "        try:\n",
        "            annot = pd.read_parquet(path)\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "\n",
        "        annot[\"lab_id\"] = lab_id\n",
        "        annot[\"video_id\"] = video_id\n",
        "        annot[\"behaviors_labeled\"] = row[\"behaviors_labeled\"]\n",
        "        annot[\"target_id\"] = np.where(\n",
        "            annot.target_id != annot.agent_id, annot[\"target_id\"].apply(lambda s: f\"mouse{s}\"), \"self\"\n",
        "        )\n",
        "        annot[\"agent_id\"] = annot[\"agent_id\"].apply(lambda s: f\"mouse{s}\")\n",
        "        solution.append(annot)\n",
        "\n",
        "    solution = pd.concat(solution)\n",
        "\n",
        "    try:\n",
        "        # Separate single and pair behaviors\n",
        "        submission_single = submission[submission[\"target_id\"] == \"self\"].copy()\n",
        "        submission_pair = submission[submission[\"target_id\"] != \"self\"].copy()\n",
        "\n",
        "        # Filter solution to match submission videos\n",
        "        solution_videos = set(submission[\"video_id\"].unique())\n",
        "        solution = solution[solution[\"video_id\"].isin(solution_videos)]\n",
        "\n",
        "        if len(solution) == 0:\n",
        "            return\n",
        "\n",
        "        # Compute overall F1 score\n",
        "        overall_f1 = score(solution, submission, \"row_id\", beta=1.0)\n",
        "        print(f\"\\n{'=' * 60}\")\n",
        "        print(\"PERFORMANCE METRICS\")\n",
        "        print(f\"{'=' * 60}\")\n",
        "        print(f\"Overall F1 Score: {overall_f1:.4f}\")\n",
        "        print(f\"Total predictions: {len(submission)}\")\n",
        "        print(f\"  - Single behaviors: {len(submission_single)}\")\n",
        "        print(f\"  - Pair behaviors: {len(submission_pair)}\")\n",
        "\n",
        "        # Compute per-action F1 scores using existing scoring function\n",
        "        solution_pl = pl.DataFrame(solution)\n",
        "        submission_pl = pl.DataFrame(submission)\n",
        "\n",
        "        # Add label_key and prediction_key\n",
        "        solution_pl = solution_pl.with_columns(\n",
        "            pl.concat_str(\n",
        "                [\n",
        "                    pl.col(\"video_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"agent_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"target_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"action\"),\n",
        "                ],\n",
        "                separator=\"_\",\n",
        "            ).alias(\"label_key\"),\n",
        "        )\n",
        "        submission_pl = submission_pl.with_columns(\n",
        "            pl.concat_str(\n",
        "                [\n",
        "                    pl.col(\"video_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"agent_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"target_id\").cast(pl.Utf8),\n",
        "                    pl.col(\"action\"),\n",
        "                ],\n",
        "                separator=\"_\",\n",
        "            ).alias(\"prediction_key\"),\n",
        "        )\n",
        "\n",
        "        # Group by action and compute metrics\n",
        "        action_stats = defaultdict(lambda: {\"single\": {\"count\": 0, \"f1\": 0.0}, \"pair\": {\"count\": 0, \"f1\": 0.0}})\n",
        "\n",
        "        for lab in solution_pl[\"lab_id\"].unique():\n",
        "            lab_solution = solution_pl.filter(pl.col(\"lab_id\") == lab).clone()\n",
        "            lab_videos = set(lab_solution[\"video_id\"].unique())\n",
        "            lab_submission = submission_pl.filter(pl.col(\"video_id\").is_in(lab_videos)).clone()\n",
        "\n",
        "            # Compute per-action F1 using same logic as single_lab_f1\n",
        "            label_frames = defaultdict(set)\n",
        "            prediction_frames = defaultdict(set)\n",
        "\n",
        "            for row in lab_solution.to_dicts():\n",
        "                label_frames[row[\"label_key\"]].update(range(row[\"start_frame\"], row[\"stop_frame\"]))\n",
        "\n",
        "            for row in lab_submission.to_dicts():\n",
        "                key = row[\"prediction_key\"]\n",
        "                prediction_frames[key].update(range(row[\"start_frame\"], row[\"stop_frame\"]))\n",
        "\n",
        "            for key in set(list(label_frames.keys()) + list(prediction_frames.keys())):\n",
        "                action = key.split(\"_\")[-1]\n",
        "                mode = \"single\" if \"self\" in key else \"pair\"\n",
        "\n",
        "                pred_frames = prediction_frames.get(key, set())\n",
        "                label_frames_set = label_frames.get(key, set())\n",
        "\n",
        "                tp = len(pred_frames & label_frames_set)\n",
        "                fn = len(label_frames_set - pred_frames)\n",
        "                fp = len(pred_frames - label_frames_set)\n",
        "\n",
        "                if tp + fn + fp > 0:\n",
        "                    f1 = (1 + 1**2) * tp / ((1 + 1**2) * tp + 1**2 * fn + fp)\n",
        "                    action_stats[action][mode][\"count\"] += 1\n",
        "                    action_stats[action][mode][\"f1\"] += f1\n",
        "\n",
        "        # Print per-action summary\n",
        "        print(\"\\nPer-Action Performance Summary:\")\n",
        "        print(f\"{'-' * 60}\")\n",
        "        print(f\"{'Action':<20} {'Mode':<10} {'Count':<10} {'Avg F1':<10}\")\n",
        "        print(f\"{'-' * 60}\")\n",
        "\n",
        "        for action in sorted(action_stats.keys()):\n",
        "            for mode in [\"single\", \"pair\"]:\n",
        "                stats = action_stats[action][mode]\n",
        "                if stats[\"count\"] > 0:\n",
        "                    avg_f1 = stats[\"f1\"] / stats[\"count\"]\n",
        "                    print(f\"{action:<20} {mode:<10} {stats['count']:<10} {avg_f1:<10.4f}\")\n",
        "\n",
        "        # Summary by mode\n",
        "        single_actions = [a for a in action_stats.keys() if action_stats[a][\"single\"][\"count\"] > 0]\n",
        "        pair_actions = [a for a in action_stats.keys() if action_stats[a][\"pair\"][\"count\"] > 0]\n",
        "\n",
        "        if single_actions:\n",
        "            single_avg_f1 = np.mean(\n",
        "                [\n",
        "                    action_stats[a][\"single\"][\"f1\"] / action_stats[a][\"single\"][\"count\"]\n",
        "                    for a in single_actions\n",
        "                    if action_stats[a][\"single\"][\"count\"] > 0\n",
        "                ]\n",
        "            )\n",
        "            print(f\"\\nSingle behaviors: {len(single_actions)} actions, Avg F1: {single_avg_f1:.4f}\")\n",
        "\n",
        "        if pair_actions:\n",
        "            pair_avg_f1 = np.mean(\n",
        "                [\n",
        "                    action_stats[a][\"pair\"][\"f1\"] / action_stats[a][\"pair\"][\"count\"]\n",
        "                    for a in pair_actions\n",
        "                    if action_stats[a][\"pair\"][\"count\"] > 0\n",
        "                ]\n",
        "            )\n",
        "            print(f\"Pair behaviors: {len(pair_actions)} actions, Avg F1: {pair_avg_f1:.4f}\")\n",
        "\n",
        "        print(f\"{'=' * 60}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            error_msg = str(e)\n",
        "            if len(error_msg) > 200:\n",
        "                error_msg = error_msg[:200] + \"...\"\n",
        "            print(f\"\\nWarning: Could not compute validation metrics: {error_msg}\")\n",
        "            if verbose:\n",
        "                print(f\"Traceback: {traceback.format_exc()[:300]}\")\n",
        "\n",
        "compute_validation_metrics(submission=pd.read_csv(WORKING_DIR / \"oof_predictions.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yy7c_msWIpzq"
      },
      "id": "Yy7c_msWIpzq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 13874099,
          "sourceId": 59156,
          "sourceType": "competition"
        },
        {
          "sourceId": 262477103,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 279806245,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 31153,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 16813.530603,
      "end_time": "2025-11-25T05:43:26.989111",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-11-25T01:03:13.458508",
      "version": "2.6.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b2658c052a45486894079a0420ddf50c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a834095a8684e1d909ef24401afcba9",
              "IPY_MODEL_56dce4ede24744709ee8fef11d9b2094",
              "IPY_MODEL_3c09bb04950d414088dc96a1d22564bf"
            ],
            "layout": "IPY_MODEL_41e3edf2053b40f983be14ecd1ae34cb"
          }
        },
        "9a834095a8684e1d909ef24401afcba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d640c0939984454a3f12b284ae30e9b",
            "placeholder": "​",
            "style": "IPY_MODEL_0f727d38eace448baec3cd90aa46f9d7",
            "value": " 30%"
          }
        },
        "56dce4ede24744709ee8fef11d9b2094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_854be417941142f99d8c71e8a8e51463",
            "max": 27,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20b02712e78a49b39baca79fc92bb7fc",
            "value": 8
          }
        },
        "3c09bb04950d414088dc96a1d22564bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dbb3b4b2b3c405fba008be5931f9f2a",
            "placeholder": "​",
            "style": "IPY_MODEL_413c16781533472bba5ba35c51bfe7d7",
            "value": " 8/27 [05:37&lt;13:38, 43.07s/it]"
          }
        },
        "41e3edf2053b40f983be14ecd1ae34cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d640c0939984454a3f12b284ae30e9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f727d38eace448baec3cd90aa46f9d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "854be417941142f99d8c71e8a8e51463": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20b02712e78a49b39baca79fc92bb7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5dbb3b4b2b3c405fba008be5931f9f2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "413c16781533472bba5ba35c51bfe7d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}